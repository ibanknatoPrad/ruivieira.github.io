<!DOCTYPE html>
<html lang="en">
<head>
    <link href="/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
    <link href="/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
    <link href="/favicons/favicon.ico" rel="icon" sizes="48x48" type="image/png">

    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <link href="https://ruivieira.dev/search.html" rel="canonical">

    <script src="https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@master/dist/flexsearch.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Nunito:400,300i,800&display=swap" rel="stylesheet"/>
    <link href="/assets/style.css" rel="stylesheet"/>
    <link href="/assets/code.css" rel="stylesheet"/>
    <title>ruivieira.dev - Search</title>
    <script async data-goatcounter="https://ruivieira-dev.goatcounter.com/count" src="//gc.zgo.at/count.js"></script>
    <style>
        #search_terms {
            width: 75%;
            font-size: 2rem;
            font-family: Nunito;
            margin-top: 7rem;
        }

        #search_button {
            background-color: #ddd;
            border: none;
            color: black;
            padding: 0.5rem 0.5rem;
            font-size: 2rem;
            font-family: Nunito;
            text-decoration: none;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 10px;
            width: 20%;
        }

        #results {
            margin-top: 2rem;
        }
    </style>
</head>
<body>
<div id="grid">
    <div id="content">

        <input id="search_terms" type="search"/>
        <button id="search_button" onclick="search()">Search</button>

        <div id="results">

        </div>

        <div class="footer">
            <span class="cc-symbol">&#127341;</span> 2020 CC BY Rui Vieira
        </div>
    </div>

    <div id="sidebar">
        <p><a href="/">Home</a></p>
        <p><a href="/content.html">All pages</a></p>
        <p><a href="/graph.html">Link network</a></p>

        <div class="footer">
            modified .
        </div>

    </div>
</div>
<script>
    var index = new FlexSearch({encode: "icase", tokenize: "forward", async: false});
let docs = [];
index.add(1, "---&#10;date: 2020-04-07T23:31:00+01:00&#10;draft: false&#10;url: &quot;/semi-handcrafted-rss.html&quot;&#10;--- # (Semi) handcrafted RSS I&apos;ve been using a minimalist blog setup for some time now. I was having something of a framework fatigue after switching between a few static site generators.&#10;Each new generator I decided to try implied usually either learning a new programming language ([Python](https://docs.getpelican.com/en/stable/), [Ruby](https://jekyllrb.com/), [Go](https://gohugo.io/)) to perform basic setup _and_ a new template engine syntax. Typically I wasn&apos;t using the vast majority of the features available&#10;for each generator. And finally, most of the generators I tried over the years rely on heavy configuration if I want to maintain the&#10;site organisation and look.&#10;I&apos;ve discussed with some friends and colleagues why it&apos;s my opinion that a plain HTML blog is still superior to other solutions (such as Markdown coupled with some generator framework). I&apos;ll leave my arguments to a future post. However, I am still using _some form_ of a generator. The blog writing process at the moment is the following: - I write the content of the post to an HTML fragment (no `HEAD`, for instance). All files are HTML and in the same folder.&#10;- I have a shell script to walk through the files in the input folder and add a common header, footer and process all code blocks with syntax highlighting.&#10;- Save the &quot;processed&quot; files to an output folder&#10;- Upload (currently to [Github](https://github.com/ruivieira/ruivieira.github.io) to be served via Github pages). The HTML fragments are minimal, for instance: ```html&#10;&lt;h1&gt;A post-modern title&lt;/h1&gt; &lt;p&gt;Yes, this could be an entire blog post.&lt;/p&gt;&#10;``` The point is, where do we draw the line on _what_ a static generator is? For this post, I won&apos;t consider a loose collection&#10;of specialised scripts to be a static generator. There is no configuration, no convention, no theming ability [^1]. You can argue that this is what many generators do, but I think that&apos;s beyond the scope of this short post. [^1]: Apart from plain CSS theming, that is. Since my static blog is straightforward, with minimal markup, why not create something equally simple for RSS generation?&#10;To do so, I&apos;ve decided to go the way of &quot;handcrafted&quot; HTML. However, I was accustomed to a static site generator to generate some goodies, such as syndication feeds automatically. I&apos;ve decided to add an RSS feed to the site, using minimal dependencies (only shell scripting and a couple of universal user-land tools such as `grep` and `cat`). This approach has the added benefit that it is applicable to expose other types of data as an RSS feed, such as server and periodic job logs. We start by adding the feed header to the `index.xml`: ```bash&#10;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&#10;&lt;rss version=&quot;2.0&quot;&gt; &lt;channel&gt; &lt;title&gt;Rui Vieira&apos;s blog&lt;/title&gt; &lt;description&gt;Rui Vieira&apos;s personal blog&lt;/description&gt; &lt;link&gt;https://ruivieira.dev/&lt;/link&gt; &lt;lastBuildDate&gt;$(date)&lt;/lastBuildDate&gt;&#10;EOF&#10;``` The [RSS 2.0 specification](https://validator.w3.org/feed/docs/rss2.html) is quite simple in terms of the minimum requirements for a valid feed.&#10;The mandatory &quot;header&quot; fields are: - `title`, the name of the channel.&#10;- `link`, the URL to the HTML website corresponding to the channel.&#10;- `description`, phrase or sentence describing the channel. In terms of feed items, according to the specification, at _least_ one of title or description must be present, and all remaining elements are optional. We use the following in this feed: - `title`, the title of the item.&#10;- `link`, the URL of the item.&#10;- `pubDate` indicates when the item was published. `pubDate` needs to conform with [RFC 822](https://tools.ietf.org/html/rfc822). &gt; Just as interesting tidbit, RFC 822 (which defines Internet Text Message formats) is one of the core email RFCs.&#10;&gt; It predates [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) by six years (1982) and it&apos;s itself based on 1977&apos;s&#10;&gt; [RFC 733](https://tools.ietf.org/html/rfc733). We then loop over all the input files to build the RSS entries. ```bash&#10;FILES=input/*.html&#10;for FILE in $FILES&#10;do FILENAME=&quot;${FILE##*/}&quot; FILENAME=&quot;${FILENAME%.*}&quot; # extract title ... # write entry to index.xml&#10;done&#10;``` ## Using Bash First, extract the title. The actual title is not inside the `&lt;title&gt;` tag, but on the first header `&lt;h1&gt;`. ```bash&#10;cat output/nb-estimation.html | grep -E &quot;&lt;h1.*&gt;(.*?)&lt;/h1&gt;&quot; | sed &apos;s/.*&lt;h1.*&gt;\\(.*\\)&lt;\\/h1&gt;.*/\\1/&apos;&#10;``` The first produces: ```html&#10;&lt;div id=&quot;main&quot;&gt; &lt;h1 id=&quot;negative-binomial-estimation&quot;&gt;Negative Binomial estimation&lt;/h1&gt;&#10;&lt;/div&gt;&#10;``` While the second produces: ```text&#10;Negative Binomial estimations&#10;``` Now, what happens if we have more than one `&lt;h1&gt;` header? UNIX pipelines to the rescue.&#10;We simple retrieve the _first_ line of the matching grep, by inserting a `head -1`. To get the modified date of `$FILE` we can use: ```bash&#10;date -r $FILE.html&#10;``` The final RSS feed build is: ```bash&#10;cat &gt;output/index.xml &lt;&lt;EOF&#10;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&#10;&lt;rss version=&quot;2.0&quot;&gt; &lt;channel&gt; &lt;title&gt;Rui Vieira&apos;s blog&lt;/title&gt; &lt;description&gt;Rui Vieira&apos;s personal blog&lt;/description&gt; &lt;link&gt;https://ruivieira.dev/&lt;/link&gt; &lt;lastBuildDate&gt;$(date)&lt;/lastBuildDate&gt;&#10;EOF FILES=input/*.html&#10;for FILE in $FILES&#10;do FILENAME=&quot;${FILE##*/}&quot; FILENAME=&quot;${FILENAME%.*}&quot; TITLE=$(cat output/$FILENAME.html | grep -E &quot;&lt;h1.*&gt;(.*?)&lt;/h1&gt;&quot; | head -1 | sed &apos;s/.*&lt;h1.*&gt;\\(.*\\)&lt;\\/h1&gt;.*/\\1/&apos;) cat &gt;&gt;output/index.xml &lt;&lt;EOF &lt;item&gt; &lt;title&gt;$TITLE&lt;/title&gt; &lt;link&gt;https://ruivieira.dev/$FILENAME.html&lt;/link&gt; &lt;pubDate&gt;$(date -r output/$FILENAME.html)&lt;/pubDate&gt; &lt;/item&gt;&#10;EOF&#10;done cat &gt;&gt;output/index.xml &lt;&lt;EOF &lt;/channel&gt;&#10;&lt;/rss&gt;&#10;EOF&#10;``` ## Using Python Another possibility is to use a specialised tool to extract an RSS item from an HTML file.&#10;To do so, we need to parse the necessary data and replace the extraction part of the loop.&#10;This is, after all, along the lines of the [Unix philosophy](https://en.wikipedia.org/wiki/Unix_philosophy):&#10;create specialised tools with a focus on modularity and reusability. To do, we create a simple script called `post_title.py`. It uses the _Beautiful Soup_ library, which you can install&#10;using: ```shell&#10;$ pip install beautifulsoup4&#10;``` The script reads an HTML file, extract the title and return: ```python&#10;from bs4 import BeautifulSoup&#10;import sys with open(sys.argv[1], &apos;r&apos;) as file: data = file.read() soup = BeautifulSoup(data, features=&quot;html.parser&quot;) print(soup.h1.string)&#10;``` This script can now be used to replace the title extraction: ```bash hl_lines=&quot;18&quot;&#10;cat &gt;output/index.xml &lt;&lt;EOF&#10;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&#10;&lt;rss version=&quot;2.0&quot;&gt; &lt;channel&gt; &lt;title&gt;Rui Vieira&apos;s blog&lt;/title&gt; &lt;description&gt;Rui Vieira&apos;s personal blog&lt;/description&gt; &lt;link&gt;https://ruivieira.dev/&lt;/link&gt; &lt;lastBuildDate&gt;$(date)&lt;/lastBuildDate&gt;&#10;EOF FILES=input/*.html&#10;for FILE in $FILES&#10;do FILENAME=&quot;${FILE##*/}&quot; FILENAME=&quot;${FILENAME%.*}&quot; cat &gt;&gt;output/index.xml &lt;&lt;EOF &lt;item&gt; &lt;title&gt;$(post_title.py $FILENAME.html)&lt;/title&gt; &lt;link&gt;https://ruivieira.dev/$FILENAME.html&lt;/link&gt; &lt;pubDate&gt;$(date -r output/$FILENAME.html)&lt;/pubDate&gt; &lt;/item&gt;&#10;EOF&#10;done cat &gt;&gt;output/index.xml &lt;&lt;EOF &lt;/channel&gt;&#10;&lt;/rss&gt;&#10;EOF&#10;``` The reason why the whole RSS feed is not generated in Python is to have the title extraction as a &quot;function&quot; which can map to&#10;whichever logic the shell script is using. Hope this could be useful to you.&#10;Happy coding!&#10;");
docs.push({"id":1,"title":"(Semi) handcrafted RSS","url":"/semi-handcrafted-rss.html"});
index.add(2, "---&#10;date: 2016-04-11T22:04:00+01:00&#10;draft: false&#10;url: &quot;/a-gibbs-sampler-in-crystal.html&quot;&#10;--- # A Gibbs Sampler in Crystal Recently, I&apos;ve been following with interest the development of the [Crystal](http://crystal-lang.org/) language. Crystal is a statically type language with a syntax resembling Ruby&apos;s. The main features which drawn me to it were its simple boilerplate-free syntax (which is ideal for quick prototyping), tied with the ability to compile directly to native code along with a dead simple way of creating bindings to existing C code. These features make it quite attractive, in my opinion, for scientific computing. To test it against more popular languages, I&apos;ve decided to run the Gibbs sampling examples created in [Darren Wilkinson&apos;s blog](https://darrenjw.wordpress.com/2011/07/16/gibbs-sampler-in-various-languages-revisited/). I recommend reading this post, and in fact, if you are interested in Mathematics and scientific computing in general, I *strongly* recommend you follow the [blog](https://darrenjw.wordpress.com/). As explained in the linked post, I will make a Gibbs sampler for $$&#10;f\\left(x,y\\right)=kx^2\\exp\\left\\lbrace-xy^2-y^2+2y-4x\\right\\rbrace&#10;$$ with $$&#10;\\begin{aligned}&#10;x|y &amp;\\sim Ga\\left(3,y^2+4\\right) \\\\\\\\ y|x &amp;\\sim N\\left(\\frac{1}{1+x},\\frac{1}{2\\left(1+x\\right)}\\right) \\end{aligned}&#10;$$ The original examples were ran again, without any code alterations. I&apos;ve just added the Crystal version. This implementation uses a [very simple wrapper](https://github.com/ruivieira/crystal-gsl) I wrote to the famous [GNU Scientific Language](http://www.gnu.org/software/gsl/) (GSL). ```crystal&#10;require &quot;../libs/gsl/statistics.cr&quot; require &quot;math&quot; def gibbs(n : Int = 50000, thin : Int = 1000) x = 0.0 y = 0.0 puts &quot;Iter x y&quot; (0..n).each do |i| (0..thin).each do |j| x = Statistics::Gamma.sample(3.0, y\\*y+4.0) y = Statistics::Normal.sample(1.0/(x+1.0), 1.0/Math.sqrt(2.0\\*x+2.0)) end puts &quot;#{i} #{x} #{y}&quot; end end gibbs&#10;``` (As you can see, the Crystal code is quite similar to the Python one). To make sure it&apos;s a fair comparison, I ran it in compiled (and optimised) mode build using ```shell&#10;$ crystal build gibbs.cr --release&#10;$ time ./gibbs &gt; gibbs_crystal.csv&#10;``` Looking at the results, you can see that they are consistent with the other implementations: ![Gibbs](./images/gibbs.png) The timings for each of the different versions (ran in a 1.7 GHz Intel Core i7 Macbook Air) were |Language|Time (s)|&#10;|--------|--------|&#10;|R|364.8|&#10;|Python|144.0|&#10;|Scala|9.896|&#10;|Crystal|5.171|&#10;|C|5.038| So there you have it. A Ruby-like language which can easily compete with C performance-wise. I sincerely hope that Crystal gets some traction in the scientific community. That of course won&apos;t depend solely on its merits but rather on an active community along with a strong library ecosystem. This is lacking at the moment, simply because it is relatively new language with the specs and standard library still being finalised.");
docs.push({"id":2,"title":"A Gibbs Sampler in Crystal","url":"/a-gibbs-sampler-in-crystal.html"});
index.add(3, "---&#10;date: 2018-04-22T15:21:00+01:00&#10;draft: false&#10;url: &quot;/a-simple-python-benchmark-exercise.html&quot;&#10;--- # A simple Python benchmark exercise Recently when discussing the Crystal language and specifically the [Gibbs sample blog post](a-gibbs-sampler-in-crystal.html) with a colleague, he mentioned that the Python benchmark numbers looked a bit off and not consistent with his experience of numerical programming in Python. To recall, the numbers were: | Language | Time(s) |&#10;| -------- | ------- |&#10;| R | 364.8 |&#10;| Python | 144.0 |&#10;| Scala | 9.896 |&#10;| Crystal | 5.171 |&#10;| C | 5.038 | To have a better understanding of what is happening, I&apos;ve decided to profile and benchmark that code (running on Python 3.6). The code is the following: ```python&#10;import random, math def gibbs(N=50000, thin=1000): x = 0 y = 0 print(&quot;Iter x y&quot;) for i in range(N): for j in range(thin): x = random.gammavariate(3, 1.0 / (y y + 4)) y = random.gauss(1.0 / (x + 1), 1.0 / math.sqrt(2 x + 2)) print(i,x,y) if __name__ == &quot;main&quot;: gibbs()&#10;``` Profiling this code with `cProfile` gives the following results: | Name | Call count | Time (ms) | Percentage |&#10;| ----------------------------------------------- | ---------- | --------- | ---------- |&#10;| gammavariate | 50000000 | 141267 | 52.1% |&#10;| gauss | 50000000 | 65689 | 24.2% |&#10;| `&lt;built-in method math.log&gt;` | 116628436 | 18825 | 6.9% |&#10;| `&lt;method &apos;random&apos; of &apos;_random.Random&apos; objects&gt;` | 170239973 | 17155 | 6.3% |&#10;| `&lt;built-in method math.sqrt&gt;` | 125000000 | 12352 | 4.6% |&#10;| `&lt;built-in method math.exp&gt;` | 60119980 | 7276 | 2.7% |&#10;| `&lt;built-in method math.cos&gt;` | 25000000 | 3338 | 1.2% |&#10;| `&lt;built-in method math.sin&gt;` | 25000000 | 3336 | 1.2% |&#10;| `&lt;built-in method builtins.print&gt;` | 50001 | 1030 | 0.4% |&#10;| gibbs.py | 1 | 271396 | 100.0% | The results look different than the original ones on account of being performed on a different machine. However, we will just look into the relative code performance between different implementations and whether the code itself has room for optimisation. Surprisingly, the console I/O took a much smaller proportion of the execution time than I expected (0.4%). On the other hand, as expected, the bulk of the execution time is spent on the `gammavariate` and `gauss` methods. These methods, however, are provided by the Python&apos;s standard library `random`, which underneath makes heavy usage of `C` code (mainly by [usage](https://github.com/python/cpython/blob/master/Lib/random.py#L35) of the `random()` function). For the second run of the code, I&apos;ve decided to use `numpy` to sample from the Gamma and Normal distributions. The new code, `gibbs_np.py`, is provided below. ```python&#10;import numpy as np&#10;import math def gibbs(N=50000, thin=1000): x = 0 y = 0 print(&quot;Iter x y&quot;) for i in range(N): for j in range(thin): x = np.random.gamma(3, 1.0 / (y y + 4)) y = np.random.normal(1.0 / (x + 1), 1.0 / math.sqrt(2 x + 2)) print(i,x,y) if __name__ == &quot;main&quot;: gibbs()&#10;``` We can see from the plots below that the results from both modules are identical. ![Gibbs modules](./images/gibbs_modules.png) The profiling results for the `numpy` version were: | Name | Call count | Time (ms) | Percentage |&#10;| --------------------------------------------------- | ---------- | --------- | ---------- |&#10;| \\&lt;method &apos;gamma&apos; of &apos;mtrand.RandomState&apos; objects\\&gt; | 50000000 | 121211 | 45.8% |&#10;| \\&lt;method &apos;normal&apos; of &apos;mtrand.RandomState&apos; objects\\&gt; | 50000000 | 83092 | 31.4% |&#10;| \\&lt;built-in method math.sqrt\\&gt; | 50000000 | 6127 | 2.3% |&#10;| \\&lt;built-in method builtins.print\\&gt; | 50001 | 920 | 0.3% |&#10;| gibbs_np.py | 1 | 264420 | 100.0% | A few interesting results from this benchmark were the fact that using `numpy` or `random` didn&apos;t make much difference overall (264.4 and 271.3 seconds, respectively). This is despite the fact that, apparently, the Gamma sampling seems to perform better in `numpy` but the Normal sampling seems to be faster in the `random` library. You will notice that we&apos;ve still used Python&apos;s built-in `math.sqrt` since it is known that for scalar usage it out-performs `numpy`&apos;s equivalent. Unfortunately, in my view, we are just witnessing a fact of life: \\*Python is not the best language for number crunching\\*. Since the bulk of the computational time, as we&apos;ve seen, is due to the sampling of the Normal and Gamma distributions, it is clear that in our code there is little room for optimisation except the sampling methods themselves. A few possible solutions would be to: - Convert the code to `Cython`&#10;- Use FFI to call a highly optimised native library which provides Gamma and Normal distributions (such as [GSL](https://www.gnu.org/software/gsl/)) Nevertheless, personally I still find Python a great language for quick prototyping of algorithms and with an excellent scientific computing libraries ecosystem. Keep on _Pythoning_.&#10;");
docs.push({"id":3,"title":"A simple Python benchmark exercise","url":"/a-simple-python-benchmark-exercise.html"});
index.add(4, "---&#10;date: 2017-12-18T12:36:00+00:00&#10;draft: false&#10;url: &quot;/a-streaming-als-implementation.html&quot;&#10;--- # A streaming ALS implementation In this blog post I would like to talk a little bit about recommendation engines in general and how to build a streaming recommendation engine on top of [Apache Spark](https://spark.apache.org/). I will start by introducing the concept of [collaborative filtering](#collab), and focus in two variants: [batch](#batch_als) and [streaming](#streaming_als) Alternating Least Squares (ALS). I will look at the principles of a streaming distributed recommendation engine on Spark and finally, I&apos;ll talk about practical issues when using these methods. ## Recommendation engines So what *are* &quot;recommendation engines&quot;? Recommendation engines are a popular method to match *users*, *products* and historical *data* on user behaviour. ## Collaborative filtering In the majority of cases, we assume there&apos;s a unique mapping between a *user* $x$, a *product* $y$ and *rating* $\\mathsf{R}_{x,y}$. $$&#10;\\left(x,y\\right) \\mapsto \\mathsf{R}_{x,y}&#10;$$ The &quot;collaborative&quot; aspect refers to the fact that we are using collective information from a group of users and &quot;filtering&quot; is simply a synonym for &quot;prediction&quot;. So, we use collaborative filtering quite frequently in our daily life and it really seems like common sense. The main principle is that if a group of people tend to collectively have similar tastes, it is more likely that they agree on an unknown product. Let&apos;s imagine that you have a number of friends with whom you share a very similar musical taste, let&apos;s call it `A` and another group, `B`, compared to which you have very different musical tastes. If group `A` and group `B` both recommend you a new album which they regard highly, which one would you pick? You will probably pick the album from group `A`, right? So that&apos;s collaborative filtering in a nutshell. ![Data](./images/als/groups.png) *Bonus question*: what if an album is considered really bad by group `B`? Does it mean you&apos;ll like it? It&apos;s difficult to tell. Because group `A` has relevance to you, it&apos;s easy to match. Because `B` is too dissimilar, a low rating is not very informative. ## Alternating Least Squares (ALS) One of the most popular collaborative filtering methods is [Alternating Least Squares](https://link.springer.com/chapter/10.1007/978-3-540-68880-8_32) (ALS). In ALS we assume that the available rating data can be represented in a sparse matrix form, that is, we will assume a sequential ordering of both users and products. Each entry of the matrix will then represent the rating for a unique pair of user and products. If we then consider ratings data as a matrix, let&apos;s call it $\\mathsf{R}$, the *user* and *product* ids will represent coordinates in a ratings matrix and the actual rating will be the value for that particular entry. To keep the notation consistent with the above we simply call the entry $(x,y)$ as $\\mathsf{R}_{x,y}$. This will look something like the matrix represented in the figure below. ![Ratings table](./images/als/ratings_table.png) The idea behind ALS is to factorise the ratings matrix $\\mathsf{R}_{x,y}$ into two matrices $\\mathsf{U}$ and $\\mathsf{P}$, which in turn, when multiplied back, will return an approximation of the original ratings matrix, that is: $$&#10;\\mathsf{R} \\approx \\hat{\\mathsf{R}} = \\mathsf{U}^T \\mathsf{P}&#10;$$ To &quot;predict&quot; a missing rating for a user $x$ and product $y$, we can simply multiply two vectors, namely the $x$ row from the user latent factors and the $y$ column from the product latent factors, $\\hat{\\mathsf{R}}_{x,y}$, that is: $$&#10;\\hat{\\mathsf{R}}_{x,y} = \\mathsf{U}_x^T \\mathsf{P}_y&#10;$$ There are several ways to tackle this factorisation problem and we will cover two of them in here. We will first look at a [batch method](#batch_als), which aims at factorising using the whole of the ratings matrix and a [stochastic gradient descent method](#streaming_als), which uses a single observation at a time. ### Batch ALS This factorisation is performed by first defining an (objective) loss function (here called $\\ell$). A general form is represented below where, as before, $\\mathsf{R}_{x,y}$ is the *true rating* and $\\hat{\\mathsf{R}}_{x,y}$ is the *predicted rating*, calculated as seen previously. The remaining terms are simply regularisation terms to help prevent overfitting. $$&#10;\\ell = \\sum c_{x,y} \\left(\\mathsf{R}_{x,y} - \\underbrace{\\mathsf{U}_x^T \\mathsf{P}_y}_{\\hat{\\mathsf{R}}_{x,t}}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right)&#10;$$ The value of $(c_{x,y})$ constitutes a penalisation function and will depend on whether we are considering *explicit* or *implicit* feedback. If we consider the known ratings as our training dataset $\\mathcal{T}$, then, in the case of explicit feedback we have $$&#10;c_{x,y} = \\begin{cases}&#10;0,\\qquad\\text{if}\\ \\left(x,y\\right) \\notin \\mathcal{T} \\\\\\\\&#10;1,\\qquad\\text{if}\\ \\left(x,y\\right) \\in \\mathcal{T}&#10;\\end{cases}&#10;$$ Constraining our loss function to only include known ratings. The implicit feedback case is different (and a possible future topic) and for the remainder of this post we will only consider the explicit feedback case. Given the above, we can then simplify our loss function, in the explicit feedback case, to $$&#10;\\ell = \\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}_{x,y} -\\hat{\\mathsf{R}}_{x,y}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right)&#10;$$ Minimizing $\\ell$ is however an NP-hard problem, due to its non-convexity. However, if we treat $\\mathsf{U}$ as constant, then $\\ell$ is a convex in relation to $\\mathsf{P}$ and if we treat $\\mathsf{P}$ as constant, $\\ell$ is convex in relation to $\\mathsf{U}$. We can then alternate between fixing $\\mathsf{U}$ and $\\mathsf{P}$, changing the values such that the loss function $\\ell$ (above) is minimized. This procedure is then repeated until we reach convergence. The way that ALS works is, in simplified terms, to find the factors $\\mathsf{U}$ and $\\mathsf{P}$, which when multiplied together provide an approximation of our ratings matrix $\\mathsf{R}$, as we&apos;ve seen previously. ![](./images/als/rupt.png) Once we have the factors $\\mathsf{U}$ and $\\mathsf{P}$, we can then predict the missing values in $\\mathsf{R}$ by using the approximation $\\hat{\\mathsf{R}}$. It is clear that in a real world scenario we would have *many* missing ratings, simply due to the assumption that no user rates all products (if they did, the case for a recommendation engine will be significantly weaker). ALS is designed to deal with sparse matrices and to fill the blanks using *predicted values*. After factorization, our approximated ratings matrix will look something like this: ![](./images/als/ratings_table_filled.png) As mentioned previously, the first step is then to minimise the loss function. In this case we take the partial derivatives and set them to zero and fortunately this has a closed form solution. We get a system of linear equations which we can easily implement. The system will correspond to the solution of $$&#10;\\frac{\\partial \\ell}{\\partial \\mathsf{U}_x}=0, \\qquad \\frac{\\partial \\ell}{\\partial \\mathsf{P}_y}=0.&#10;$$ We start by solving the user latent factor minimisation using: $$&#10;\\frac{1}{2}\\frac{\\partial \\ell}{\\partial \\mathsf{U}_x}=0 \\\\\\\\ \\frac{1}{2}\\frac{\\partial}{\\partial \\mathsf{U}_x} \\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}_{x,y} - \\mathsf{U}_x^T \\mathsf{P}_y\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right)=0 \\\\\\\\ -\\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}_{x,y} - \\mathsf{U}_x^T \\mathsf{P}_y\\right)\\mathsf{P}_y^T + \\lambda \\mathsf{U}\\_x^T=0\\\\\\\\ -\\left(\\mathsf{R}_x -\\mathsf{U}_x^T \\mathsf{P}^T\\right)\\mathsf{P} + \\lambda \\mathsf{U}_x^T=0\\\\\\\\ \\mathsf{U}_x^T\\left(\\mathsf{P}^T \\mathsf{P} + \\lambda \\boldsymbol{\\mathsf{I}}\\right) = \\mathsf{R}_x \\mathsf{P} \\\\\\\\ \\mathsf{U}_x^T = \\mathsf{R}_x \\mathsf{P} \\left(\\mathsf{P}^T \\mathsf{P} + \\lambda \\boldsymbol{\\mathsf{I}}\\right)^{-1}.&#10;$$ Similarly, we can solve for the product latent factor by using: $$&#10;\\frac{1}{2}\\frac{\\partial \\ell}{\\partial \\mathsf{P}_y}=0 \\\\\\\\ -\\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}\\_{x,y} - \\mathsf{P}\\_y^T \\mathsf{U}_x\\right)\\mathsf{U}_x^T + \\lambda \\mathsf{P}_y^T=0\\\\\\\\ -\\left(\\mathsf{R}_y - \\mathsf{P}_y^T \\mathsf{U}^T\\right)\\mathsf{U} + \\lambda \\mathsf{P}_y^T=0\\\\\\\\ \\mathsf{P}_y^T\\left(\\mathsf{U}^T \\mathsf{U} + \\lambda \\boldsymbol{\\mathsf{I}}\\right) = \\mathsf{R}_y \\mathsf{U} \\\\\\\\ \\mathsf{P}_y^T = \\mathsf{R}_y \\mathsf{U} \\left(\\mathsf{U}^T \\mathsf{U} + \\lambda \\boldsymbol{\\mathsf{I}}\\right)^{-1}.&#10;$$ We can then calculate each factor iteratively, by fixing the other one and solving the estimator. While this process is alternated, an error measure (usually the *Root Mean Squared Error* [RMSE](error-metrics.html#Root Mean Squared error)), or $RMSE$ is calculated (as below) between the rating matrix approximation given by the latent factors and the ratings which we have, $\\mathcal{T}$. This method is guaranteed to converge and when we consider out approximation to be good enough, or after a set number of iterations we can then stop the refinement. $$&#10;RMSE = \\sqrt{\\frac{1}{n}\\sum_{x,y \\in \\mathcal{T}}\\lvert \\hat{\\mathsf{R}}_{x,y} - \\mathsf{R}_{x,y}\\rvert}&#10;$$ After the latent factors are estimated, we can then use them to try to recreate the original ratings matrix with the approximation as we&apos;ve seen. The missing ratings in the original matrix will now be filled by values which minimize the least squares recursion and these are taken as the ratings &quot;predictions&quot;. To illustrate the working of ALS, let&apos;s assume we have a very quirky shop that only ever sells 300 products and has exactly 300 customers. On top of that, users are allowed to use 8 bit to rate the products. We will also assume in this unusual shop that every user has rated every product. Now we&apos;re humans, and we visualise patterns in colour more easily than in numbers. We will assign a palette to the ratings, so that each rating corresponds to a colour. ![](./images/als/colour_matrix.png) I think you know where this is going ... we make up this final ratings matrix so now we can visualise the ALS progress. ![](./images/als/mona_lisa_pixelated_viridis.png) So how do we perform this factorisation? The initial step is to fill the latent factors $\\mathsf{U}$ and $\\mathsf{P}$) with random values. Since at this point, we assume we don&apos;t have any ratings, having random factors will lead to an initial random guess of the ratings matrix. ![](./images/als/initial.png) We then proceed to calculate each factor matrix, as we&apos;ve seen, by calculating one using the estimator while keeping the other one constant and then alternating. We can see by the movie below that at each iteration the approximation to the original ratings gets better, stabilising after a few steps. This is to be expected, in this case, since this would be the simplest implementation of ALS: a batch ALS on a single machine where we know all the ratings. &lt;video width=&quot;100%&quot; controls loop autoplay&gt;&lt;source src=&apos;./images/als/simple_factorization.mp4&apos; type=&apos;video/mp4&apos;&gt;&lt;/video&gt; So a fair question that arises is: why can&apos;t we update this model and perform recommendations in a streaming fashion using this method? After all, if users add product ratings, we can simply update the predictions by recalculating the factors! The problem is that when a new rating is added, or when new users and new products are added, we need to recalculate the entirety of the $\\mathsf{U}$ and $\\mathsf{P}$ matrices, and to do so, we need to have access to all of the data, $\\mathsf{R}$. ### Streaming ALS Ideally, we want a method that would allow us to update $\\mathsf{U}$ and $\\mathsf{P}$ using one observation, $\\mathsf{R}_{x,y}$ at a time It turns out that the *Stochastic Gradient Descent* (or SGD) method allows us to do precisely that. We&apos;ll look at the specific variant of SGD we&apos;ve used which is called *Bias-Stochastic Gradient Descent* (B-SGD). It is important to keep in mind, under a certain point of view, both methods aim at the same thing. ![](./images/als/batch_streaming_comparison.png) They both try to factorise the ratings matrix as latent factors, which would then be used to perform predictions. The main differences are of course, how the data is used (batch or one observation at the time) and how the factorisation is calculated. In the SGD case we use the concept of biases in both users and items. The bias is a measure of how consistently a product is rated by different users. The bias of rating $(x,y)$, that is the rating given by user $x$ to product $y$, can be calculated as the sum of $\\mu$, an overall average rating and the observed deviations of user $x$, which we call $b_x$, and the observed deviations of product $y$, called $b_y$, that is: $$&#10;b_{x,y} = \\mu + b_x + b_y&#10;$$ This bias information is now incorporated in the rating prediction. We can see that the SGD prediction is simply the batch prediction plus the corresponding bias term $$&#10;\\hat{\\mathsf{R}}_{x,y} = b_{x,y} + \\underbrace{\\mathsf{U}^T_x \\cdot \\mathsf{P}_y}_{batch}&#10;$$ If we take the loss function definition for the batch method (and still considering the explicit feedback case), we can then replace the predicted rating formulation with our new one. We have, as before, some regularisation terms, but now also include a new regularisation term for the bias components, but we don&apos;t need to go into that. $$&#10;\\ell_{SGD} = \\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}_{x,y} - b_{x,y} - \\hat{\\mathsf{R}}_{x,y}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2 + b_x^2 + b_y^2\\right)&#10;$$ Since calculating the full gradient is computationally very expensive, we calculate it for a single observation. As we can see, the SGD method allows us to update the user and product specific bias as well as a single user and product latent factor row given a single rating. Provided we have a single rating, the rating of user $x$ for product $y$, we can update the biases as well as the latent vectors for user $x$ and for product $y$, that is, we no longer need to update the entire matrices $\\mathsf{U}$ and $\\mathsf{P}$, while still maintaining a convergence property. Provided with a learning rate $\\gamma$ and defining our *prediction error* as $$&#10;\\epsilon_{x,y}=\\mathsf{R}_{x,y}-\\hat{\\mathsf{R}}_{x,y},&#10;$$ the biases and latent factors can now be updated in the opposite direction of the calculated gradient, proportionally to the learning rate, such that $$&#10;\\begin{aligned} b_x &amp;\\leftarrow b_x + \\gamma \\left(\\epsilon_{x,y}-\\lambda_x b_x\\right) \\\\\\\\ b_y &amp;\\leftarrow b_y + \\gamma \\left(\\epsilon_{x,y}-\\lambda_y b_y\\right) \\\\\\\\ \\mathsf{U}_x &amp;\\leftarrow \\mathsf{U}_x + \\gamma \\left(\\epsilon_{x,y}\\mathsf{P}_y - \\lambda^\\prime_x \\mathsf{U}_x\\right) \\\\\\\\ \\mathsf{P}_y &amp;\\leftarrow \\mathsf{P}_y + \\gamma \\left(\\epsilon_{x,y}\\mathsf{U}_x - \\lambda^\\prime_y \\mathsf{P}_y\\right) \\end{aligned}&#10;$$ So the practical difference, in terms of streaming data is evident now. Given that, in both methods, the objective is to estimate the latent factors, given the ratings: with batch ALS, whenever we get a new rating, we need to fully recalculate the factors iteratively until we reach convergence. Conversely, with an SGD based factorisation, whenever we have a new rating, we can simply estimate the relevant row and column in the latent factors, by calculating the gradients and adjusting its values. ![](./images/als/batch_streaming_comparison_2.png) Next we show the previous manufactured ratings matrix being factorised using B-SGD. We now simply recalculate the biases and a single latent factor vector, one observation at the time. We can see that, as expected, the convergence is slower (we *are* using a single observation at each step) but in the end, it produces a similar result. &lt;video width=&quot;100%&quot; controls loop autoplay&gt;&lt;source src=&apos;./images/als/sgd_factorization.mp4&apos; type=&apos;video/mp4&apos;&gt;&lt;/video&gt; Now, this works fine for a single machine implementing streaming ALS. But we are interested in scaling this to something larger than this example so we will use a distributed implementation of ALS. And this is were it can start to get tricky. As it is the case with distributed algorithms, there are some pitfalls which we need to avoid in order to have a performant implementation. We will look at a few of these by looking at the Apache Spark&apos;s and its default ALS implementation. ### Apache Spark As probably most of you are familiar with, Spark is an Apache community project which aims at providing a modern platform for distributed computations. Spark provides several core data structures, such as `RDD`s ([Resilient Distributed Datasets](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)), `Dataframes` and `Datasets`. The `RDD` is an immutable, distributed typed collection of objects. The `RDD` is partitioned across the cluster. This allows the spark operations, such as function mapping, to be applied to each subset of the `RDD` in parallel at each partition. ![](./images/als/RDD.png) For the streaming ALS application we will use `RDD`s to implement the algorithm. Spark&apos;s MLlib provides a collaborative filtering implementation based on the distributed batch ALS which we&apos;ve covered previously. The API is quite simple and to train a model we need: ```scala&#10;val model = ALS.train(ratings, rank, iterations, lambda) case class Rating(int user, int product, double rating) val ratings: RDD[Rating]&#10;val rank: int&#10;val iterations: int&#10;val lambda: Double&#10;``` * An `RDD` containing the ratings. The `RDD` has elements of the class `Rating`, which is basically a wrapper around a tuple of user id, product id and rating. This `RDD` corresponds to all the entries in our ratings matrix used previously.&#10;* The rank which corresponds to the number of elements in our latent factor vectors (this would be the number of columns or rows in our $\\mathsf{U}$ and $\\mathsf{P}$ matrices).&#10;* A stopping criteria in terms of iterations for the ALS.&#10;* And finally, we set the `lambda` parameter, a regularisation parameter, which we&apos;ve shown to be a part of the loss function&apos;s regularisation. Since we have the data, the question is then how to choose the parameters. A typical method is to split the original ratings data into two random sets, one for training and one for validation. We then proceed to train the model to several different parameters, usually according to a grid search, and calculate some error measure between the predicted and validation ratings, choosing the parameters which minimise the error. Once the model is trained, we get a `MatrixFactorizationModel` instance, which is basically a wrapper for the latent factors as `RDD`s. ```scala&#10;val model = ALS.train(ratings, rank, iterations, lambda) model: MatrixFactorizationModel class MatrixFactorizationModel { val userFeatures: RDD[(Int, Array[Double])] val productFeatures: RDD[(Int, Array[Double])] }&#10;``` One we have the trained model, we can now perform predictions. ### Streaming data We now want to build a streaming recommender system. For this scenario, we will assume that the observations take the form of a Spark&apos;s *Discretised Stream* or [DStream](https://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams). With DStreams we consume the stream as mini-batches of `RDD`s over a certain interval window. ![](./images/als/dstream.png) We can for instance, use the first mini-batch to initialise the model and the following batches to continuously train the model. One immediate advantage of using observations as a stream is that we no longer need to keep the entirety of the data in memory or read it from storage. If we consider the batch implementation with a very large dataset if we had a single new observation and wanted to retrain the model, we would have to, for instance, read several million ratings from a database. With a streaming variant we can use that single observation to update the latent factors. We will try to recreate Spark&apos;s batch ALS API by allowing model training using a ratings `RDD`, however this time, we consume each `RDD` from the stream mini-batch and will incrementally train the model as observations trickle in. ![](./images/als/dstream_als.png) First, we start by establishing the quantities and data structures needed to implement streaming ALS. We&apos;ve seen in the previous slides that the recursions for the gradient calculation take the following form, here presented in pseudo-code: ```python&#10;userBias += gamma * (error - lambda \\* userBias) userFeature(i) += gamma * (error prodFeature(j) - lambda * userFeature(i))&#10;``` We create a `Factor` class to encapsulate the features and the corresponding bias. We recall that in the Spark ALS implementation, the features were stored in `RDD`s typed as a tuple of `(id, Array)`, where now we have an equivalent form of `(id, Factor)` which allows us to capture the bias. I&apos;ll now provide a quick overview of the steps required to go from the initial ratings stream to the trained model, in terms of Spark&apos;s `RDD` operations. Similarly to Spark&apos;s ALS, we can assume that the model data will be in the form of `Rating`&apos;s `RDD`s. These, as we&apos;ve seen, will correspond to a mini-batch of ratings from our data stream. We first need to create the initial user and product latent factors for the observed data and we would start by creating two separate `RDD`s from the data, one keyed by user id, the other by product id. ![](./images/als/stream_als_step_1.png) For each entry of those new `RDD`s (the user and product indexed ones) we will now generate a random vector of features. This can be done by simply filling a vector of size `rank` with random uniform values, but as you will recall, we now also have a bias associated with each entry, which will initially also be set to a random value. ![](./images/als/stream_als_step_2.png) We now join the incoming ratings, with the generated user factors (using the user id as the key) getting a resulting `RDD` consisting of product ids, user ids, ratings and user factors (and the same thing for products and product factors). ![](./images/als/stream_als_step_3.png) Finally, we have these two joint `RDD`s which have all the necessary quantities needed to calculate the partial gradient in each element. Recalling how to calculate a predicted rating in streaming ALS, we need the global bias, the user and product bias and the corresponding user and product latent vectors. This is straightforward to calculate for each element as we can see from the pseudo-code. Here the `dot` function is simply a function to calculate the dot product treating the two factor arrays as vectors. $$&#10;\\hat{\\mathsf{R}}_{x,y}=\\mu + b_x + b_y + \\mathsf{U}_x^T \\cdot \\mathsf{P}_y&#10;$$ ```scala&#10;prediction = dot(userFactors.features, itemFactors.features) + userFactors.bias + itemFactors.bias + bias&#10;``` Given the prediction, the error is also straightforward to calculate, since the real rating is also included in this `RDD`. $$&#10;\\epsilon_{x,y} = \\mathsf{R}_{x,y} - \\hat{\\mathsf{R}}_{x,y}&#10;$$ ```scala&#10;eps = rating - prediction&#10;``` And now, since we have the error, we can also easily calculate the update term for the user and product features. As mentioned previously, `gamma` and `lambda` are known model parameters which we pick ourselves when instantiating the streaming ALS algorithm. $$&#10;\\gamma \\left(\\epsilon_{x,y}\\mathsf{U}_x-\\lambda^{\\prime}\\mathsf{P}_y\\right)&#10;$$ ```scala&#10;(0 until rank).map { i =&gt; gamma * (eps * userFactors.features(i) - lambda * itemFactors.features(i))&#10;}&#10;``` Finally, we update the user and product biases given the model parameters and the previous bias. $$&#10;\\gamma \\left(\\epsilon_{x,y}-\\lambda_y b_y\\right)&#10;$$ ```scala&#10;gamma * (eps - lambda * itemFactors.bias)&#10;``` These calculated gradients can now be mapped to a new `RDD` which we will use to update the final biases and latent factors. The last step is to split the gradients according to user and product, and finally, when in possession of all the individual gradients, we reduce them into latent factors by performing an aggregated sum for each user and product. So these steps define the entirety of the streaming ALS operation. For each observation window, we calculate the latent factors `RDD`, and on the following window we update these factors, given the current observations. We&apos;ve covered the initialisation case, that is, we assumed the case where the model is not initialised and we received the first mini-batch of ratings. If, on the following window, we receive ratings for previously unseen user or products, the procedure is exactly the same, that is, we generate random factors and update as described. ![](./images/als/stream_als_overview.png) Now I&apos;ll just quickly cover the case where we get some ratings from users or for some product we&apos;ve already seen. For this new set of observations, we proceed exactly as previously, that is, we split the data into separate `RDD`s, each one containing the ratings but keyed by user and product id. I&apos;ll assume that we get a mixture of completely new data, that is, unseen user and products and some ratings for previously seen users and products shown in red. ![](./images/als/stream_als_post_step_1.png) The difference now is that, instead of assigning random factors and biases to each entry of these `RDD`s, we perform a full outer join between them and the current latent factors. The strategy is then to keep the matching existing latent factor and create random features and biases just for the user and product entries we haven&apos;t seen before. Now that we are in possession of this joint `RDD`, we can apply exactly the same steps as previously to update the factors and repeat these steps for all future incoming observations, allowing us to continuously update the model. It is now easy to see that, in the limit situation where we only have one new rating, we would now only have to update a single entry of the latent factors `RDD`, in contrast with the batch method, where the entirety of the factors would be used in the ALS step. ![](./images/als/stream_als_post_step_2.png) Let&apos;s look at some results comparing the streaming implementation with the Spark&apos;s batch implementation. The dataset we have chosen to use in these tests is one of the [MovieLens&apos; datasets](https://grouplens.org/datasets/movielens/latest/). These dataset are a widely used data in recommendation engine research. They are managed by the Lens corporation and are freely available for non-commercial applications. These datasets come in several variants, namely a [small](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html) variant, useful for a quick algorithm prototyping and testing, and a [full](http://files.grouplens.org/datasets/movielens/ml-latest-README.html) variant, with approximately 26 million ratings (from 45,000 movies and 270,000 users), useful for a more comprehensive testing and benchmark. The data is available as a set of Comma Separated Value files, each containing different variables, but we are mainly interested in the `ratings` file which contains four variables: a unique user and movie id, represented as integers, a rating represented by a value from 0 to 5 with steps of 0.5 and a timestamp for when the movie was rated by this user. First, we&apos;ll start by training a batch ALS model using the MovieLens data. We assume that we already have the observations as an `RDD` of ratings and simply split the data into 80% for training and 20% for validation. ```scala&#10;val split: Array[RDD[Rating]] = ratings.randomSplit(0.8, 0.2) val model = ALS.train(split(0), rank, iter, lambda)&#10;``` Here, we won&apos;t show the steps to determine the best parameters for this dataset, were we performed a simple parameter grid search over a number of possible candidates. The Spark ALS API is quite simple and to train the model we simply pass the training `RDD` and the parameters. We can now use the remaining 20% of the observations to calculate the RMSE between the model predictions and the actual ratings. We now can persist the validation `RDD`, so we can use the exact same one for the streaming ALS run. ```scala&#10;val predictions: RDD[Rating] = model .predict(split(1).map { x =&gt;(x.user, x.product)) } val pairs = predictions .map(x =&gt; ((x.user, x.product), x.rating)) .join( split(1) .map(x =&gt; ((x.user, x.product), x.rating)) .values val RMSE = math.sqrt( pairs.map(x =&gt; math.pow(x._1 - x._2, 2)).mean())&#10;``` In order to test the streaming version, we first need to define a data source. We start with the original MovieLens data and remove all the ratings from the validation observations. We then create a simulated stream of observations using [Kafka](https://kafka.apache.org/), with an interval of 5 seconds and with 1000 observations in each mini-batch. These are arbitrary numbers, chosen just for practical reasons. We could have, for instance, a single observation in each mini-batch. It is not guaranteed that the best parameters (namely `rank` and `lambda`) chosen for the batch version are the best for the streaming implementation, however we&apos;ve decided to use the same ones. For each mini-batch we then incrementally train the model and calculate the RMSE up to that point. Given the actual ratings in the validation set and the model&apos;s prediction, the RMSE calculation is the same as in the batch version. And looking at the results, we can see that with each mini-batch (of 1000 observations), the RMSE from the streaming version (in blue) is edging towards the batch value (plotted as the horizontal dashed line). ![](./images/als/mse_comparison.png) ### Caveats However, streaming ALS has pitfalls which we have to take into account. #### Cold start An issue, which is shared with batch ALS, is usually called the *cold start* problem. This refers to initial point in a recommender engine where we have too few observations to make meaningful predictions. As we now know, when having a small number of ratings, since our latent factors are initialised to random numbers, most of our predicted ratings will also be random. Although this is not an exclusive problem to the streaming implementation, we might be tempted, since the system is suited for realtime recommendations, to immediately start serving predictions. It might be wise to exercise caution and train the model offline with a larger dataset or at least perform some model diagnostics to check how sensible our predictions are. #### Hyperparameter estimation Another challenge we encounter is hyperparameter estimation. In the batch ALS case, we can perform a grid search for instance and estimate the hyperparameters. If, after some time, we find ourselves with a new ratings or even new product and users, we can simply repeat this procedure using the totality of the data. As an example, if in batch ALS at any point we wish to estimate the model with a different rank, this would be perfectly acceptable. ![](./images/als/hyperparameters.png) In the streaming case, we can&apos;t do that. When we have a new batch of observations, we assume that previous ones were discarded since they are already incorporated in the latent factors. If they weren&apos;t and we keep all the observations in the stream, we might as well use batch ALS. ![](./images/als/hyperparameters_2.png) A solution is to perform a grid search in parallel from the start and prune the least performant models as time progresses. This has the disadvantage of being expensive in terms of resources, since we have to keep several models simultaneously and again, we have the cold start problem surfacing. This means that we have no guarantee that the best parameters for a initial batch with few observations will still be the best further on. ![](./images/als/hyperparameters_3.png) #### Performance Also, there are some performance considerations. As we&apos;ve seen, we implement some operations which can be costly in a Spark setting. We have several join operations which can lead to a considerable amount of data shuffling between partitions. Care must be taken into choosing an appropriate partitioning strategy to minimise data shuffling. Spark&apos;s implementation of batch ALS uses a specific method called blocked ALS, which computes outgoing and ingoing links between user and products vectors and then partitioning them in blocks in order to minimise data transfer between nodes. ![](./images/als/partitioning.png) Also, to make predictions we might have to try and perform random access to the latent factors `RDDs`. This also can be quite inefficient since we are using `lookup` methods. If you want to get straight setting up your own distributed recommendation engine, I highly suggest you start with Spark&apos;s builtin solution. I would highly recommended looking at the `jiminy` project (part of the [radanalytics.io](https://radanalytics.io/) community), a micro-service oriented complete recommendation engine, ready to deploy on [OpenShift](https://www.openshift.com/). The engine is split into services such as a [predictor](https://github.com/radanalyticsio/jiminy-predictor) and a [modeler](https://github.com/radanalyticsio/jiminy-modeler), along with a [front-end](https://github.com/radanalyticsio/jiminy-html-server) and [tools](https://github.com/radanalyticsio/jiminy-tools) to simplify tasks (such as using the MovieLens data) and it&apos;s a great way to look at how to put a modern recommender engine together and also a great code read.");
docs.push({"id":4,"title":"A streaming ALS implementation","url":"/a-streaming-als-implementation.html"});
index.add(5, "# Ansible Ansible notes. ## Reference ");
docs.push({"id":5,"title":"Ansible","url":"/ansible.html"});
index.add(6, "---&#10;date: 2016-11-29T20:02:00+00:00&#10;draft: false&#10;url: &quot;/bayesian-estimation-of-changepoints.html&quot;&#10;--- # Bayesian estimation of changepoints A common introductory problem in Bayesian changepoint detection is the record of UK coal mining disasters from 1851 to 1962. More information can be found in [Carlin, Gelfand and Smith](http://www.jstor.org/stable/2347570) (1992). As we can see from the plot below, the number of yearly disasters ranges from 0 to 6 and we will assume that at some point within this time range a change in the accident rate has occured. ![Data](./images/coal_disasters_data.png) The number of yearly disasters can be modelled as a Poisson with a unknown rate depending on the changepoint $k$: $$&#10;y_t \\sim \\text{Po}\\left(\\rho\\right),\\qquad \\rho = \\begin{cases}&#10;\\mu, &amp; \\text{if}\\ t=1,2,\\dots,k \\\\\\\\&#10;\\lambda, &amp; \\text{if}\\ t = k +1, k + 2, \\dots,m&#10;\\end{cases}&#10;$$ Our objective is to estimate in which year the change occurs (the changepoint $k$) and the accident rate before ($\\mu$) and after ($\\lambda$) the changepoint amounting to the parameter set $\\Phi = \\left\\lbrace\\mu,\\lambda,k\\right\\rbrace$. We will use [Crystal](https://crystal-lang.org/) (with [crystal-gsl](https://github.com/ruivieira/crystal-gsl)) to perform the estimation. We start by placing independent priors on the parameters: - $k \\sim \\mathcal{U}\\left(0, m\\right)$&#10;- $\\mu \\sim \\mathcal{G}\\left(a_1, b_1\\right)$&#10;- $\\lambda \\sim \\mathcal{G}\\left(a_2, b_2\\right)$ For the remainder we&apos;ll set $a_1=a_2=0.5$, $c_1=c_2=0$ and $d_1=d_2=1$. The joint posterior of $\\Phi$ is then: $$&#10;\\pi\\left(\\Phi|Y\\right) \\propto p\\left(Y|\\Phi\\right) \\pi\\left(k\\right) \\pi\\left(\\mu\\right) \\pi\\left(\\lambda\\right),&#10;$$ where the likelihood is $$&#10;\\begin{aligned}&#10;p\\left(Y|\\Phi\\right) &amp;= \\prod_{i=1}^{k} p\\left(y_i|\\mu,k\\right) \\prod_{i=k+1}^{m} p\\left(y_i|\\lambda,k\\right) \\\\\\\\&#10;&amp;= \\prod_{i=1}^{k} \\frac{\\mu^{y_i}e^{-\\mu}}{y_i!} \\prod_{i=k+1}^{m} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!}.&#10;\\end{aligned}&#10;$$ As such, the full joint posterior can be written as: $$&#10;\\begin{aligned}&#10;\\pi\\left(\\Phi|Y\\right) &amp;\\propto \\prod_{i=1}^{k} \\frac{\\mu^{y_i}e^{-\\mu}}{y_i!} \\prod_{i=k+1}^{m} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\left(\\mu^{a_1-1} e^{-\\mu b_1}\\right) \\left(\\lambda^{a_2-1} e^{-\\lambda b_2}\\right) \\frac{1}{m} \\\\\\\\&#10;&amp;= \\mu^{a_1 + \\sum_{1}^{k}y_i - 1}e^{-\\mu\\left(k+b\\_1\\right)} \\lambda^{a_2 + \\sum_{k+1}^{m}y_i - 1}e^{-\\lambda\\left(m-k+b_2\\right)}&#10;\\end{aligned}.&#10;$$ It follows that the full conditionals are, for $\\mu$: $$&#10;\\begin{aligned}&#10;\\pi\\left(\\mu|\\lambda,k,Y\\right) &amp;\\propto \\mu^{a_1 + \\sum_{i=1}^{k}y_i-1}e^{-\\mu\\left(k+b_1\\right)} \\\\\\\\&#10;&amp;= \\mathcal{G}\\left(a_1+\\sum_{i=1}^{k}y_i, k + b_1\\right)&#10;\\end{aligned}&#10;$$ We can define the $\\mu$ update as: ```crystal&#10;def mu_update(data : Array(Int), k : Int, b1 : Float64) : Float64 Gamma.sample(0.5 + data[0..k].sum, k + b1) end&#10;``` The full conditional for $\\lambda$ is: $$&#10;\\begin{aligned}&#10;\\pi\\left(\\lambda|\\mu,k,Y\\right) &amp;\\propto \\lambda^{a_2 + \\sum_{i=k+1}^{m}y_i-1}e^{-\\lambda\\left(m-k+b_2\\right)} \\\\\\\\&#10;&amp;= \\mathcal{G}\\left(a_2+\\sum_{i=k+1}^{m}y_i, m - k + b_2\\right),&#10;\\end{aligned}&#10;$$ which we implement as: ```crystal&#10;def lambda_update(data : Array(Int), k : Int, b2 : Float64) : Float64 Gamma.sample(0.5 + data[(k+1)..M].sum, M - k + b2) end&#10;``` The next step is to take $$&#10;\\begin{aligned}&#10;b_1 &amp;\\sim \\mathcal{G}\\left(a_1 + c_1,\\mu + d_1\\right) \\\\\\\\&#10;b_2 &amp;\\sim \\mathcal{G}\\left(a_2 + c_2,\\lambda + d_2\\right),&#10;\\end{aligned}&#10;$$ which we will implement as: ```crystal&#10;def b1_update(mu : Float64) : Float64 Gamma.sample(0.5, mu + 1.0) end def b2_update(lambda : Float64) : Float64 Gamma.sample(0.5, lambda + 1.0) end&#10;``` And finally we choose the next year, $k$, according to $$&#10;p\\left(k|Y,\\Phi\\right)=\\frac{L\\left(Y|\\Phi\\right)}{\\sum_{k^{\\prime}} L\\left(Y|\\Phi^{\\prime}\\right)}&#10;$$ where $$&#10;L\\left(Y|\\Phi\\right) = e^{\\left(\\lambda-\\mu\\right)k}\\left(\\frac{\\mu}{\\lambda}\\right)^{\\sum_i^k y_i}&#10;$$ implemented as ```crystal&#10;def l(data : Array(Int), k : Int, lambda : Float64, mu : Float64) : Float64 Math::E**((lambda - mu)*k) * (mu / lambda)**(data[0..k].sum) end&#10;``` So, let&apos;s start by writing our initials conditions: ```crystal&#10;iterations = 100000 b1 = 1.0&#10;b2 = 1.0 M = data.size # number of data points # parameter storage mus = Array(Float64).new(iterations, 0.0) lambdas = Array(Float64).new(iterations, 0.0) ks = Array(Int32).new(iterations, 0)&#10;``` We can then cast the priors: ```crystal&#10;mus[0] = Gamma.sample(0.5, b1) lambdas[0] = Gamma.sample(0.5, b2) ks[0] = Random.new.rand(M)&#10;``` And define the main body of our Gibbs sampler: ```crystal&#10;(1...iterations).map { |i| k = ks[i-1] mus[i] = mu_update(data, k, b1) lambdas[i] = lambda_update(data, k, b2) b1 = b1_update(mus[i]) b2 = b2_update(lambdas[i]) ks[i] = Multinomial.sample((0...M).map { |kk| l(data, kk, lambdas[i], mus[i]) }) }&#10;``` Looking at the results, we see that the mean value of $k$ is 38.761, which seems to indicate that the change in accident rates occurred somewhere near $1850+38.761\\approx 1889$. We can visually check this by looking at the graph below. Also plotted are the density for the accident rates before ($\\mu$) and after ($\\lambda$) the change. ![Coal disasters years](./images/coal_disasters_years_gibbs.png) ![Mu, Lambda Gibbs](./images/coal_disasters_mu_lambda_gibbs.png) Of course, one the main advantages of implementing the solution in Crystal is not only the boilerplate-free code, but the execution speed. Compared to an equivalent implementation in `R` the Crystal code executed roughly 17 times faster. |Language|Time (s)|&#10;|--------|--------|&#10;|R|58.678|&#10;|Crystal|3.587|");
docs.push({"id":6,"title":"Bayesian estimation of changepoints","url":"/bayesian-estimation-of-changepoints.html"});
index.add(7, "# Brutalist web design The major guidelines of the Brutalist web design[^brutalist] are: [^brutalist]: [https://brutalist-web.design/](https://brutalist-web.design/) * Content is readable on all reasonable screens and devices.&#10;* Only hyperlinks and buttons respond to clicks.&#10;* Hyperlinks are underlined and buttons look like buttons.&#10;* The back button works as expected.&#10;* View content by scrolling.&#10;* Decoration when needed and no unrelated content.&#10;* Performance is a feature.");
docs.push({"id":7,"title":"Brutalist Web Design","url":"/brutalist-web-design.html"});
index.add(8, "# Clojure Notes on Clojure. ## Reference ### Concatenating strings ```clojure&#10;(require &apos;\\[clojure.string :as string\\]) (string/join [&quot;foo&quot; &quot;bar&quot;])&#10;``` ### List files recursively ```clojure&#10;(file-seq &quot;/etc&quot;)&#10;``` (Compare with the [Java](java.html#List files recursively) version.) Filter by extension: ```clojure&#10;(filter #(.endsWith (.toString %) &quot;.conf&quot;) (file-seq &quot;/etc&quot;)))&#10;``` ### Get home directory ```clojure&#10;def home (System/getProperty &quot;user.home&quot;))&#10;```");
docs.push({"id":8,"title":"Clojure","url":"/clojure.html"});
index.add(9, "---&#10;date: 2018-05-12T19:21:00+01:00&#10;draft: false&#10;url: &quot;/containerised-streaming-data-generation-using-state-space-models.html&quot;&#10;--- # Containerised Streaming Data Generation using State-Space Models To prototype and test almost any application some type of input data is needed. Getting the right data can be difficult for several reasons, including strict licenses, a considerable amount of data engineering to shape the data to our requirements and the setup of dedicated data producers. Additionally, in modern applications, we are often interested in realtime/streaming and distributed processing of data with platforms such as [Apache Kafka](https://kafka.apache.org/) and [Apache Spark](https://spark.apache.org/) and deployment in a cloud environment like [OpenShift](https://www.openshift.com/) with tools such as [oshinko](https://radanalytics.io/). Simulating data is not trivial, since we might want to capture complex characteristic to evaluate our algorithms in conditions similar to the real world.&#10;In this post I&apos;ll introduce a tool, [timeseries-mock](https://github.com/ruivieira/timeseries-mock), which allows for a simple, containerised deployment of a data simulator along with some of the theory behind the data generation. # State-space models A common way of modelling these patterns is to use state-space models (SSM). SSMs can be divided into a model and a observation structure. $$&#10;Y_t|\\theta_t,\\Phi \\sim f\\left(y_t|\\theta_t,\\Phi_t\\right) \\\\&#10;\\theta_t|\\theta_{t-1},\\Phi_t \\sim g\\left(\\theta_t|\\theta_{t-1},\\Phi_t\\right).&#10;$$ ![](./images/ssm/ssm_diagram.png) It is clear from the above that the state possesses a Markovian nature. The state at time $t$, $\\theta_t$ will on depend on the previous value, $\\theta_{t-1}$ and an observation at time $t$, $y_t$ will only depend on the current state, $\\theta_t$, that is: $$&#10;p\\left(\\theta_{t}|\\theta_{0:t-1},y_{0:t-1}\\right)=p\\left(\\theta_{t}|\\theta_{t-1}\\right) \\\\&#10;p\\left(\\theta_{t-1}|\\theta_{t:T},y_{t:T}\\right)=p\\left(\\theta_{t-1}|\\theta_{t}\\right) \\\\&#10;p\\left(y_{t}|\\theta_{0:t},y_{0:t-1}\\right)=p\\left(y_{t}|\\theta_{t}\\right).&#10;$$ In this post we will focus on a specific instance of SSMs, namely Dynamic Generalised Linear Models (DGLMs). If you want a deeper theoretical analysis of DGLMs I strongly recommend Mike West and Jeff Harrison&apos;s &quot;[Bayesian Forecasting and Dynamic Models](https://www.springer.com/gb/book/9780387947259)&quot; (1997).&#10;In DGLMs, the observation follows a distribution from the exponential family, $E\\left(\\cdot\\right)$ such, that $$&#10;Y_t|\\theta_t,\\Phi \\sim E\\left(\\eta_t,\\Phi\\right) \\\\&#10;\\eta_t|\\theta_t = L\\left(\\mathsf{F}^T \\theta_t\\right)&#10;$$ where $L\\left(\\cdot\\right)$ is the linear predictor and the state evolves according to a multivariate normal (MVN) distribution: $$&#10;\\theta_t \\sim \\mathcal{N}\\left(\\theta_t;\\mathsf{G}\\theta_{t-1},\\mathsf{W}\\right)&#10;$$ ## Structure The fundamental way in which `timeseries-mock` works is by specifying the underlying structure and observational model in a YAML configuration file. In the following sections we will look at the options available in terms of structural and observational components and look at how to represent them. As we&apos;ve seen from (5), the structure will allows us to define the underlying patterns of the state evolution $\\lbrace \\theta_1, \\theta_2, \\cdots, \\theta_t\\rbrace$. One of the advantages of DGLMs is the ability to compose several simpler components into a single complex structure. We will then look at some of these &quot;fundamental&quot; components. ### Mean&#10;An underlying mean component will represent a random-walk scalar state which can be specified in the configuration file by ```yaml&#10;structure: - type: mean start: 0.0 noise: 1.5&#10;``` In this case `start` will correspond the mean of the state prior, $m_0$, and `noise` will correspond to the prior&apos;s variance, $\\tau^2$, that is $$&#10;\\theta_0 \\sim \\mathcal{N}\\left(m_0, \\tau^2\\right).&#10;$$ In the figure below we can see the above configuration for, respectively, a higher and lower value of `noise`. ![](./images/ssm/lc.png) ### Seasonality Seasonality is represented by Fourier components. A Fourier component can be completely specified by providing the `period`, `start`, `noise` and `harmonics`.&#10;The `start` and `noise` parameters are analogous to the mean components we saw previously.&#10;The `period` parameter refers to how long does it take for the cyclical pattern to repeat. This is done relatively to your time-point interval, such that $$&#10;P = p_{\\text{fourier}}\\cdot p_{\\text{stream}}.&#10;$$ That is, if your stream&apos;s rate is one observation every 100 milliseconds, $p_{\\text{stream}}=0.1$, and the harmonic&apos;s period is 2000, $p_{\\text{fourier}}=1000$, then the seasonal component will repeat every $200$ seconds. The configuration example ```yaml&#10;structure: - type: season period: 200 harmonics: 5 start: 0.0 noise: 0.7&#10;``` will create a sequence of state vectors $\\boldsymbol{\\theta}_{0:T}$ with five components, such that: $$&#10;\\boldsymbol{\\theta}_t = \\lbrace\\theta_{1,t},\\cdots,\\theta_{5,t}\\rbrace.&#10;$$ In this example, `period` refers to the number of time-points for each cycle&apos;s repetition and `harmonics` to the number of Fourier harmonics used. &quot;Simpler&quot; cyclic patterns usually require less harmonics. In the figure below we show on the lowest and highest frequency harmonics, on the left and right respectively. ![](./images/ssm/fourier.png) ### AR-*p* An AR($p$) (Auto-Regressive) component can be specified using the directives: ```yaml&#10;structure: - type: arma start: 0.0 coefficients: 0.1,0.3,0.15 noise: 0.5&#10;``` In the above example we would be creating an AR(3) component, with respective coefficients $\\phi=\\lbrace 0.1,0.3,0.15 \\rbrace$. These coefficients will take part of the state model as $$&#10;\\mathsf{G} = \\begin{bmatrix}&#10;\\phi_1 &amp; \\phi_2 &amp; \\cdots &amp; \\phi_{p-1} &amp; \\phi_p \\\\&#10;1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\&#10;\\vdots &amp; &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\&#10;0 &amp; \\cdots &amp; 0 &amp; 1 &amp; 0&#10;\\end{bmatrix}&#10;$$ In the following plots we show respectively the first and second component of the AR(3) state vector. ![](./images/ssm/arma3.png) ### Composing Structural composition of DGLM structures amounts to the individual composition of the state covariance matrix and state/observational evolution matrices such that: $$&#10;\\mathsf{F}^T = \\begin{bmatrix}\\mathsf{F}_1 &amp;amp; \\mathsf{F}_2 &amp;amp; \\dots \\mathsf{F}_i\\end{bmatrix}^T \\\\&#10;\\mathsf{G} = \\text{blockdiag}\\left(\\mathsf{G}_1, \\mathsf{G}_2, \\dots, \\mathsf{G}_i\\right) \\\\&#10;\\mathsf{W} = \\text{blockdiag}\\left(\\mathsf{W}_1, \\mathsf{W}_2, \\dots, \\mathsf{W}_i\\right)&#10;$$ To express the composition of structures in the YAML configuration, we simply enumerate the separate components under the `structure` key. As as example, to compose the previous `mean` and `seasonal` components, we would simply write: ```yaml&#10;structure: - type: mean start: 0.0 noise: 1.5 - type: season period: 200 harmonics: 5 start: 0.0 noise: 0.7&#10;``` This would create a structure containing both an underlying mean *and* a seasonal component. ## Observations As we have seen from (3) that an observational model can be coupled with a structure to complete the DGLM specification. In the following sections we will look at some example observational models and in which situations they might be useful. ### Continuous Continuous observations are useful to model real valued data such as stock prices, temperature readings, *etc.* This can be achieved by specifying the observational component as a Gaussian distribution such that: $$&#10;Y_t|\\Phi \\sim \\mathcal{N}\\left(y_t|\\eta_t, \\mathsf{W}\\right).&#10;$$ ```yaml&#10;observations: - type: continuous noise: 1.5&#10;``` The following plot shows the coupling of the structure used in the `mean` section with the continuous (example above) observational model. ![](./images/ssm/continous.png) ### Discrete Discrete observations, sometimes referred as &quot;count data&quot;, can be used to model integer quantities. This can be achieved by using a Poisson distribution in the observational model, such that: $$&#10;Y_t|\\Phi \\sim \\text{Po}\\left(y_t|\\eta_t\\right)&#10;$$ An example configuration would be: ```yaml&#10;observations: - type: discrete&#10;``` In this case we will use the previous ARMA(3) structure example and couple it with a discrete observational model. The result is shown in the plot below: ![](./images/ssm/discrete.png) ### Categorical In the categorical case, we model the observations according to a binomial distribution, such that $$&#10;Y_t \\sim \\text{Bin}\\left(y_t|\\eta_t,r\\right),&#10;$$ where $r$ represents the number of categories. A typical example would be the case where $r=1$ which would represent a binary outcome (0 or 1). The following configuration implements this very case: ```yaml&#10;observations: - type: categorical categories: 1&#10;``` We can see in the plot below (*states on the left, observations on the right*) a realisation of this stream, when using the previous seasonal example structure. ![](./images/ssm/categorical.png) Often, when simulating a data stream, we might be interested in the category labels themselves, rather than a numerical value. The generator allows to pass directly a list of labels and output the labelled observations. Let&apos;s assume we wanted to generate a stream of random DNA nucleotides (C, T, A and G). The generator allows to pass the labels directly and output the direct mapping between observations and label, that is: $$&#10;y_t: \\lbrace 0, 1, 2, 3 \\rbrace \\mapsto \\lbrace\\text{C, T, A, G}\\rbrace&#10;$$ ```yaml&#10;observations: - type: categorical values: C,T,A,G&#10;``` Using the same seasonal structure and observation model as above, the output would then be: ![](./images/ssm/categorical_nucleotides.png) ## Composite model In a real-world scenario we are interested in simulating multivariate data and that comprises of different observational models. For instance, combining observation components from categorical, continuous, *etc.* The approach taken for multivariate composite models, is that the structures are composed as seen previously into a single one and the resulting state vector is then &quot;collapsed&quot; into a vector on natural parameters, $\\eta_t$ which are then used to sample the individual observation components. $$&#10;\\theta_t = \\lbrace\\underbrace{\\theta_{1}, \\theta_{2}, \\theta_{3}}_{\\eta_1}, \\underbrace{\\theta_{4}, \\theta_{5}, \\theta_{6}}_{\\eta_2}\\rbrace \\\\&#10;y = f(\\eta_t) \\\\&#10;= \\lbrace f_1(\\eta_1), f_2(\\eta_2)\\rbrace&#10;$$ The model composition can be expressed by grouping the different structures and observations under a `compose` key: ```yaml&#10;compose:&#10;- structure: # component 1&#10;- type: mean&#10;start: 0.0&#10;noise: 0.5&#10;- observations:&#10;- type: continuous&#10;noise: 0.5&#10;- structure: # component 2&#10;- type: mean&#10;start: 5.0&#10;noise: 3.7&#10;- observations:&#10;- type: continuous&#10;noise: 1.5&#10;``` ### Examples We will look at two separate examples, one that creates a stream of simulated stock prices and one that generates a fake HTTP log.&#10;We assume we want to simulate a stream of per-day stock prices for 3 different companies, each with different characteristics. In this case, we will model the following: * Company A&apos;s stocks start at quite a high value ($700) and are quite stable throughout time&#10;* Company B&apos;s stocks start slightly lower than A ($500) and are quite stable in the long run, but show heavy fluctuation from day to day&#10;* Company C&apos;s stocks start at $600 are very unpredictable Since we will be using per-day data we won&apos;t be streaming this in realtime! We can map each daily observation to a second in our stream so we will specify a `period=1`. All stocks will exhibit a small monthly effect (`period=30`), which will be indicated by a `noise=0.01` and a yearly effect (`period=365`) with a `noise=2.5`. The resulting configuration will be: ```yaml&#10;compose: - structure: # company A - type: mean start: 700 noise: 0.01 # low structural variance - type: season period: 30 # monthly seasonality noise: 0.1 - type: season period: 365. # yearly seasonality noise: 1.7 - observations: - type: continuous noise: 0.05 # low observational variance - structure: # company B - type: mean start: 500 noise: 0.01 # low structural variance - type: season period: 30 # monthly seasonality noise: 0.7 - type: season period: 365. # yearly seasonality noise: 3.7 - observations: - type: continuous noise: 3.00 # higher observational variance - structure: # company C - type: mean start: 600 noise: 3.0 # higher structural variance - type: season period: 30 # monthly seasonality noise: 0.1 - type: season period: 365. # yearly seasonality noise: 0.25 - observations: - type: continuous noise: 4.0 # higher observational variance&#10;``` A realisation of this stream looks like the figure below. ![](./images/ssm/mv_continous.png) To generate the fake HTTP log we will make the following assumptions: * We will have a request type (`GET`, `POST`, `PUT`) which will vary following a random walk&#10;* A set of visited pages which, for illustration purposes, will be limited to (`/site/page.htm`, `/site/index.htm` and `/internal/example.htm`). We also want that the URLs visited follow a seasonal pattern.&#10;* An IP address in the IPv4 format (*i.e.* `0-255.0-255.0-255.0-255`) It is clear that for all variables the appropriate observational model is the categorical one. For the request type and the visited page we can pass directly the category name in the configuration file and for the IP we simply need four categorical observations with $r=255$. If the underlying structure is the same, a useful shortcut to specify several observation component is the `replicate` key. In this particular example to generate four `0-255` numbers with an underlying mean as the structure, we simple use: ```yaml&#10;- replicate: 4 structure: - type: mean start: 0.0 noise: 2.1 observations: type: categorical categories: 255&#10;``` The full configuration for the HTTP log simulation could then be something like this: ```yaml&#10;compose: - structure: - type: mean start: 0.0 noise: 0.01 observations: type: categorical values: GET,POST,PUT - structure: - type: mean start: 0.0 noise: 0.01 - type: season start: 1.0 period: 15 noise: 0.2&#10;observations: type: categorical values: /site/page.htm,/site/index.htm,/internal/example.htm - replicate: 4 structure: - type: mean start: 0.0 noise: 2.1&#10;observations: type: categorical categories: 255&#10;``` ```text&#10;[&quot;PUT&quot;, &quot;/internal/example.htm&quot;, 171, 158, 59, 89]&#10;[&quot;GET&quot;, &quot;/internal/example.htm&quot;, 171, 253, 71, 146]&#10;[&quot;PUT&quot;, &quot;/internal/example.htm&quot;, 224, 252, 9, 156]&#10;[&quot;POST&quot;, &quot;/site/index.htm&quot;, 143, 253, 6, 126]&#10;[&quot;POST&quot;, &quot;/site/page.htm&quot;, 238, 254, 2, 48]&#10;[&quot;GET&quot;, &quot;/site/page.htm&quot;, 228, 252, 52, 126]&#10;[&quot;POST&quot;, &quot;/internal/example.htm&quot;, 229, 234, 103, 233]&#10;[&quot;GET&quot;, &quot;/internal/example.htm&quot;, 185, 221, 109, 195]&#10;...&#10;``` ## Setting up the generator As I have mentioned in the beginning of this post, we want to fit the data simulation solution into a cloud computing workflow. To illustrate this we will use the [OpenShift](https://www.openshift.com/) platform which allows for the deployment of containerised applications. A typical setup for a streaming data processing application would be as illustrated in the figure below. We have several sources connected to a message broker, such as Apache Kafka in this case. Data might be partitioned into &quot;topics&quot; which are then consumed by different applications, each performing data processing, either independently or in a distributed manner. ![](./images/ssm/kafka.png) An advantage of `timeseries-mock` would then be to replace the &quot;real&quot; data sources with a highly configurable simulator either for the prototyping or initial testing phase. If we consider our previous example of the &quot;fake&quot; HTTP log generation, an application for Web log analytics could be prototyped and tested with simulated log data very quickly, without being blocked by the lack of suitable real data.&#10;Since the data is consumed by proxy via the message broker&apos;s topics, we could later on replace the simulator with real data sources seamlessly without an impact on any of the applications.&#10;To setup the generator (and assuming Kafka and your consumer application are already running on OpenShift) we only need to perform two steps: * Write the data specifications in a YAML configuration&#10;* Use the `s2i` to deploy the simulator The `s2i` functionality of OpenShift [allows to create](https://github.com/openshift/source-to-image) deployment ready images by simply pointing to a source code location. In this case we could simply write: ```shell&#10;oc new-app centos/python-36-centos7~https://github.com/ruivieira/timeseries-mock \\&#10;-e KAFKA_BROKERS=kafka:9092 \\&#10;-e KAFKA_TOPIC=example \\&#10;-e CONF=examples/mean_continuous.yml \\&#10;--name=emitter&#10;``` In this case, we would deploy a simulator generating data according to the specifications in `mean_continuous.yml`. This data will be sent to the topic `example` of a Kafka broker running on port `9092`. ![](./images/ssm/kafka_ssm.png) The stream will be ready to consume and message payload will a stream of serialised JSON strings. In the case of the simulated HTTP log this would be: ```json&#10;{&#10;name: &quot;HTTP log&quot;&#10;values: [&quot;GET&quot;, &quot;/internal/example.htm&quot;, 185, 221, 109, 195]&#10;}&#10;``` * `name` - the name given to this stream in the configuration file&#10;* `values` - a single observation for this stream After consuming the data it is straight-forward to do any post-processing if needed. For instance, the `values` above could be easily transformed into a standard Apache Web server log line. I hope you found this tool useful, simple to use and configure. Some future work includes adding more observations distributions beyond the exponential family and the ability to directly add transformation rules to the generated observations.&#10;If you have any suggestions, use cases (or found an issue!), please let me know in the [repository](https://github.com/ruivieira/timeseries-mock). If you have any comments please let me know on [Mastodon](https://mastodon.social/@ruivieira) (or [Twitter](https://twitter.com/ruimvieira)). Happy coding!");
docs.push({"id":9,"title":"Containerised Streaming Data Generation using State-Space Models","url":"/containerised-streaming-data-generation-using-state-space-models.html"});
index.add(10, "# Containers Notes on containers.");
docs.push({"id":10,"title":"Containers","url":"/containers.html"});
index.add(11, "# Cookiecutter data science&#10;* Main documentation is available [here](https://drivendata.github.io/cookiecutter-data-science/). ## Setup ### Requirements For the purpose of these instructions we will assume the following are installed: * [Python](python.html) 3.9.0&#10;* `virtualenv` A new `venv` can be created with `virtualenv env`[^venv] and activated with `source venv/bin/activate`. Once the environment is active we can install the `cookiecutter` package using `pip install cookiecutter`. The create of the `cookiecutter` project can be done with ```bash&#10;cookiecutter https://github.com/drivendata/cookiecutter-data-science&#10;``` For the remainder of this text we will call the of the project you&apos;ve just created as `$PROJ`. [^venv]: More details at [Python environments](python-environments.html#venv).");
docs.push({"id":11,"title":"Cookiecutter Data Science","url":"/cookiecutter-data-science.html"});
index.add(12, "# Counterfactual fairness&#10;## Building counterfactually fair models&#10;### Data&#10;To evaluate _counterfactual fairness_ we will be using the &quot;law school&quot; dataset[^mcintyre2018law]. The Law School Admission Council conducted a survey across 163 law schools in the United States. It contains information on 21,790 law students such as their entrance exam scores (`LSAT`), their&#10;grade-point average (`GPA`) collected prior to law school, and their first year average grade (`FYA`).&#10;Given this data, a school may wish to predict if an applicant will have a high `FYA`. The school would&#10;also like to make sure these predictions are not biased by an individual&rsquo;s race and sex. However, the `LSAT`, `GPA`, and `FYA` scores, may be biased due to social factors. We start by importing the data into a [Pandas](pandas.html) `DataFrame`. [^mcintyre2018law]: McIntyre, Frank, and Michael Simkovic. &quot;Are law degrees as valuable to minorities?.&quot; International Review of Law and Economics 53 (2018): 23-37.&#10;```python&#10;import warnings warnings.filterwarnings(&quot;ignore&quot;)&#10;``` ```python&#10;import pandas as pd df = pd.read_csv(&quot;data/law_data.csv&quot;, index_col=0)&#10;df.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;race&lt;/th&gt; &lt;th&gt;sex&lt;/th&gt; &lt;th&gt;LSAT&lt;/th&gt; &lt;th&gt;UGPA&lt;/th&gt; &lt;th&gt;region_first&lt;/th&gt; &lt;th&gt;ZFYA&lt;/th&gt; &lt;th&gt;sander_index&lt;/th&gt; &lt;th&gt;first_pf&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;White&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;39.0&lt;/td&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;-0.98&lt;/td&gt; &lt;td&gt;0.782738&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;White&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;36.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;td&gt;0.735714&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;White&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;30.0&lt;/td&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;MS&lt;/td&gt; &lt;td&gt;-0.35&lt;/td&gt; &lt;td&gt;0.670238&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;Hispanic&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;39.0&lt;/td&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;NE&lt;/td&gt; &lt;td&gt;0.58&lt;/td&gt; &lt;td&gt;0.697024&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;White&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;-1.26&lt;/td&gt; &lt;td&gt;0.786310&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;### Pre-processing&#10;We now pre-process the data. We start by creating categorical &quot;dummy&quot; variables according to the `race` variable.&#10;```python&#10;df = pd.get_dummies(df, columns=[&quot;race&quot;], prefix=&quot;&quot;, prefix_sep=&quot;&quot;)&#10;df.iloc[:, : 7].head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;sex&lt;/th&gt; &lt;th&gt;LSAT&lt;/th&gt; &lt;th&gt;UGPA&lt;/th&gt; &lt;th&gt;region_first&lt;/th&gt; &lt;th&gt;ZFYA&lt;/th&gt; &lt;th&gt;sander_index&lt;/th&gt; &lt;th&gt;first_pf&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;39.0&lt;/td&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;-0.98&lt;/td&gt; &lt;td&gt;0.782738&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;36.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;td&gt;0.735714&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;30.0&lt;/td&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;MS&lt;/td&gt; &lt;td&gt;-0.35&lt;/td&gt; &lt;td&gt;0.670238&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;39.0&lt;/td&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;NE&lt;/td&gt; &lt;td&gt;0.58&lt;/td&gt; &lt;td&gt;0.697024&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;-1.26&lt;/td&gt; &lt;td&gt;0.786310&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;df.iloc[:, 7 :].head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Amerindian&lt;/th&gt; &lt;th&gt;Asian&lt;/th&gt; &lt;th&gt;Black&lt;/th&gt; &lt;th&gt;Hispanic&lt;/th&gt; &lt;th&gt;Mexican&lt;/th&gt; &lt;th&gt;Other&lt;/th&gt; &lt;th&gt;Puertorican&lt;/th&gt; &lt;th&gt;White&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;We also want to expand the `sex` variable into `male`/`female` categorical variables and remove the original.&#10;```python&#10;df[&quot;male&quot;] = df[&quot;sex&quot;].map(lambda x: 1 if x == 2 else 0)&#10;df[&quot;female&quot;] = df[&quot;sex&quot;].map(lambda x: 1 if x == 1 else 0)&#10;df = df.drop(axis=1, columns=[&quot;sex&quot;])&#10;``` ```python&#10;df.iloc[:, 0:7].head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;LSAT&lt;/th&gt; &lt;th&gt;UGPA&lt;/th&gt; &lt;th&gt;region_first&lt;/th&gt; &lt;th&gt;ZFYA&lt;/th&gt; &lt;th&gt;sander_index&lt;/th&gt; &lt;th&gt;first_pf&lt;/th&gt; &lt;th&gt;Amerindian&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;39.0&lt;/td&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;-0.98&lt;/td&gt; &lt;td&gt;0.782738&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;36.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;td&gt;0.735714&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;30.0&lt;/td&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;MS&lt;/td&gt; &lt;td&gt;-0.35&lt;/td&gt; &lt;td&gt;0.670238&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;39.0&lt;/td&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;NE&lt;/td&gt; &lt;td&gt;0.58&lt;/td&gt; &lt;td&gt;0.697024&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;-1.26&lt;/td&gt; &lt;td&gt;0.786310&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;df.iloc[:, 7:].head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Asian&lt;/th&gt; &lt;th&gt;Black&lt;/th&gt; &lt;th&gt;Hispanic&lt;/th&gt; &lt;th&gt;Mexican&lt;/th&gt; &lt;th&gt;Other&lt;/th&gt; &lt;th&gt;Puertorican&lt;/th&gt; &lt;th&gt;White&lt;/th&gt; &lt;th&gt;male&lt;/th&gt; &lt;th&gt;female&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;We will also convert the entrance exam scores (`LSAT`) to a discrete variable.&#10;```python&#10;df[&quot;LSAT&quot;] = df[&quot;LSAT&quot;].astype(int)&#10;df.iloc[:, :6].head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;LSAT&lt;/th&gt; &lt;th&gt;UGPA&lt;/th&gt; &lt;th&gt;region_first&lt;/th&gt; &lt;th&gt;ZFYA&lt;/th&gt; &lt;th&gt;sander_index&lt;/th&gt; &lt;th&gt;first_pf&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;39&lt;/td&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;-0.98&lt;/td&gt; &lt;td&gt;0.782738&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;0.09&lt;/td&gt; &lt;td&gt;0.735714&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;3.1&lt;/td&gt; &lt;td&gt;MS&lt;/td&gt; &lt;td&gt;-0.35&lt;/td&gt; &lt;td&gt;0.670238&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;39&lt;/td&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;NE&lt;/td&gt; &lt;td&gt;0.58&lt;/td&gt; &lt;td&gt;0.697024&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;37&lt;/td&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;td&gt;GL&lt;/td&gt; &lt;td&gt;-1.26&lt;/td&gt; &lt;td&gt;0.786310&lt;/td&gt; &lt;td&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;df.iloc[:, 6:].head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Amerindian&lt;/th&gt; &lt;th&gt;Asian&lt;/th&gt; &lt;th&gt;Black&lt;/th&gt; &lt;th&gt;Hispanic&lt;/th&gt; &lt;th&gt;Mexican&lt;/th&gt; &lt;th&gt;Other&lt;/th&gt; &lt;th&gt;Puertorican&lt;/th&gt; &lt;th&gt;White&lt;/th&gt; &lt;th&gt;male&lt;/th&gt; &lt;th&gt;female&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;### Protected attributes&#10;_Counterfactual fairness_ enforces that a distribution over possible predictions for an individual should&#10;remain unchanged in a world where an individual&rsquo;s protected attributes $A$ had been different in a causal sense.&#10;Let&apos;s start by defining the _protected attributes_. Obvious candidates are the different categorical variables for ethnicity (`Asian`, `White`, `Black`, _etc_) and gender (`male`, `female`).&#10;```python&#10;A = [ &quot;Amerindian&quot;, &quot;Asian&quot;, &quot;Black&quot;, &quot;Hispanic&quot;, &quot;Mexican&quot;, &quot;Other&quot;, &quot;Puertorican&quot;, &quot;White&quot;, &quot;male&quot;, &quot;female&quot;,&#10;]&#10;``` ### Training and testing subsets We will now divide the dataset into training and testing subsets.&#10;We will use the same ratio as in [^Kusner2017], that is 20%. [^Kusner2017]: Kusner, Matt J., Joshua Loftus, Chris Russell, and Ricardo Silva. &quot;Counterfactual fairness.&quot; In Advances in neural information processing systems, pp. 4066-4076. 2017.&#10;```python&#10;from sklearn.model_selection import train_test_split df_train, df_test = train_test_split(df, random_state=23, test_size=0.2);&#10;``` ## Models ### Unfair model As detailed in [^Kusner2017], the concept of counterfactual fairness holds&#10;under three levels of assumptions of increasing strength. The first of such levels is *Level 1*, where $\\hat{Y}$ is built using only the observable non-descendants of $A$. This only requires *partial* causal ordering and no further causal assumptions, but in many problems there will be few, if any,&#10;observables which are not descendants of protected demographic factors. For this dataset, since `LSAT`, `GPA`, and `FYA` are all biased by ethnicity and gender, we cannot use any observed&#10;features to construct a Level 1 counterfactually fair predictor as described in Level 1. Instead (and in order to compare the performance with Level 2 and 3 models) we will build two _unfair baselines_. * A *Full* model, which will be trained with the totality of the variables&#10;* An *Unaware* model (FTU), which will be trained will all the variables, except the protected attributes $A$. Let&apos;s proceed with calculating the *Full* model.&#10;### Full model As mentioned previously, the full model will be a simple linear regression in order to predict `ZFYA` using all of the variables.&#10;```python&#10;from sklearn.linear_model import LinearRegression linreg_unfair = LinearRegression()&#10;``` The inputs will then be the totality of the variabes (protected variables $A$, as well as `UGPA` and `LSAT`).&#10;```python&#10;import numpy as np X = np.hstack( ( df_train[A], np.array(df_train[&quot;UGPA&quot;]).reshape(-1, 1), np.array(df_train[&quot;LSAT&quot;]).reshape(-1, 1), )&#10;)&#10;print(X)&#10;``` ```&#10;[[ 0. 0. 0. ... 1. 3.1 39. ] [ 0. 0. 0. ... 1. 3.5 36. ] [ 0. 0. 0. ... 1. 3.9 46. ] ... [ 0. 0. 0. ... 1. 2.9 33. ] [ 0. 0. 0. ... 0. 2.9 31. ] [ 0. 0. 0. ... 0. 3.6 39. ]] ``` As for our target, we are trying to predict `ZFYA` (first year average grade).&#10;```python&#10;y = df_train[&quot;ZFYA&quot;]&#10;y[:10]&#10;``` We fit the model:&#10;```python&#10;linreg_unfair = linreg_unfair.fit(X, y)&#10;``` And perform some predictions on the test subset.&#10;```python&#10;X_test = np.hstack( ( df_test[A], np.array(df_test[&quot;UGPA&quot;]).reshape(-1, 1), np.array(df_test[&quot;LSAT&quot;]).reshape(-1, 1), )&#10;)&#10;X_test&#10;``` ```python&#10;predictions_unfair = linreg_unfair.predict(X_test)&#10;predictions_unfair&#10;``` We will also calculate the _unfair model_ score for future use.&#10;```python&#10;score_unfair = linreg_unfair.score(X_test, df_test[&quot;ZFYA&quot;])&#10;print(score_unfair)&#10;``` ```&#10;0.12701634112845117 ``` ```python&#10;from sklearn.metrics import mean_squared_error RMSE_unfair = np.sqrt(mean_squared_error(df_test[&quot;ZFYA&quot;], predictions_unfair))&#10;print(RMSE_unfair)&#10;``` ```&#10;0.8666709890234552 ``` ### Fairness through unawareness (FTU) As also mentioned in [^Kusner2017], the second baseline we will use is an **Unaware** model (FTU), which will be trained will all the variables, except the protected attributes $A$.&#10;```python&#10;linreg_ftu = LinearRegression()&#10;``` We will create the inputs as previously, but without using the protected attributes, $A$.&#10;```python&#10;X_ftu = np.hstack( ( np.array(df_train[&quot;UGPA&quot;]).reshape(-1, 1), np.array(df_train[&quot;LSAT&quot;]).reshape(-1, 1), )&#10;)&#10;X_ftu&#10;``` And we fit the model:&#10;```python&#10;linreg_ftu = linreg_ftu.fit(X_ftu, y)&#10;``` Again, let&apos;s perform some predictions on the test subset.&#10;```python&#10;X_ftu_test = np.hstack( (np.array(df_test[&quot;UGPA&quot;]).reshape(-1, 1), np.array(df_test[&quot;LSAT&quot;]).reshape(-1, 1))&#10;)&#10;X_ftu_test&#10;``` ```python&#10;predictions_ftu = linreg_ftu.predict(X_ftu_test)&#10;predictions_ftu&#10;``` As previously, let&apos;s calculate this model&apos;s score.&#10;```python&#10;ftu_score = linreg_ftu.score(X_ftu_test, df_test[&quot;ZFYA&quot;])&#10;print(ftu_score)&#10;``` ```&#10;0.0917442226187073 ``` ```python&#10;RMSE_ftu = np.sqrt(mean_squared_error(df_test[&quot;ZFYA&quot;], predictions_ftu))&#10;print(RMSE_ftu)&#10;``` ```&#10;0.8840061503773576 ``` ### Latent variable model Still according to [^Kusner2017], a **Level 2** approach will model latent &lsquo;fair&rsquo; variables which are parents of observed variables. If we consider a predictor parameterised by $\\theta$, such as: $$&#10;\\hat{Y} \\equiv g_\\theta (U, X_{\\nsucc A})&#10;$$ with $X_{\\nsucc A} \\subseteq X$ are non-descendants of $A$.&#10;Assuming a loss function $l(\\cdot,\\cdot)$ and training data $\\mathcal{D}\\equiv\\{(A^{(i), X^{(i)}, Y^{(i)}})\\}$, for $i=1,2\\dots,n$, the empirical loss is defined as $$&#10;L(\\theta)\\equiv \\sum_{i=1}^n \\mathbb{E}[l(y^{(i)},g_\\theta(U^{(i)}, x^{(i)}_{\\nsucc A}))]/n&#10;$$ which has to be minimised in order to $\\theta$. Each $n$ expectation is with respect to random variable $U^{(i)}$ such that $$&#10;U^{(i)}\\sim P_{\\mathcal{M}}(U|x^{(i)}, a^{(i)})&#10;$$ where $P_{\\mathcal{M}}(U|x,a)$ is the conditional distribution of the background variables as given by a causal model M that is available by assumption. If this expectation cannot be calculated analytically, Markov chain Monte Carlo (MCMC) can be used to approximate it as in the following algorithm. We will follow the model specified in the original paper, where the latent variable considered is $K$, which represents a student&apos;s **knowledge**.&#10;$K$ will affect `GPA`, `LSAT` and the outcome, `FYA`.&#10;The model can be defined by: $$&#10;\\begin{aligned}&#10;GPA &amp;\\sim \\mathcal{N}(GPA_0 + w_{GPA}^KK + w_{GPA}^RR + w_{GPA}^SS, \\sigma_{GPA}) \\\\&#10;LSAT &amp;\\sim \\text{Po}(\\exp(LSAT_0 + w_{LSAT}^KK + w_{LSAT}^RR + w_L^SS)) \\\\&#10;FYA &amp;\\sim \\mathcal{N}(w_{FYA}^KK + w_{FYA}^RR + w_{FYA}^SS, 1) \\\\&#10;K &amp;\\sim \\mathcal{N}(0,1)&#10;\\end{aligned}&#10;$$&#10;The priors used will be: $$&#10;\\begin{aligned}&#10;GPA_0 &amp;\\sim \\mathcal{N}(0, 1) \\\\&#10;LSAT_0 &amp;\\sim \\mathcal{N}(0, 1) \\\\&#10;GPA_0 &amp;\\sim \\mathcal{N}(0, 1)&#10;\\end{aligned}&#10;$$&#10;```python&#10;import pymc3 as pm K = len(A) def MCMC(data, samples=1000): N = len(data) a = np.array(data[A]) model = pm.Model() with model: # Priors k = pm.Normal(&quot;k&quot;, mu=0, sigma=1, shape=(1, N)) gpa0 = pm.Normal(&quot;gpa0&quot;, mu=0, sigma=1) lsat0 = pm.Normal(&quot;lsat0&quot;, mu=0, sigma=1) w_k_gpa = pm.Normal(&quot;w_k_gpa&quot;, mu=0, sigma=1) w_k_lsat = pm.Normal(&quot;w_k_lsat&quot;, mu=0, sigma=1) w_k_zfya = pm.Normal(&quot;w_k_zfya&quot;, mu=0, sigma=1) w_a_gpa = pm.Normal(&quot;w_a_gpa&quot;, mu=np.zeros(K), sigma=np.ones(K), shape=K) w_a_lsat = pm.Normal(&quot;w_a_lsat&quot;, mu=np.zeros(K), sigma=np.ones(K), shape=K) w_a_zfya = pm.Normal(&quot;w_a_zfya&quot;, mu=np.zeros(K), sigma=np.ones(K), shape=K) sigma_gpa_2 = pm.InverseGamma(&quot;sigma_gpa_2&quot;, alpha=1, beta=1) mu = gpa0 + (w_k_gpa * k) + pm.math.dot(a, w_a_gpa) # Observed data gpa = pm.Normal( &quot;gpa&quot;, mu=mu, sigma=pm.math.sqrt(sigma_gpa_2), observed=list(data[&quot;UGPA&quot;]), shape=(1, N), ) lsat = pm.Poisson( &quot;lsat&quot;, pm.math.exp(lsat0 + w_k_lsat * k + pm.math.dot(a, w_a_lsat)), observed=list(data[&quot;LSAT&quot;]), shape=(1, N), ) zfya = pm.Normal( &quot;zfya&quot;, mu=w_k_zfya * k + pm.math.dot(a, w_a_zfya), sigma=1, observed=list(data[&quot;ZFYA&quot;]), shape=(1, N), ) step = pm.Metropolis() trace = pm.sample(samples, step) return trace&#10;``` ```python&#10;train_estimates = MCMC(df_train)&#10;``` ```&#10;Multiprocess sampling (4 chains in 4 jobs)&#10;CompoundStep&#10;&gt;Metropolis: [sigma_gpa_2]&#10;&gt;Metropolis: [w_a_zfya]&#10;&gt;Metropolis: [w_a_lsat]&#10;&gt;Metropolis: [w_a_gpa]&#10;&gt;Metropolis: [w_k_zfya]&#10;&gt;Metropolis: [w_k_lsat]&#10;&gt;Metropolis: [w_k_gpa]&#10;&gt;Metropolis: [lsat0]&#10;&gt;Metropolis: [gpa0]&#10;&gt;Metropolis: [k] ``` ```&#10;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 87 seconds.&#10;The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.&#10;The estimated number of effective samples is smaller than 200 for some parameters. ``` Let&apos;s plot a single trace for $k^{(i)}$.&#10;```python&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;from plotutils import * # Thin the samples before plotting&#10;k_trace = train_estimates[&quot;k&quot;][:, 0].reshape(-1, 1)[0::100]&#10;plt.subplot(1, 2, 1)&#10;plt.hist(k_trace, color=colours[0], bins=100)&#10;plt.subplot(1, 2, 2)&#10;plt.scatter(range(len(k_trace)), k_trace, s=1, c=colours[0])&#10;plt.show()&#10;``` ![counterfactual-fairness_1](./images/counterfactual-fairness_1.png)&#10;```python&#10;train_k = np.mean(train_estimates[&quot;k&quot;], axis=0).reshape(-1, 1)&#10;train_k&#10;``` We can now estimate $k$ using the test data:&#10;```python&#10;test_map_estimates = MCMC(df_test)&#10;``` ```&#10;Multiprocess sampling (4 chains in 4 jobs)&#10;CompoundStep&#10;&gt;Metropolis: [sigma_gpa_2]&#10;&gt;Metropolis: [w_a_zfya]&#10;&gt;Metropolis: [w_a_lsat]&#10;&gt;Metropolis: [w_a_gpa]&#10;&gt;Metropolis: [w_k_zfya]&#10;&gt;Metropolis: [w_k_lsat]&#10;&gt;Metropolis: [w_k_gpa]&#10;&gt;Metropolis: [lsat0]&#10;&gt;Metropolis: [gpa0]&#10;&gt;Metropolis: [k] ``` ```&#10;Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 35 seconds.&#10;The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.&#10;The estimated number of effective samples is smaller than 200 for some parameters. ``` ```python&#10;test_k = np.mean(test_map_estimates[&quot;k&quot;], axis=0).reshape(-1, 1)&#10;test_k&#10;``` We now build the Level 2 predictor, using $k$ as the input.&#10;```python&#10;linreg_latent = LinearRegression()&#10;``` ```python&#10;linreg_latent = linreg_latent.fit(train_k, df_train[&quot;ZFYA&quot;])&#10;``` ```python&#10;predictions_latent = linreg_latent.predict(test_k)&#10;predictions_latent&#10;``` ```python&#10;latent_score = linreg_latent.score(test_k, df_test[&quot;ZFYA&quot;])&#10;print(latent_score)&#10;``` ```&#10;0.008509520014148064 ``` ```python&#10;RMSE_latent = np.sqrt(mean_squared_error(df_test[&quot;ZFYA&quot;], predictions_latent))&#10;print(RMSE_latent)&#10;``` ```&#10;0.9236245677858551 ``` ### Additive error model&#10;Finally, in **Level 3**, we model `GPA`, `LSAT`, and `FYA` as continuous variables with additive error terms&#10;independent of race and sex[^1]. [^1]: That may in turn be correlated with one-another. This corresponds to $$&#10;\\begin{aligned}&#10;GPA &amp;= b_G + w^R_{GPA}R + w^S_{GPA}S + \\epsilon_{GPA}, \\epsilon_{GPA} \\sim p(\\epsilon_{GPA}) \\\\&#10;LSAT &amp;= b_L + w^R_{LSAT}R + w^S_{LSAT}S + \\epsilon_{LSAT}, \\epsilon_{LSAT} \\sim p(\\epsilon_{LSAT}) \\\\&#10;FYA &amp;= b_{FYA} + w^R_{FYA}R + w^S_{FYA}S + \\epsilon_{FYA} , \\epsilon_{FYA} \\sim p(\\epsilon_{FYA})&#10;\\end{aligned}&#10;$$&#10;We estimate the error terms $\\epsilon_{GPA}, \\epsilon_{LSAT}$ by first fitting two models that each use race and sex to individually&#10;predict `GPA` and `LSAT`. We then compute the residuals of each model (_e.g._, $\\epsilon_{GPA} =GPA&minus;\\hat{Y}_{GPA}(R, S)$).&#10;We use these residual estimates of $\\epsilon_{GPA}, \\epsilon_{LSAT}$ to predict $FYA$. In [^Kusner2017] this is called _Fair Add_.&#10;Since the process is similar for the individual predictions for `GPA` and `LSAT`, we will write a method to avoid repetion.&#10;```python&#10;def calculate_epsilon(data, var_name, protected_attr): X = data[protected_attr] y = data[var_name] linreg = LinearRegression() linreg = linreg.fit(X, y) predictions = linreg.predict(X) return data[var_name] - predictions&#10;``` Let&apos;s apply it to each variable, individually.&#10;First we calculate $\\epsilon_{GPA}$:&#10;```python&#10;epsilons_gpa = calculate_epsilon(df, &quot;UGPA&quot;, A)&#10;epsilons_gpa&#10;``` Next, we calculate $\\epsilon_{LSAT}$:&#10;```python&#10;epsilons_LSAT = calculate_epsilon(df, &quot;LSAT&quot;, A)&#10;epsilons_LSAT&#10;``` Let&apos;s visualise the $\\epsilon$ distribution quickly:&#10;```python&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns plt.subplot(1, 2, 1)&#10;plt.hist(epsilons_gpa, color=colours[0], bins=100)&#10;plt.title(&quot;$\\epsilon_{GPA}$&quot;)&#10;plt.xlabel(&quot;$\\epsilon_{GPA}$&quot;) plt.subplot(1, 2, 2)&#10;plt.hist(epsilons_LSAT, color=colours[1], bins=100)&#10;plt.title(&quot;$\\epsilon_{LSAT}$&quot;)&#10;plt.xlabel(&quot;$\\epsilon_{LSAT}$&quot;)&#10;plt.show()&#10;``` ![counterfactual-fairness_2](./images/counterfactual-fairness_2.png)&#10;We finally use the calculated $\\epsilon$ to train a model in order to predict `FYA`.&#10;We start by getting the subset of the $\\epsilon$ which match the training indices.&#10;```python&#10;X = np.hstack( ( np.array(epsilons_gpa[df_train.index]).reshape(-1, 1), np.array(epsilons_LSAT[df_train.index]).reshape(-1, 1), )&#10;)&#10;X&#10;``` ```python&#10;linreg_fair_add = LinearRegression() linreg_fair_add = linreg_fair_add.fit( X, df_train[&quot;ZFYA&quot;],&#10;)&#10;``` We now use this model to calculate the predictions&#10;```python&#10;X_test = np.hstack( ( np.array(epsilons_gpa[df_test.index]).reshape(-1, 1), np.array(epsilons_LSAT[df_test.index]).reshape(-1, 1), )&#10;) predictions_fair_add = linreg_fair_add.predict(X_test)&#10;predictions_fair_add&#10;``` And as previously, we calculate the model&apos;s score:&#10;```python&#10;fair_add_score = linreg_fair_add.score(X_test, df_test[&quot;ZFYA&quot;])&#10;print(fair_add_score)&#10;``` ```&#10;0.04475841449183948 ``` ```python&#10;RMSE_fair_add = np.sqrt(mean_squared_error(df_test[&quot;ZFYA&quot;], predictions_fair_add))&#10;print(RMSE_fair_add)&#10;``` ```&#10;0.9065835039365202 ``` ### Comparison&#10;The scores, so far, are:&#10;```python&#10;print(f&quot;Unfair score:\\t{score_unfair}&quot;)&#10;print(f&quot;FTU score:\\t{ftu_score}&quot;)&#10;print(f&quot;L2 score:\\t{latent_score}&quot;)&#10;print(f&quot;Fair add score:\\t{fair_add_score}&quot;)&#10;``` ```&#10;Unfair score:&Tab;0.12701634112845117&#10;FTU score:&Tab;0.0917442226187073&#10;L2 score:&Tab;0.008509520014148064&#10;Fair add score:&Tab;0.04475841449183948 ``` ```python&#10;print(f&quot;Unfair RMSE:\\t{RMSE_unfair}&quot;)&#10;print(f&quot;FTU RMSE:\\t{RMSE_ftu}&quot;)&#10;print(f&quot;L2 RMSE:\\t{RMSE_latent}&quot;)&#10;print(f&quot;Fair add RMSE:\\t{RMSE_fair_add}&quot;)&#10;``` ```&#10;Unfair RMSE:&Tab;0.8666709890234552&#10;FTU RMSE:&Tab;0.8840061503773576&#10;L2 RMSE:&Tab;0.9236245677858551&#10;Fair add RMSE:&Tab;0.9065835039365202 ``` ## Measuring counterfactual fairness&#10;First, we will measure two quantities, the **Statistical Parity Difference** (SPD)[^spd] and **Disparate impact** (DI)[^di]. [^spd]: See {ref}`fairness:demographic-parity-difference`.&#10;[^di]: See {ref}`fairness:disparate-impact`.&#10;### Statistical Parity Difference / Disparate Impact&#10;```python&#10;from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio parities = []&#10;impacts = [] for a in A: parity = demographic_parity_difference(df_train[&quot;ZFYA&quot;], df_train[&quot;ZFYA&quot;], sensitive_features = df_train[a]) di = demographic_parity_ratio(df_train[&quot;ZFYA&quot;], df_train[&quot;ZFYA&quot;], sensitive_features = df_train[a]) parities.append(parity) impacts.append(di)&#10;``` ```python&#10;df_parities = pd.DataFrame({&apos;protected&apos;:A,&apos;parity&apos;:parities,&apos;impact&apos;:impacts})&#10;``` ```python&#10;import matplotlib.pyplot as plt&#10;from plotutils import * fig = plt.figure() ax = fig.add_subplot(111)&#10;ax2 = ax.twinx() fig.suptitle(&apos;Statistical Parity Difference and Disparate Impact&apos;) width = 0.4&#10;df_parities.plot(x =&apos;protected&apos;, y = &apos;parity&apos;, kind = &apos;bar&apos;, ax = ax, width = width, position=1, color=colours[0], legend=False) df_parities.plot(x =&apos;protected&apos;, y = &apos;impact&apos;, kind = &apos;bar&apos;, ax = ax2, width = width, position = 0, color = colours[1], legend = False) ax.axhline(y = 0.1, linestyle = &apos;dashed&apos;, alpha = 0.7, color = colours[0])&#10;ax2.axhline(y = 0.55, linestyle = &apos;dashed&apos;, alpha = 0.7, color = colours[1]) patches, labels = ax.get_legend_handles_labels()&#10;ax.legend(patches, [&apos;Stat Parity Diff&apos;], loc = &apos;upper left&apos;) patches, labels = ax2.get_legend_handles_labels()&#10;ax2.legend(patches, [&apos;Disparate Impact&apos;], loc = &apos;upper right&apos;) labels = [item.get_text() for item in ax.get_xticklabels()] for i in range(len(A)): labels[i] = A[i] ax.set_xticklabels(labels)&#10;ax.set_xlabel(&apos;Protected Features&apos;) ax.set_ylabel(&apos;Statistical Parity Difference&apos;)&#10;ax2.set_ylabel(&apos;Disparate Impact&apos;) plt.show()&#10;``` ![counterfactual-fairness_3](./images/counterfactual-fairness_3.png)&#10;### Finding sensitive features Typically a $SPD &gt; 0.1$ and a $DI &lt; 0.9$ might indicate discrimination on those features.&#10;All _protected attributes_ fail the SPD test and, in our dataset, we have two features (`Hispanic` and `Mexican`) which clearly fail the DI test.&#10;```python&#10;for a in [&quot;Mexican&quot;, &quot;Hispanic&quot;]: spd = demographic_parity_difference(y_true=df_train[&quot;ZFYA&quot;], y_pred=df_train[&quot;ZFYA&quot;], sensitive_features = df_train[a]) print(f&quot;SPD({a}) = {spd}&quot;) di = demographic_parity_ratio(y_true=df_train[&quot;ZFYA&quot;], y_pred=df_train[&quot;ZFYA&quot;], sensitive_features = df_train[a]) print(f&quot;DI({a}) = {di}&quot;)&#10;``` ```&#10;SPD(Mexican) = 0.0014017257538768636&#10;DI(Mexican) = 0.5556529360210342&#10;SPD(Hispanic) = 0.003272247102713093&#10;DI(Hispanic) = 0.34227833235466826 ``` ```python ```&#10;");
docs.push({"id":12,"title":"Counterfactual Fairness","url":"/counterfactual-fairness.html"});
index.add(13, "# Counterfactuals with constraint solvers ## Scoring An implementation on how to calculate [counterfactuals](counterfactuals.html) with Constraint Solvers (namely [OptaPlanner](optaplanner.html)) is available [here](https://github.com/kiegroup/trusty-ai-sandbox/tree/master/explainability/core/counterfactuals). This implementation satisfies several criteria of the [counterfactuals desiderata](counterfactuals.html#Desiderata). The penalisation score is represented with a `BendableBigDecimalScore`[^BendableBigDecimalScore], having three &quot;*hard*&quot; levels and one &quot;*soft*&quot; level. [^BendableBigDecimalScore]: *c.f.* https://docs.optaplanner.org/8.0.0.Final/optaplanner-javadoc/org/optaplanner/core/api/score/buildin/bendablebigdecimal/BendableBigDecimalScore.html ![](./images/diagrams/counterfactual_score.png) The first hard level component, &lt;span class=&quot;point&quot;&gt;1&lt;/span&gt;, penalises the score according to the distance between the prediction, $y^{\\prime}$ for the currently proposed solution, $x^{\\prime}$ and the original prediction $y$, that is this our $(\\hat{f}(x^{\\prime})-y^{\\prime})^2$. The [actionability](counterfactuals.html#Actionability) is score with &lt;span class=&quot;point&quot;&gt;2&lt;/span&gt;. This component penalises the score according to number of *immutable* features which were changed in the counterfactual. A confidence score component, &lt;span class=&quot;point&quot;&gt;3&lt;/span&gt; is use to, optionally, impose a minimum confidence threshold to the counterfactual&apos;s associated prediction, $x^{\\prime}$. Finally, the *feature distance*, &lt;span class=&quot;point&quot;&gt;4&lt;/span&gt;, penalises the score according to the feature distance. This is the representation of $$&#10;d(x, x^{\\prime}).&#10;$$ In the concrete implementation linked above, the distance, $d$, chosen is a [Manhattan](distance-metrics.html#Manhattan distance L1) (or $L^1$) distance calculated feature-wise. ## Implementation ![](./images/diagrams/counterfactual_impl.png) Entities are defined by classes such as `Integer`, `Categorical`, `Boolean` or `Float`, as shown in &lt;span class=&quot;point&quot;&gt;5&lt;/span&gt;.&#10;Each of the features, shown in &lt;span class=&quot;point&quot;&gt;6&lt;/span&gt;, is created as an instance of one of these entities. For instance, `feature1` would be of type `Integer` and `feature2` would be of type `Categorical`, *etc.* The original data point, $x$ is represented by this set of features (&lt;span class=&quot;point&quot;&gt;6&lt;/span&gt;). A planning solution (`PlanningSolution`), illustrated in &lt;span class=&quot;point&quot;&gt;7&lt;/span&gt; will produce candidate solutions (shown in &lt;span class=&quot;point&quot;&gt;8&lt;/span&gt;) For each solution, we propose a new set of features ($x^{\\prime}$) as a counterfactual candidate. For instance, `solution A` in &lt;span class=&quot;point&quot;&gt;8&lt;/span&gt;. In the following section we will look at how each component is calculated. We will refer to each &quot;hard&quot; level component as $H_1, H_2$ and $H_3$ and the &quot;soft&quot; component as $S_1$. The overal score consists, then, of $S=\\{H_1, H_2, H_3, S_1 \\}$ ### Prediction distance The first component of the score, &lt;span class=&quot;point&quot;&gt;1&lt;/span&gt; is established by sending the proposed counterfactual $x^{\\prime}$, &lt;span class=&quot;point&quot;&gt;8&lt;/span&gt; to a predictive model, &lt;span class=&quot;point&quot;&gt;9&lt;/span&gt; and calculating the distance between the desired outcome, $y^{\\prime}$ and the model&apos;s prediction. This is done component wise, for each feature of the output. That is, for a prediction with $N$ features, we calculate $$&#10;H_1=\\left(\\sum_i^Nf(x^{\\prime}_i) - y^{\\prime}_i\\right)^2&#10;$$ ### Actionability score For the second component, the actionability score, &lt;span class=&quot;point&quot;&gt;2&lt;/span&gt;. We calculate the number of features for the protected set $\\mathcal{A}$, which have a different value from the original. That is, assuming we have a certain number of protectd features $M$, such that $\\mathcal{A}=\\{A_1,A_2,\\dots,A_M\\}$, we calculate: $$&#10;H_2 = \\sum_{a \\in \\mathcal{A}} \\mathbb{1}(x_a \\neq x^{\\prime}_a), $$ ### Confidence score For each feature $i$, if we have a prediction confidence, $p_i(f(x^{\\prime}))$, we calculate the number of features which have a confidence below a certain predefined threshold, $P_i$. If the threshold is not defined, this component will always be zero and not influence the counterfactual selection. Assuming we have defined a threshold for all $N$ features, $P = \\{P_1, P_2, \\dots, P_N\\}$ we calculate this score as $$&#10;H_3 = \\sum_i^N \\mathbb{1} \\left( p_i \\left( f(x^{\\prime}) &lt; P_i \\right) \\right)&#10;$$ ### Feature distance Considering that each datapoint $x$ consists of different $N$ features, such that $x=\\left(f_1,\\dots,f_n\\right)$ and that each feature might be numerical or categorical[^categorical], we calculate the distance between a datapoint $x$ and a potential counterfactual $x^{\\prime}$: $$&#10;d\\left(x,x^{\\prime}\\right)=\\sum_{i=1}^Nd^{\\prime}\\left(x_i,x_i^{\\prime}\\right)&#10;$$&#10;$$&#10;d^{\\prime}\\left(x_i,x_i^{\\prime}\\right)=&#10;\\begin{cases}&#10;\\left(x_i-x_i^{\\prime}\\right)^2,\\quad\\text{if}\\ x_i,x_i^{\\prime}\\in\\mathbb{N} \\lor x_i,x_i^{\\prime}\\in\\mathbb{R}\\\\&#10;1-\\delta_{x,x^{\\prime}},\\quad\\text{if}\\ x_i,x_i^{\\prime}\\ \\text{categorical}&#10;\\end{cases}&#10;$$ Since in many scenarios we might not have access to the training data, the above distance are not normalised. In the event that we do have access to training data, then we can use the **standard deviation** ($SD$) to normalise the features. The $SD$ can be calculated as: $$&#10;SD=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N\\left(x_i-\\bar{x}\\right)^2}&#10;$$ so that, in this case, we scale the numerical features with $$&#10;\\bar{d}^{\\prime}\\left(x_i,x_i^{\\prime}\\right)=&#10;\\frac{\\left(x_i-x_i^{\\prime}\\right)^2}{SD}.&#10;$$ ## Searching To search for a counterfactual, we start by specifying a search domain for each feature.&#10;This will include: - An upper and lower bounds for numerical features, respectively $\\mathcal{D}_l, \\mathcal{D}_u$&#10;- A set of categories for categorical features, $\\mathcal{C}$&#10;- $\\mathcal{B}=\\{0,1\\}$ for the specific case of boolean/binary values Typically these values would be either established by someone with domain knowledge, or by values that might reflect our expectation for the actual counterfactual (for instance, an `age` would have realistic values). The algorithm used for the search is **Tabu search**[^tabu] (Glover, 1989). ![](./images/diagrams/tabu_chart.png) [^categorical]: Here we are considering binary and boolean values as categorical.&#10;[^tabu]: Glover, Fred. &quot;Tabu search&mdash;part I.&quot; _ORSA Journal on computing_ 1, no. 3 (1989): 190-206. ");
docs.push({"id":13,"title":"Counterfactuals with Constraint Solvers","url":"/counterfactuals-with-constraint-solvers.html"});
index.add(14, "# Counterfactuals A special type of [Explainability](explainability.html). ## Desiderata According to Verma *et al* [^verma2020] the counterfactual desiderata is: * [Validity](counterfactuals.html#Validity)&#10;* [Actionability](counterfactuals.html#Actionability)&#10;* [Sparsity](counterfactuals.html#Sparsity)&#10;* [Data manifold closeness](counterfactuals.html#Data manifold closeness)&#10;* [Causality](counterfactuals.html#Causality)&#10;* [Amortised inference](counterfactuals.html#Amortised inference) [^verma2020]: Verma, Sahil, John Dickerson, and Keegan Hines. &quot;*Counterfactual Explanations for Machine Learning: A Review.*&quot; _arXiv preprint arXiv:2010.10596_ (2020). ### Validity We assume that a counterfactual is *valid* if it solves the optimisation as states in Wachter *et al*[^wachter]. If we defined the loss function as $$&#10;L(x,x^{\\prime},y^{\\prime},\\lambda)=\\lambda\\cdot(\\hat{f}(x^{\\prime})&minus;y^{\\prime})^2+d(x,x^{\\prime}),&#10;$$ we can define the counterfactual as $$&#10;\\arg \\underset{x^{\\prime}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\cdot(\\hat{f}(x^{\\prime})&minus;y^{\\prime})^2+d(x,x^{\\prime})&#10;$$ where:&#10;* $x \\in \\mathcal{X}$ is the original data point&#10;* $x^{\\prime} \\in \\mathcal{X}$ is the counterfactual&#10;* $y^{\\prime} \\in \\mathcal{Y}$ is the desired label&#10;* $d$ is a [distance metric](distance-metrics.html) to measure the distance between $x$ and $x^{\\prime}$. this could be a [L1](distance-metrics.html#Manhattan distance L1) or L2 distance, a quadratic distance, *etc.* [^wachter]: Wachter, Sandra, Brent Mittelstadt, and Chris Russell. &quot;*Counterfactual explanations without opening the black box: Automated decisions and the GDPR.*&quot; _Harv. JL &amp; Tech._ 31 (2017): 841. ### Actionability Still according to [^wachter], *actionability* refers to the ability of a counterfactual method to separate between *mutable* and *immutable* features. Immutable, and additionally legally protected features, shouldn&apos;t be changed by a counterfactual implementation. Formally, if we defined our set of mutable (or *actionable*) features as $\\mathcal{A}$, we have $$&#10;\\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\cdot(\\hat{f}(x^{\\prime})&minus;y^{\\prime})^2+d(x,x^{\\prime})&#10;$$ ### Sparsity According to [^wachter] Shorter counterfactuals are easier to understand and an effective counterfactual implementation should change the least amount of features as possible. If a *sparsity penalty* term is added to our definition $$&#10;g(x^{\\prime}-x)&#10;$$ which increases the more features are changed and could be a L0 or [L1](distance-metrics.html#Manhattan distance L1) metric, for instance. We can then define the counterfactual as $$&#10;\\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\cdot(\\hat{f}(x^{\\prime})&minus;y^{\\prime})^2+d(x,x^{\\prime})+g(x^{\\prime}-x)&#10;$$ ### Data manifold closeness Still according to [^wachter], *data manifold closeness* is the property which guarantees that the counterfactual will be as close to the training data as possible. This can translate into a more &quot;realistic&quot; counterfactual, since it is possible that the counterfactual would take extreme or never seen before values in order to satisfy the previous conditions. Formally, we can write a penalty term for the adherence to the training data manifold, $\\mathcal{X}$ as $l(x^{\\prime};\\mathcal{X})$ and the define the counterfactual as $$&#10;\\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\cdot(\\hat{f}(x^{\\prime})&minus;y^{\\prime})^2+d(x,x^{\\prime})+g(x^{\\prime}-x)+l(x^{\\prime};\\mathcal{X})&#10;$$ ### Causality Causality refers to the property where feature changes will impact dependent features. That is, we no longer assume that all features are independent. This implies that the counterfactual method needs to mantain the causal relations between features. ### Amortised inference Amortised inference refers to the property of a counterfactual search to provide multiple counterfactuals for a single data point. ## Alternative methods ### Constraint solvers An alternative method to find counterfactuals is to use constraint solvers. This is explored more in-depth in [Counterfactuals with Constraint Solvers](counterfactuals-with-constraint-solvers.html).");
docs.push({"id":14,"title":"Counterfactuals","url":"/counterfactuals.html"});
index.add(15, "# DOOM Emacs Go ## Setup The following modules must be enabled in `init.el`: * `(go +lsp)` in the `lang` section&#10;* `lsp` in the `tools` section&#10;* `snippets` in the `editor` section `gopls` should be installed. Running `doom sync` will finish the setup.");
docs.push({"id":15,"title":"DOOM Emacs Go","url":"/doom-emacs-go.html"});
index.add(16, "# Deno types&#10;## Union types&#10;```typescript&#10;function add(a: any, b: any) { if (typeof a === &apos;number&apos; &amp;&amp; typeof b === &apos;number&apos;) { return a + b; } if (typeof a === &apos;string&apos; &amp;&amp; typeof b === &apos;string&apos;) { return a.concat(b); } throw new Error(&apos;Parameters must be numbers or strings&apos;);&#10;}&#10;``` ```typescript&#10;add(true, false);&#10;``` ```&#10;evalmachine.&lt;anonymous&gt;:10 throw new Error(&apos;Parameters must be numbers or strings&apos;); ^ Error: Parameters must be numbers or strings at Proxy.add (evalmachine.&lt;anonymous&gt;:10:11) at evalmachine.&lt;anonymous&gt;:3:30 at evalmachine.&lt;anonymous&gt;:5:3&#10;&#27;[90m at sigintHandlersWrap (node:vm:271:12)&#27;[39m&#10;&#27;[90m at Script.runInThisContext (node:vm:129:14)&#27;[39m&#10;&#27;[90m at Object.runInThisContext (node:vm:308:38)&#27;[39m at Object.execute (/Users/rui/.asdf/installs/nodejs/15.11.0/.npm/lib/node_modules/&#27;[4mtslab&#27;[24m/dist/executor.js:162:38) at JupyterHandlerImpl.handleExecuteImpl (/Users/rui/.asdf/installs/nodejs/15.11.0/.npm/lib/node_modules/&#27;[4mtslab&#27;[24m/dist/jupyter.js:219:38) at /Users/rui/.asdf/installs/nodejs/15.11.0/.npm/lib/node_modules/&#27;[4mtslab&#27;[24m/dist/jupyter.js:177:57 at async JupyterHandlerImpl.handleExecute (/Users/rui/.asdf/installs/nodejs/15.11.0/.npm/lib/node_modules/&#27;[4mtslab&#27;[24m/dist/jupyter.js:177:21) ``` ```typescript&#10;function add(a: number | string, b: number | string) { if (typeof a === &apos;number&apos; &amp;&amp; typeof b === &apos;number&apos;) { return a + b; } if (typeof a === &apos;string&apos; &amp;&amp; typeof b === &apos;string&apos;) { return a.concat(b); } throw new Error(&apos;Parameters must be numbers or strings&apos;);&#10;}&#10;``` ```typescript&#10;add(true, false);&#10;``` ```&#10;1:5 - Argument of type &apos;boolean&apos; is not assignable to parameter of type &apos;string | number&apos;. ``` ```typescript&#10;add(1, 3);&#10;``` ```&#10;&#27;[33m4&#27;[39m ``` ```typescript&#10;add(&apos;a&apos;, &apos;b&apos;);&#10;``` ```&#10;ab ``` ```typescript ```&#10;");
docs.push({"id":16,"title":"Deno types","url":"/deno-types.html"});
index.add(17, "# Deno ## Installation ### Fedora To install Deno on Fedora, first download the installation file: ```shell&#10;curl -fsSL https://deno.land/x/install/install.sh | sh&#10;``` And then add the following to your shell&apos;s profile (*e.g.* `~/.bashrc`): ```shell&#10;export DENO_INSTALL=&quot;/home/$USER/.deno&quot;&#10;export PATH=&quot;$DENO_INSTALL/bin:$PATH&quot;&#10;```");
docs.push({"id":17,"title":"Deno","url":"/deno.html"});
index.add(18, "# Digital garden Digital gardens are places *where information grows*. ## Challenges * Chronological is the wrong metaphor, but how to capture time and sequence? * Example, have a [Git](git.html) commit hash as well as a recent changes history.");
docs.push({"id":18,"title":"Digital Garden","url":"/digital-garden.html"});
index.add(19, "# Distance metrics ## L-p metrics ### Manhattan distance (L1) Given two vectors $p$ and $q$, such that $$&#10;\\begin{aligned}&#10;p &amp;= \\left(p_1, p_2, \\dots,p_n\\right) \\\\&#10;q &amp;= \\left(q_1, q_2, \\dots,q_n\\right)&#10;\\end{aligned}&#10;$$ we define the Manhattan distance as: $$&#10;d_1(p, q) = \\|p - q\\|_1 = \\sum_{i=1}^n |p_i-q_i|&#10;$$ ### Euclidean distance (L2) In general, for points given by Cartesian coordinates in $n$-dimensional Euclidean space, the distance is $$&#10;d(p,q)=\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\cdots +(p_{i}-q_{i})^{2}+\\cdots +(p_{n}-q_{n})^{2}}&#10;$$ ## Cluster distances ### Within-cluster sum of squares (WCSS) Given a set of observations ($x_1, x_2,\\dots,x_n$), where each observation is a $d$-dimensional real vector, $k$-means clustering aims to partition the $n$ observations into $k$ ($\\leq n$) sets $S=\\lbrace S_1, S_2, \\dots, S_k\\rbrace$ so as to minimize the within-cluster sum of squares (WCSS) (*i.e.* variance). Formally, the objective is to find: $$&#10;{\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i}&#10;$$ where $\\mu_i$ is the mean of points in $S_i$. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster: $$&#10;{\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\,{\\frac {1}{2|S_{i}|}}\\,\\sum _{\\mathbf {x} ,\\mathbf {y} \\in S_{i}}\\left\\|\\mathbf {x} -\\mathbf {y} \\right\\|^{2}}&#10;$$ The equivalence can be deduced from identity $${\\displaystyle \\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}=\\sum _{\\mathbf {x} \\neq \\mathbf {y} \\in S_{i}}(\\mathbf {x} -{\\boldsymbol {\\mu }}_{i})({\\boldsymbol {\\mu }}_{i}-\\mathbf {y} )}.&#10;$$ Because the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in _different_ clusters (between-cluster sum of squares, BCSS) which follows from the law of total variance. ### Dunn index A full explanation is available at [Dunn index](dunn-index.html).");
docs.push({"id":19,"title":"Distance metrics","url":"/distance-metrics.html"});
index.add(20, "# Dunn index There are several ways to measure the robustness of a clustering algorithm. Three commonly used metrics are the **Dunn index**, **Davis-Bouldin index** and **Silhoutte index**. But before we start, let&apos;s introduce some concepts. We are interested in clustering algorithms for a dataset $\\mathcal{D}$ with $N$ elements in a $n$-dimensional real space, that is: $$&#10;\\mathcal{D} = {x_1, x_2, \\ldots, x_N} \\in \\mathbb{R}^p&#10;$$ The clustering algorithm will create a set $C$ of $K$ distinct disjoint groups from $\\mathcal{D}$ $C={c_1, c_2, \\ldots, c_k}$, such that: $$&#10;\\cup_{c_k\\in C}c_k=\\mathcal{D} \\\\&#10;c_k \\cap c_l \\neq \\emptyset \\forall k\\neq l&#10;$$ Each group (or cluster) $c_k$, will have a **centroid**, $\\bar{c}_k$, which is the mean vector of its elements such that: $$&#10;\\bar{c}_k=\\frac{1}{|c_k|}\\sum_{x_i \\in c_k}x_i&#10;$$ We will also make use of the dataset&apos;s mean vector, $\\bar{\\mathcal{D}}$, defined as: $$&#10;\\bar{\\mathcal{D}}=\\frac{1}{N}\\sum_{x_i \\in X}x_i&#10;$$ ## Dunn index The **Dunn index**[^1] aims at quantifying the compactness and variance of the clustering.&#10;A cluster is considered **compact** if there is small variance between members of the cluster.&#10;This can be calculated using $\\Delta(c_k)$, where $$&#10;\\Delta(c_k) = \\max_{x_i, x_j \\in c_k}{d_e(x_i, x_j)}&#10;$$ and $d_e$ is the [Euclidian distance](distance-metrics.html#Euclidean distance L2) defined as: $$&#10;d_e=\\sqrt{\\sum_{j=1}^p (x_{ij}-x_{kj})^2}.&#10;$$ A cluster is considered *well separated* if the cluster are far-apart. This can quantified using $$&#10;\\delta(c_k, c_l) = \\min_{x_i \\in c_k}\\min_{x_j\\in c_l}{d_e(x_i, x_j)}.&#10;$$ Given these quantities, the *Dunn index* for a set of clusters $C$, $DI(C)$, is then defined by: $$&#10;DI(C)=\\frac{\\min_{c_k \\in C}{\\delta(c_k, c_l)}}{\\max_{c_k\\in C}\\Delta(c_k)}&#10;$$ A higher *Dunn Index* will indicate compact, well-separated clusters, while a lower index will indicate less compact or less well-separated clusters. We can now try to calculate the metric for the dataset we&apos;ve created previously.&#10;Let&apos;s simulate some data and apply the Dunn index from scratch.&#10;First, we will create a compact and well-separated dataset using the [`make_blobs`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make\\_blobs.html) method in `scikit-learn`.&#10;We will create a dataset of $\\mathbb{R}^2$ data (for easier plotting), with three clusters.&#10;```python&#10;from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=23)&#10;``` ```python&#10;import pandas as pd&#10;from plotnine import *&#10;from plotnine.data import *&#10;from plotutils import * data = pd.DataFrame(X, columns=[&quot;x1&quot;, &quot;x2&quot;])&#10;data[&quot;y&quot;] = y&#10;data[&quot;y&quot;] = data.y.astype(&apos;category&apos;) ggplot(data=data) +\\&#10;geom_point(mapping=aes(x=&quot;x1&quot;, y=&quot;x2&quot;, colour=&quot;y&quot;)) + \\&#10;scale_color_manual(values=[colours[0], colours[1], colours[2]]) + theme_classic()&#10;``` ![dunn-index_1](./images/dunn-index_1.png)&#10;We now cluster the data[^2] and we will have, as expected three distinct clusters, plotted below.&#10;```python&#10;from sklearn import cluster k_means = cluster.KMeans(n_clusters=3)&#10;k_means.fit(data)&#10;y_pred = k_means.predict(data) prediction = pd.concat([data, pd.DataFrame(y_pred, columns=[&apos;pred&apos;])], axis = 1) clus0 = prediction.loc[prediction.pred == 0] clus1 = prediction.loc[prediction.pred == 1] clus2 = prediction.loc[prediction.pred == 2] k_list = [clus0.values, clus1.values,clus2.values]&#10;``` Let&apos;s focus now on two of these cluster, let&apos;s call them $c_k$ and $c_l$.&#10;```python&#10;ck = k_list[0]&#10;cl = k_list[1]&#10;``` We know we have to calculate the distance between the points in $c_k$ and $c_l$. We know that the `len(ck)=len(cl)=333` we create&#10;```python&#10;import numpy as np values = np.ones([len(ck), len(cl)])&#10;values&#10;``` For each pair of points, we then get the norm of $x_i-x_j$. For instance, for $i=0\\in c_k$ and $i=1\\in c_l$, we would have:&#10;```python&#10;values[0, 1] = np.linalg.norm(ck[0]-cl[1])&#10;print(ck[0], cl[1])&#10;print(values[0, 1])&#10;``` ```&#10;[1.76127766 9.39696306 0. 0. ] [ 5.46312794 -3.08938807 1. 1. ]&#10;13.100101521169044 ``` The calculation of $\\delta(c_k, c_l)$ between two clusters $c_k$ and $c_l$ will be defined as follows:&#10;```python&#10;import numpy as np def &delta;(ck, cl): values = np.ones([len(ck), len(cl)]) for i in range(0, len(ck)): for j in range(0, len(cl)): values[i, j] = np.linalg.norm(ck[i]-cl[j]) return np.min(values)&#10;``` So, for our two clusters above, $\\delta(c_k, c_l)$ will be:&#10;```python&#10;&delta;(ck, cl)&#10;``` Within a single cluster $c_k$, we can calculate $\\Delta(c_k)$ similarly as:&#10;```python&#10;def &Delta;(ci): values = np.zeros([len(ci), len(ci)]) for i in range(0, len(ci)): for j in range(0, len(ci)): values[i, j] = np.linalg.norm(ci[i]-ci[j]) return np.max(values)&#10;``` So, for instance, for our $c_k$ and $c_l$ we would have:&#10;```python&#10;print(&Delta;(ck))&#10;print(&Delta;(cl))&#10;``` ```&#10;5.8077337425156745&#10;6.173844284636552 ``` We can now define the *Dunn index* as&#10;```python&#10;def dunn(k_list): &delta;s = np.ones([len(k_list), len(k_list)]) &Delta;s = np.zeros([len(k_list), 1]) l_range = list(range(0, len(k_list))) for k in l_range: for l in (l_range[0:k]+l_range[k+1:]): &delta;s[k, l] = &delta;(k_list[k], k_list[l]) &Delta;s[k] = &Delta;(k_list[k]) di = np.min(&delta;s)/np.max(&Delta;s) return di&#10;``` and calculate the *Dunn index* for our clustered values list as&#10;```python&#10;dunn(k_list)&#10;``` Intuitively, we can expect a dataset with less well-defined clusters to have a lower *Dunn index*. Let&apos;s try it.&#10;We first generate the new dataset.&#10;```python&#10;X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=10.0, random_state=24) df = pd.DataFrame(X, columns=[&apos;A&apos;, &apos;B&apos;]) k_means = cluster.KMeans(n_clusters=3) k_means.fit(df) #K-means training y_pred = k_means.predict(df) prediction = pd.concat([df,pd.DataFrame(y_pred, columns=[&apos;pred&apos;])], axis = 1)&#10;prediction[&quot;pred&quot;] = prediction.pred.astype(&apos;category&apos;)&#10;``` ```python&#10;ggplot(data=prediction) +\\&#10;geom_point(mapping=aes(x=&quot;A&quot;, y=&quot;B&quot;, colour=&quot;pred&quot;)) + \\&#10;scale_color_manual(values=[colours[0], colours[1], colours[2]]) + theme_classic()&#10;``` ![dunn-index_2](./images/dunn-index_2.png)&#10;```python&#10;clus0 = prediction.loc[prediction.pred == 0]&#10;clus1 = prediction.loc[prediction.pred == 1]&#10;clus2 = prediction.loc[prediction.pred == 2] k_list = [clus0.values, clus1.values,clus2.values]&#10;``` ```python&#10;dunn(k_list)&#10;``` ```python ```&#10;");
docs.push({"id":20,"title":"Dunn index","url":"/dunn-index.html"});
index.add(21, "# Emacs Notes on Emacs. * My current flavour/distribution of Emacs is [DOOM Emacs](https://github.com/hlissner/doom-emacs). Some configuration/usage notes: * [Go support in DOOM Emacs](doom-emacs-go.html)");
docs.push({"id":21,"title":"Emacs","url":"/emacs.html"});
index.add(22, "# Error metrics ## Root Mean Squared error A typical way of measuring the difference between observations and results from a predictor. The formal definition is: $$&#10;\\begin{aligned}&#10;RMSE(\\hat{\\theta}) &amp;= \\sqrt{\\operatorname{MSE}(\\hat{\\theta})} \\\\&#10;&amp;= \\sqrt{\\operatorname{E}((\\hat{\\theta}-\\theta)^2)}.&#10;\\end{aligned}&#10;$$ For $N$ observations $Y=\\{y_1,\\dots,y_N\\}$ we can express it as: $$&#10;RMSE=\\sqrt{\\frac{\\sum_{n=1}^{N}({\\hat {y}}_{n}-y_{n})^{2}}{N}}.&#10;$$");
docs.push({"id":22,"title":"Error metrics","url":"/error-metrics.html"});
index.add(23, "&#10;# Topics * [Counterfactuals](counterfactuals.html) * [Counterfactuals with Constraint Solvers](counterfactuals-with-constraint-solvers.html)&#10;* Fairness * [Counterfactual Fairness](counterfactual-fairness.html) # Resources&#10;* A nice presentation on AI/ML explainability: [https://explainml-tutorial.github.io/neurips20](https://explainml-tutorial.github.io/neurips20)");
docs.push({"id":23,"title":"Explainability","url":"/explainability.html"});
index.add(24, "# GPG ## Installation ### macOS You can install GPG on macOS using: ```shell&#10;$ brew install gpg&#10;``` ## Troubleshooting ### macOS #### Can&apos;t access keychain from UI If a program cannot access the keychain from the UI, probably there&apos;s some problem in prompting you for the passphrase. You can install, for instance `pinentry` to solve this. Install it with ```shell&#10;$ brew install pinentry-mac&#10;``` and then register `pinentry` as the passphrase input option: ```shell&#10;$ echo &quot;pinentry-program /usr/local/bin/pinentry-mac&quot; &gt;&gt; ~/.gnupg/gpg-agent.conf&#10;```");
docs.push({"id":24,"title":"GPG","url":"/gpg.html"});
index.add(25, "# Gemini&#10;This site is also available on Gemini: [gemini://hollowed.space](gemini://hollowed.space). A recommended graphical Gemini browser is [Lagrange](https://github.com/skyjake/lagrange) or [Amfora](https://github.com/makeworld-the-better-one/amfora) for a text-based/terminal browser. ## Syntax * Gemini syntax is quite similar to Markdown ## Setup * Using the `agate` server&#10;");
docs.push({"id":25,"title":"Gemini","url":"/gemini.html"});
index.add(26, "# Git ## Hooks ### pre-push The pre-push script is called by `git push`the push actually happens. If the exist status is 0, then the push will proceed, otherwise it will be stopped. The script is supplied with the following arguments: ``` $1 -- Name of the remote to which the push is being done (Ex: origin) $2 -- URL to which the push is being done (Ex: https://&lt;host&gt;:&lt;port&gt;/&lt;username&gt;/&lt;project_name&gt;.git)&#10;``` Information about the commits which are being pushed is supplied as lines to the standard input in the form: ```&#10;&lt;local_ref&gt; &lt;local_sha1&gt; &lt;remote_ref&gt; &lt;remote_sha1&gt;&#10;``` Sample values: ```&#10;local_ref = refs/heads/master&#10;local_sha1 = 68a07ee4f6af8271dc40caae6cc23f283122ed11&#10;remote_ref = refs/heads/master&#10;remote_sha1 = efd4d512f34b11e3cf5c12433bbedd4b1532716f&#10;``` ");
docs.push({"id":26,"title":"Git","url":"/git.html"});
index.add(27, "# Go filesystem operations Notes on Go filesystem operations. ## Copying files Go does not have an utility method to copy files. We have to rely on writing our own implementation using the reading and writing functionality in other packages. As an example: ```go&#10;package main import ( &quot;io&quot; &quot;log&quot; &quot;os&quot;&#10;) func main() { from, err := os.Open(&quot;./foo.txt&quot;) if err != nil { log.Fatal(err) } defer from.Close() to, err := os.OpenFile(&quot;./bar.txt&quot;, os.O_RDWR|os.O_CREATE, 0666) if err != nil { log.Fatal(err) } defer to.Close() _, err = io.Copy(to, from) if err != nil { log.Fatal(err) }&#10;}&#10;``` ## Path operations ### Basepath To get the basepath of a path string we use the [Dir](https://golang.org/pkg/path/#Dir) method: ```go&#10;filepath.Dir(&quot;/etc/foo/file.txt&quot;) // &quot;/etc/foo&quot;&#10;``` ### Check if directory exists ```go&#10;if _, err := os.Stat(&quot;/etc/foo/&quot;); os.IsNotExist(err) { // do something because it does not exist&#10;}&#10;``` ### Create nested directories Use [MkdirAll](https://golang.org/pkg/os/#MkdirAll): ```go&#10;os.MkdirAll(&quot;/etc/long/nested/path/to/create&quot;, os.ModePerm)&#10;```");
docs.push({"id":27,"title":"Go filesystem operations","url":"/go-filesystem-operations.html"});
index.add(28, "# Go resource bundling Notes on the installation and usage of [pkger](https://github.com/markbates/pkger). Installation done with ```shell&#10;go get github.com/markbates/pkger/cmd/pkger&#10;``` `pkger` works by bundling the resources with a code-generated `pkg.go`. The configuration of assets to be bundled is done by reflection at compile time and not direct configuration.&#10;This is done by replacing standard Go file operations with `pkger` proxy ones, such as: ```go&#10;type Pkger interface { Parse(p string) (Path, error) Current() (here.Info, error) Info(p string) (here.Info, error) Create(name string) (File, error) MkdirAll(p string, perm os.FileMode) error Open(name string) (File, error) Stat(name string) (os.FileInfo, error) Walk(p string, wf filepath.WalkFunc) error Remove(name string) error RemoveAll(path string) error&#10;}&#10;type File interface { Close() error Info() here.Info Name() string Open(name string) (http.File, error) Path() Path Read(p \\[\\]byte) (int, error) Readdir(count int) (\\[\\]os.FileInfo, error) Seek(offset int64, whence int) (int64, error) Stat() (os.FileInfo, error) Write(b \\[\\]byte) (int, error)&#10;}&#10;``` ## Example Bundling a Go template file. ```go&#10;tmplFile, _ := pkger.Open(&quot;/templates/page.tmpl&quot;)&#10;tmplBytes, _ := ioutil.ReadAll(tmplFile)&#10;tmplString := string(tmplBytes) tpl, err := template.New(&quot;page&quot;).Parse(tmplString) _ = tpl.Execute(f, ...)&#10;``` The bundling is simply done by running ```shell&#10;pkger&#10;``` and building as usual ```shell&#10;go build&#10;```");
docs.push({"id":28,"title":"Go resource bundling","url":"/go-resource-bundling.html"});
index.add(29, "# The Go language&#10;Some notes regarding the [Go](https://golang.org/) language.&#10;Some topics have graduated to their own page: * [Go resource bundling](go-resource-bundling.html)&#10;* [Go filesystem operations](go-filesystem-operations.html) ## Language design ### Go doesn&apos;t have sets The Go language, notoriously, does not have[^1] some common data structures like sets. There are two main reasons for that: 1. Go does not have generics[^1]&#10;2. Go relies on you writing your own data structures, generally Go lacks generics, which prevent writing a ... well, generic and efficient set implementation.&#10;Also, writing your own (non-generic) set with `map`s is quite straight-forward.&#10;The usual structure for a type `T` is `map[T]bool`, where the key is the element and the value is just a placeholder. For instance, for a `int` set: ```go&#10;s := map[int]bool{1: true, 3: true}&#10;``` where we can add elements: ```go&#10;s[1] = true // already present&#10;s[2] = true // adds new element&#10;``` Some other techniques for maps replacing sets: #### Set union ```go&#10;set_union := map[int]bool{} for k, _ := range set_1{ set_union[k] = true&#10;}&#10;for k, _ := range set_2{ set_union[k] = true&#10;}&#10;``` #### Set intersection ```go&#10;set_intersection := map[int]bool{}&#10;for k,_ := range set_1 { if set_2[k] { set_intersection[k] = true }&#10;} ``` #### Set to array To convert a (map) set to an array: ```go&#10;array := make([]int, 0) for k := range set_1 { array = append(array, k) }&#10;``` [^1]: As of the time of writing, that is Go 1.15. ## CI&#10;### GitHub A potential workflow for GitHub is to use [GitHub Actions for Go](https://github.com/mvdan/github-actions-golang).&#10;An example workflow file, `.github/workflows/test.yml`, which runs `go test` (see [Go](go.html#Testing in Go)) and `go vet` is: ```yaml&#10;on: [push, pull_request\\]&#10;name: Test&#10;jobs: test: strategy: matrix: go-version: [1.14.x, 1.15.x] os: [ubuntu-latest] runs-on: ${{ matrix.os }} steps: - name: Install Go uses: actions/setup-go@v2 with: go-version: ${{ matrix.go-version }} - name: Checkout code uses: actions/checkout@v2 - name: Test run: go test ./... - name: Vet run: go vet ./...&#10;``` ## Containers ### Minimal example A minimal example of a Go [container](containers.html) configuration for a web server running on port `8080`: ```Dockerfile&#10;# Start from the latest golang base image FROM golang:latest # Add Maintainer Info LABEL maintainer=&quot;Rui Vieira&quot; # Set the Current Working Directory inside the container WORKDIR /app # Copy go mod and sum files COPY go.mod go.sum ./ # Download all dependencies. Dependencies will be cached if the go.mod and go.sum files are not changed RUN go mod download # Copy the source from the current directory to the Working Directory inside the container COPY . . # Build the Go app RUN go build -o main . # Expose port 8080 to the outside world EXPOSE 8080 # Command to run the executable CMD [&quot;./main&quot;]&#10;``` ## Reference ### Conversions&#10;#### How to convert a string to byte array? ```go&#10;b := []byte(&quot;This is a string&quot;)&#10;``` ### Collections #### Sort map keys alphabetically If a `map` contains `string` keys, i.e. `var myMap map[string]T`, we must sort the map keys independently. For instance: ```go&#10;keys := make([]string, 0)&#10;for k, _ := range myMap { keys = append(keys, k)&#10;}&#10;sort.Strings(keys)&#10;for _, k := range keys { fmt.Println(k, myMap[k])&#10;}&#10;``` #### Check for element If we consider a collection, say, `[]string collection`, the way to check for an element already present is, for instance: ```go&#10;func existsIn(needle string, haystack []string) bool { for _, element := range haystack { if element == needle { return true } } return false&#10;}&#10;``` ### Templates #### Check if variable empty In a [Go](go.html) template you check if a variable is empty by doing: ```go&#10;{{if .Items}} &lt;ul&gt; {{range .Items}} &lt;li&gt;{{.Name}}&lt;/li&gt; {{end}} &lt;/ul&gt;&#10;{{end}}&#10;``` #### Looping over a map Looping over the map `var data map[string]bool` in a Go template: ```go&#10;{{range $index, $element := .}} {{$index}}: {{$element}}&#10;{{end}}&#10;``` ### Processes #### Executing external processes Executing an external process and directing input and output to `Stdout` and `Stderr`. ```go&#10;cmd := exec.Command(&quot;ls&quot;, &quot;-1ao&quot;)&#10;cmd.Stdout = os.Stdout&#10;cmd.Stderr = os.Stderr&#10;err := cmd.Run()&#10;if err != nil { log.Fatalf(&quot;cmd.Run() failed with %s\\\\n&quot;, err)&#10;}&#10;``` ### Testing in Go Place the tests in your place of choosing, but keep the package declaration. Test functions should be parameterised as `(t *testing.T` and start with the prefix `Test`, for instance: ```go&#10;package main func TestFoo(t *testing.T) { value := Foo(5, 5) // ... assertions&#10;``` The test files themselves must have the suffix `*_test.go`.&#10;Call the tests with `go test`. ");
docs.push({"id":29,"title":"Go","url":"/go.html"});
index.add(30, "# Gradient-free optimisation * [Hill-climbing optimisation](hill-climbing-optimisation.html)");
docs.push({"id":30,"title":"Gradient-free optimisation","url":"/gradient-free-optimisation.html"});
index.add(31, "# Hill-climbing optimisation&#10;## Global maximum Let&apos;s try it with the function $$&#10;f(x,y) = e^{-\\left(x^2+y^2\\right)}&#10;$$&#10;```python&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;from plotutils import * x = np.linspace(-2.0, 2.0, 1000)&#10;y = np.linspace(-2.0, 2.0, 1000)&#10;X, Y = np.meshgrid(x, y) Z = np.exp(-(X**2 + Y**2)) fig,ax=plt.subplots(1,1)&#10;cp = ax.contourf(X, Y, Z, cmap=cmaps[1]) ax.set_title(&apos;f(x,y)&apos;)&#10;ax.set_xlabel(&apos;x&apos;)&#10;ax.set_ylabel(&apos;y&apos;)&#10;plt.show()&#10;``` ![hill-climbing-optimisation_1](./images/hill-climbing-optimisation_1.png)&#10;```python&#10;from gradient_free_optimizers import HillClimbingOptimizer search_space = { &quot;x&quot;: x, &quot;y&quot;: y,&#10;}&#10;``` ```python&#10;opt = HillClimbingOptimizer(search_space)&#10;``` ```python&#10;def f(pos): x = pos[&quot;x&quot;] y = pos[&quot;y&quot;] z = np.exp(-(x**2 + y**2)) return z&#10;``` ```python&#10;result = opt.search(f, n_iter=30000, verbosity=[&apos;print_times&apos;], random_state=23)&#10;``` ``` Evaluation time : 0.8271605968475342 sec [33.25 %] Optimization time : 1.6604328155517578 sec [66.75 %] Iteration time : 2.487593412399292 sec [12059.85 iter/sec] ``` ```python&#10;opt.results&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;score&lt;/th&gt; &lt;th&gt;x&lt;/th&gt; &lt;th&gt;y&lt;/th&gt; &lt;th&gt;eval_time&lt;/th&gt; &lt;th&gt;iter_time&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.010&lt;/td&gt; &lt;td&gt;1.792&lt;/td&gt; &lt;td&gt;1.195&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.015&lt;/td&gt; &lt;td&gt;1.888&lt;/td&gt; &lt;td&gt;-0.815&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;0.411&lt;/td&gt; &lt;td&gt;-0.667&lt;/td&gt; &lt;td&gt;-0.667&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.411&lt;/td&gt; &lt;td&gt;-0.667&lt;/td&gt; &lt;td&gt;0.667&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;0.411&lt;/td&gt; &lt;td&gt;0.667&lt;/td&gt; &lt;td&gt;-0.667&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;...&lt;/th&gt; &lt;td&gt;...&lt;/td&gt; &lt;td&gt;...&lt;/td&gt; &lt;td&gt;...&lt;/td&gt; &lt;td&gt;...&lt;/td&gt; &lt;td&gt;...&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29995&lt;/th&gt; &lt;td&gt;0.953&lt;/td&gt; &lt;td&gt;0.178&lt;/td&gt; &lt;td&gt;0.126&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29996&lt;/th&gt; &lt;td&gt;0.981&lt;/td&gt; &lt;td&gt;-0.138&lt;/td&gt; &lt;td&gt;0.018&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29997&lt;/th&gt; &lt;td&gt;0.991&lt;/td&gt; &lt;td&gt;-0.082&lt;/td&gt; &lt;td&gt;0.050&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29998&lt;/th&gt; &lt;td&gt;0.995&lt;/td&gt; &lt;td&gt;-0.058&lt;/td&gt; &lt;td&gt;-0.038&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29999&lt;/th&gt; &lt;td&gt;0.974&lt;/td&gt; &lt;td&gt;0.158&lt;/td&gt; &lt;td&gt;0.042&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;p&gt;30000 rows &times; 5 columns&lt;/p&gt;&#10;&lt;/div&gt;&#10;![SegmentLocal](./images/hill-climbing-1.gif &quot;segment&quot;)&#10;## Local maximum Let&apos;s try it with the function $$&#10;f(x,y) = e^{-\\left(x^2+y^2\\right)}+2e^{-\\left((x-1.7)^2+(y-1.7)^2\\right)}&#10;$$&#10;```python&#10;x = np.linspace(-1.0, 3.0, 1000)&#10;y = np.linspace(-1.0, 3.0, 1000)&#10;X, Y = np.meshgrid(x, y) Z = np.exp(-(X**2 + Y**2))+2*np.exp(-((X-1.7)**2+(Y-1.7)**2)) fig,ax=plt.subplots(1,1)&#10;cp = ax.contourf(X, Y, Z, cmap=cmaps[1]) ax.set_title(&apos;f(x,y)&apos;)&#10;ax.set_xlabel(&apos;x&apos;)&#10;ax.set_ylabel(&apos;y&apos;)&#10;plt.show()&#10;``` ![hill-climbing-optimisation_2](./images/hill-climbing-optimisation_2.png)&#10;```python&#10;opt = HillClimbingOptimizer(search_space)&#10;``` ```python&#10;def f(pos): x = pos[&quot;x&quot;] y = pos[&quot;y&quot;] z = np.exp(-(x**2 + y**2))+2*np.exp(-((x-1.7)**2+(y-1.7)**2)) return z&#10;``` ```python&#10;result = opt.search(f, n_iter=30000, verbosity=[&apos;print_times&apos;], random_state=23)&#10;``` ``` Evaluation time : 0.9191639423370361 sec [35.42 %] Optimization time : 1.6756830215454102 sec [64.58 %] Iteration time : 2.5948469638824463 sec [11561.38 iter/sec] ``` ```python&#10;opt.results&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;score&lt;/th&gt; &lt;th&gt;x&lt;/th&gt; &lt;th&gt;y&lt;/th&gt; &lt;th&gt;eval_time&lt;/th&gt; &lt;th&gt;iter_time&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;1.547&lt;/td&gt; &lt;td&gt;1.792&lt;/td&gt; &lt;td&gt;1.195&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.018&lt;/td&gt; &lt;td&gt;1.888&lt;/td&gt; &lt;td&gt;-0.815&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;0.411&lt;/td&gt; &lt;td&gt;-0.667&lt;/td&gt; &lt;td&gt;-0.667&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.414&lt;/td&gt; &lt;td&gt;-0.667&lt;/td&gt; &lt;td&gt;0.667&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;0.414&lt;/td&gt; &lt;td&gt;0.667&lt;/td&gt; &lt;td&gt;-0.667&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;...&lt;/th&gt; &lt;td&gt;...&lt;/td&gt; &lt;td&gt;...&lt;/td&gt; &lt;td&gt;...&lt;/td&gt; &lt;td&gt;...&lt;/td&gt; &lt;td&gt;...&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29995&lt;/th&gt; &lt;td&gt;1.915&lt;/td&gt; &lt;td&gt;1.872&lt;/td&gt; &lt;td&gt;1.820&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29996&lt;/th&gt; &lt;td&gt;1.963&lt;/td&gt; &lt;td&gt;1.556&lt;/td&gt; &lt;td&gt;1.712&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29997&lt;/th&gt; &lt;td&gt;1.984&lt;/td&gt; &lt;td&gt;1.612&lt;/td&gt; &lt;td&gt;1.744&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29998&lt;/th&gt; &lt;td&gt;1.992&lt;/td&gt; &lt;td&gt;1.636&lt;/td&gt; &lt;td&gt;1.656&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;29999&lt;/th&gt; &lt;td&gt;1.954&lt;/td&gt; &lt;td&gt;1.852&lt;/td&gt; &lt;td&gt;1.736&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;td&gt;0.000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;p&gt;30000 rows &times; 5 columns&lt;/p&gt;&#10;&lt;/div&gt;&#10;![SegmentLocal](./images/hill-climbing-2.gif &quot;segment&quot;)&#10;```python ```&#10;");
docs.push({"id":31,"title":"Hill-climbing optimisation","url":"/hill-climbing-optimisation.html"});
index.add(32, "---&#10;date: 2019-04-17T19:21:00+01:00&#10;draft: false&#10;url: &quot;/introduction-to-balanced-box-decomposition-trees.html&quot;&#10;--- # Introduction to Balanced Box-Decomposition Trees Stardate 96893.29. You are the USS Euler&apos;s Science Officer at a moment when the computer graphical displays and voice systems went down. You only have enough deuterium for a short travel and need to find the nearest star system. This is not a simple matter of looking at a chart. You have multiple dimensions in which you can travel. In a bid for galactic peace, the Federation mandated that _both_ Emacs and Vim should be installed in all computers. You open your favourite editor and, fortunately, know exactly how to formulate the solution to your problem: **a $d$-dimensional nearest neighbour algorithm**. Given a dataset $\\mathcal{D}$ of $n$ points in a space $X$ we want to be able to tell which are the _closest_ point to a query point $q \\in X$, preferably in a way which is computationally cheaper than _brute force_ methods (_e.g._ iterating through all of the points) which typically solve this problem in $\\mathcal{O}(dn)$ [^arya1998]. $X$ could have $d$ dimensions (that is $\\mathcal{D} \\subset X : \\mathbb{R}^d$) and we define _closest_ using[^1] Minkowski distance metrics, that is: [^1]: The $L_m$ distance may be pre-computed in this method to avoid recalculation for each query. $$&#10;L_m = \\left(\\sum_{i=1}^d |p_i - q_i|^m\\right)^{\\frac{1}{m}},\\qquad p,q \\in X : \\mathbb{R}^d.&#10;$$ A potential solution for this problem would be to use _kd_-trees, which for low dimension scenarios provide $\\mathcal{O}(\\log n)$ query times [^friedman1977]. However, as the number of dimensions increase (as quickly as $d&gt;2$) the query times also increase as $2^d$. The case can be made then for _approximate_ nearest neighbour (NN) algorithms and that&apos;s precisely what we will discuss here, namely the _Balanced Box-Decomposition Tree_ (BBD, [^arya1998]). The definition of _approximate_ NN for a query point $q$ can be given as $$&#10;\\text{dist}(p, q) \\leq (1+\\epsilon)\\text{dist}(p^{\\star},q),\\qquad \\epsilon &gt; 0,&#10;$$ where $p$ is the _approximate_ NN and $p^{\\star}$ is the _true_ NN. Let&apos;s consider, for the sake of visualisation, a small two dimensional dataset $\\mathcal{D} \\to \\mathbb{R}^2$ as shown in Figure 1. ![](./images/bbdtrees/small_data.png)&#10;**Figure 1.** A small test dataset in $\\mathbb{R}^2, n=7$. ## Space decomposition BBD trees belong to the category of hierarchical space decomposition trees. In BBD trees, specifically, space is divided in $d$-dimensional rectangles and _cells_. Cells can either represent another $d$-dimensional rectangle or the intersection of two rectangles (one, the _outer box_ fully enclosing the other, the _inner box_). Another important distinction of BBD trees is that rectangle&apos;s _size_ (in this context, the largest length in all of the $d$ dimensions) is bounded by a constant value.&#10;The space decomposition must follow an additional rule which is boxes must be _sticky_. If we consider a inner box $[x_{inner}, y_{inner}]$ contained in a outer box $[x_{outer}, y_{outer}]$, such that $$&#10;[x_{inner}, y_{inner}] \\subseteq [x_{outer}, y_{outer}],&#10;$$ then, considering $w = y_{inner} - x_{inner}$, the box is considered _sticky_ if either $$&#10;\\begin{aligned}&#10;x_{inner}-x_{outer} = 0 &amp;\\lor x_{inner}-x_{outer} \\nleq w \\\\&#10;y_{outer}-y_{inner} = 0 &amp;\\lor y_{outer}-y_{inner} \\nleq w.&#10;\\end{aligned}&#10;$$ An illustration of the stickiness concept can viewed in the diagram below. ![](./images/bbdtrees/sticky.png)&#10;**Figure 2.** Visualisation of the &quot;stickiness&quot; criteria for \\(\\mathbb{R}^2\\) rectangles. Stickiness provides some important geometric properties to the space decomposition which will be discussed further on. The actual process of space decomposition will produce a tree of nodes, each with an associated $d$-dimensional rectangle enclosing a set of points. Each node will be further decomposed into children nodes, containing a region of space with a subset of the parent&apos;s data points. If a node has no children it will be called a _leaf_ node. The division process can occur either by means of: - a _fair split_, this is done by partitioning the space with an hyperplane, resulting in a _low_ and _high_ children nodes&#10;- a _shrink_, splitting the box into a inner box (the _inner_ child) and a outer box (the _outer_ child). ![](./images/bbdtrees/split.png)&#10;**Figure 3.** &quot;Fair split&quot; and &quot;shrinking&quot; division strategies example in $\\mathbb{R}^2$ with respective high/low and outer/inner children. The initial node of the tree, the _root node_, will include all the dataset points, $\\mathcal{D}$. In the Figure 4 we can see a representation of the root node for the dataset presented above. We can see the node boundaries in dashed red lines as well as the node&apos;s center, marked as $\\mu_{root}$. ![](./images/bbdtrees/root_node.png)&#10;**Figure 4.** Associated cell for the BBD-tree root node for the example dataset. Node boundaries in red and node centre labelled as $\\mu_{root}$. The actual method to calculate the division can either be based on the _midpoint algorithm_ or the _middle interval algorithm_. The method used for these examples is the latter, for which more details can be found in [^kosaraju1995]. The next step is to divide the space according to the previously mentioned rules. As an example, we can see the root node&apos;s respective children in Figure 5. ![](./images/bbdtrees/root_children.png)&#10;**Figure 5.** BBD-tree root node&apos;s lower (_left_) and upper (_right_) children. Node boundaries in red and centres labelled with a red cross. This process is repeated until the child nodes are leaves and cannot be divided anymore.&#10;To better visualise the construction process it would be helpful to have a larger tree, so we will now consider still the 2-dimensional case, but now with a larger dataset (Figure 6), consisting of 2000 samples in total, each half from a bivariate Gaussian distribution: $$&#10;\\begin{aligned}&#10;\\text{X}_1 &amp;amp;\\sim \\mathcal{N}([0,0], \\mathbf{I}) \\\\&#10;\\text{X}_2 &amp;amp;\\sim \\mathcal{N}([3, 3], \\mathbf{I}). \\\\&#10;\\end{aligned}&#10;$$ ![](./images/bbdtrees/gaussian_data.png)&#10;**Figure 6.** Larger example dataset in $\\mathbb{R}^2$ consisting of a realisation of $n=2000$ from two bivariate Gaussian distributions centred in $\\mu_1=(0,0)$ and $\\mu_2=(3,3)$ and with $\\Sigma=\\mathbf{I}$. With this larger dataset, we have enough points to illustrate the tree node building. This time, we will start from the root node and always follow either the &quot;lower&quot; nodes or the &quot;upper&quot; nodes (as show in Figure 7). We can clearly see the cells getting smaller, until finally we have a single point included (_i.e._ a _leaf_ node). ![](./images/bbdtrees/gaussian_boxes.gif)&#10;**Figure 7.** BBD-tree node building process for the bivariate dataset. On the left we traverse the upper tree nodes and on the right the lower tree nodes. This division process illustrates an important property of BBD-trees. Although other space decomposition algorithms (such as _kd_-trees) display a geometric reduction of number of points enclosed in each _cell_, methods such as the BBD-tree, which impose constraints on the cell&apos;s size aspect ratio as stated before, display not only a geometric reduction in the number of points, but also in the cell&apos;s size as well. The construction cost of a BBD-tree is $\\mathcal{O}(dn \\log n)$ and the tree itself will have $\\mathcal{O}(n)$ nodes and $\\mathcal{O}(\\log n)$ height. ## Tree querying Now that we have successfully constructed a BBD-tree, we want to actually find the (approximate) nearest neighbour of an arbitrary query point $q$ (Figure 8). ![](./images/bbdtrees/gaussian_query_point.png)&#10;**Figure 8.** Query point $q$ (red) for the bivariate dataset. The first step consists in descending the tree in order to locate the smallest cell containing the query point $q$. This process is illustrated for the bivariate data in Figure 9. ![](./images/bbdtrees/gaussian_query.gif)&#10;**Figure 9.** BBD-tree descent to locate the smallest cell containing $q$ (red). Once the cell has been located, we proceed to enumerate all the _leaf_ nodes contained by it and calculate our distance metric $L_2$ in this case) between the query point $q$ and the leaf nodes, eventually declaring the point with the smallest $L_2$ as the aproximate NN. BBD-trees provide strong guarantees that the ANN will be located within this cell and not in a neighbouring cell. In Figure 10 we zoomed in the smallest cell containing $q$ and show the associated calculated $L_2$ distance for each node. ![](./images/bbdtrees/gaussian_dist.gif)&#10;**Figure 10.** $L_2$ distance between leaf nodes and the query point $q$ inside the smallest cell containing $q$. An important property of BBD-trees is that the tree structure does not need to be recalculated if we change either $\\epsilon$ or if we decide to use another $L_m$ distance metric [^arya1998]. The query time for a point $q$ in a BBD-tree is $\\mathcal{O}(\\log n)$. For comparison, if you recall, the query time for a _brute force_ method is typically $\\mathcal{O}(dn)$. ## Filtering and _k_-NN Great. Now that you solved the USS Euler&apos;s problem, you want to make a suggestion to the federation. Where to place several star-bases and how to divide the system&apos;s coverage between them. An immediate generalisation of this method is easily applicable to the problem of clustering. Note that, at the moment, we are not concerned with determining the &quot;best&quot; clusters for our data[^2]. Given a set of points $Z = \\{z_1, z_2, \\dots, z_n\\}$, we are concerned now in partitioning the data in clusters centred in each of the $Z$ points. A way of looking at this, is that we are building, for each point $z_n$ a Voronoi cell $V(z_n)$. This is achieved by a method called _filtering_. Filtering, in general terms, works by walking the tree with the list of _candidate centres_ ($Z$) and pruning points from the candidate list as we move down. We will denote an arbitrary node as $n$, $z^{\\star}_w$ and $n_w$ respectively as the candidate and the node weight, $z^{\\star}_n$ and $n_n$ as the candidate and node count. The algorithm steps, as detailed in [^kanungo2002], are detailed below: Filter($n$, $Z$) {&#10;$\\qquad C \\leftarrow n.cell$&#10;$\\qquad$ **if** ($n$ is a leaf) {&#10;$\\qquad\\qquad z^{\\star} \\leftarrow$ the closest point in $Z$ to $n.point$&#10;$\\qquad\\qquad z^{\\star}_w \\leftarrow z^{\\star}_w + n.point$&#10;$\\qquad\\qquad z^{\\star}_n \\leftarrow z^{\\star}_n + 1\\qquad$&#10;} &lt;strong&gt;else&lt;/strong&gt; {&#10;$\\qquad\\qquad z^{\\star} \\leftarrow$ the closest point in $Z$ to $C$&apos;s midpoint&#10;$\\qquad\\qquad$**for each** ($z \\in Z \\setminus \\{z^{\\star}\\}$) {&#10;$\\qquad\\qquad\\qquad$ **if** ($z.isFarther(z^{\\star},C)$) {&#10;$\\qquad\\qquad\\qquad\\qquad Z \\leftarrow Z \\setminus \\{z\\}$&#10;$\\qquad\\qquad$}&#10;$\\qquad\\qquad$**if** ($|Z|=1$) {&#10;$\\qquad\\qquad\\qquad z^{\\star}_w \\leftarrow z^{\\star}_w + n_w$&#10;$\\qquad\\qquad\\qquad z^{\\star}_n \\leftarrow z^{\\star}_n + n_n$&#10;$\\qquad\\qquad$} **else** {&#10;$\\qquad\\qquad\\qquad$Filter($n_{left}, Z$)&#10;$\\qquad\\qquad\\qquad$Filter($n_{right}, Z$)&#10;$\\qquad\\qquad$}&#10;} To illustrate the assignment of data points to the centres, we will consider the previous bivariate Gaussian data along with two centres, $z_1 = \\{0,0\\}$ and $z_2 = \\{3, 3\\}$. Figure 11 shows the process of splitting the dataset $\\mathcal{D}$ into two clusters, namely the subsets of data points closer to $z_1$ or $z_2$. ![](./images/bbdtrees/gaussian_filtering.gif)&#10;**Figure 11.** Assignment of points in $\\mathcal{D}$ to $Z$. Data points coloured according to the assigned center. Lines represent the distance from the cells midpoint to $Z$. We can see in Figure 12 the final cluster assignment of the data points. With a $\\mathbb{R}^2$ dataset and only two centres the organisation of points follows a simple perpendicular bisection of the segment connecting the centres, as expected. ![](./images/bbdtrees/gaussian_filtering_clusters.png)&#10;**Figure 12.** Final $\\mathcal{D}$ point assignment to clusters centred in $z_1$ and $z_2$. In Figure 13 we can see more clearly the dataset clusters changing when center $z_1$ is moving around the plane. BBD-trees can play an important role in improving $k$-means performance, as described in [^kanungo2002]. ![](./images/bbdtrees/gaussian_clustering_dynamic.gif)&#10;**Figure 13.** Dynamic assignment of points to a cluster using a BBD-tree. This concludes a (short) introduction to BBD-trees, I hope you enjoyed it. If you have any comments or suggestions, please let me know at [Mastodon](https://mastodon.technology/@ruivieira). [^2]: This would be a _k_-means problem. I intend to write a blog post on _k_-means clustering (and the role BBD-trees can play) in the future.&#10;[^arya1998]: Arya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., &amp;amp; Wu, A. Y. (1998). An optimal algorithm for approximate nearest neighbor searching fixed dimensions. _Journal of the ACM_. https://doi.org/10.1145/293347.293348&#10;[^friedman1977]: Friedman, J. H., &amp;amp; Bentley, J. L. (1977). RA Finkel. An algorithm for finding best matches in logarithmic expected time. _ACM Transactions on Mathematical Software_, 3(3), 209-226.&#10;[^kanungo2002]: Kanungo, T., Mount, D. M., Netanyahu, N. S., Piatko, C. D., Silverman, R., &amp;amp; Wu, A. Y. (2002). An efficient k-means clustering algorithms: Analysis and implementation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, _24_(7), 881&ndash;892. https://doi.org/10.1109/TPAMI.2002.1017616&#10;[^kosaraju1995]: Callahan, P. B., &amp;amp; Kosaraju, S. R. (1995). A decomposition of multidimensional point sets with applications to k-nearest-neighbors and n-body potential fields. _Journal of the ACM_, _42_(1), 67-90.&#10;");
docs.push({"id":32,"title":"Introduction to Balanced Box-Decomposition Trees","url":"/introduction-to-balanced-box-decomposition-trees.html"});
index.add(33, "---&#10;date: 2018-10-30T19:21:00+01:00&#10;draft: false&#10;url: &quot;/introduction-to-isolation-forests.html&quot;&#10;--- # Introduction to Isolation Forests *Isolation Forests* (IFs), presented in Liu[^Liu] et. al (2012), are a popular algorithm used for outlier classification. In a very simplified way, the method consists of building an ensemble of&#10;*Isolation Trees* (ITs) for a given data set and observations are deemed anomalies if they have short adjusted average path lengths on the ITs. ITs, which will be covered shortly, have several properties in common with a fundamental data structure: the [Binary Search Tree](https://en.wikipedia.org/wiki/Binary_search_tree) (BSTs). In a very simplified way, BSTs are a special instance of tree structures where keys are kept in such an order that a node search is performed by iteratively (or recursively) choosing a left or right branch based on a quantitative comparison (*e.g.* lesser or greater). Node insertion is performed by doing a tree search, using the method described previously, until reaching an *external node*, where the new node will be inserted. This allows for efficient node searches since, on average, half the tree will not be visited. To illustrate this assume the values $x=[1, 10, 2, 4, 3, 5, 26, 9, 7, 54]$ and the respective insertion on a BST. The intermediate steps would then be as shown below. ![](./images/isolationforests/bst_steps.png) One of the properties of BSTs is that, with randomly generated data, the path between the root node and the outliers will typically be shorter. We can see from the illustration below that, with our example data, the path length for (say) 7 is twice the length than for the suspicious value of 54. This property will play an important role in the IF algorithm, as we will see further on. ![](./images/isolationforests/bst_path_length.png) ## Isolation Trees Since ITs are the fundamental component of IFs, we will start by describing their building process. We start by defining $t$ as the number of trees in the IF, $\\mathcal{D}$ as the training data (contained in an $n$-dimensional feature space, $\\mathcal{D} \\subset \\mathbb{R}^n$) and $\\psi$ as the subsampling size. The building of a IT consists then in recursively partitioning the data $\\mathcal{D}$ by sampling (without replacement) a subsample $\\mathcal{D}^{\\prime}$ of size $\\psi$. We then build an isolation tree $\\mathcal{T}^{\\prime}$ with this subsample (in order to later add it to the isolation forest $\\mathcal{F}$) and the process is repeated $t$ times. To build an isolation tree $\\mathcal{T}^{\\prime}$ from the subsample we proceed as follows: if the data subsample $\\mathcal{D}^{\\prime}$ is indivisible, a tree is returned containing a single *external node* corresponding to the feature dimensions, $n$. If it can be divided, a series of steps must be performed. Namely, if we consider $Q = \\lbrace q_1,\\dots,q_n\\rbrace$ as the list of features in $\\mathcal{D}^{\\prime}$, we select a random feature $q \\in Q$ and a random *split point* $p$ such that $$&#10;\\min(q) &lt; p &lt; \\max(q), \\qquad q \\in Q.&#10;$$ Based on the cut-off point $p$, we filter the features into a BST&rsquo;s left and right nodes according to $$&#10;\\mathcal{D}_l := \\lbrace \\mathcal{D}^{\\prime} : q \\in Q, q&lt;p\\rbrace \\\\&#10;\\mathcal{D}_r := \\lbrace \\mathcal{D}^{\\prime} : q \\in Q, q \\geq p\\rbrace,&#10;$$ and return an *internal node* having an isolation tree with left and right nodes as respectively $\\mathcal{D}_l$ and $\\mathcal{D}_r$. To illustrate this (and the general method of identifying anomalies in a two dimensional feature space, $x\\in\\mathbb{R}^2$) we will look at some simulated data and its processing. We start by simulating two clusters of data from a multivariate normal distribution, one centred in $x_a=[-10, 10]$ and another centred in $x_b=[10, 10]$, with a variance of $\\Sigma=\\text{diag}(2, 2)$, that is $$&#10;X_a \\sim \\mathcal{N}\\left([-10, -10], \\text{diag}(2, 2)\\right) \\\\&#10;X_b \\sim \\mathcal{N}\\left([10, 10], \\text{diag}(2, 2)\\right).&#10;$$ The particular realisation of this simulation looks like this: ![](./images/isolationforests/data.png) Below we illustrate the building of a *single* IT (given the data), illustrating the feature split point $p$ and respective division of the feature list into *left* or *right* IT nodes. The process is conducted recursively until the feature list is no longer divisible. As mentioned previously, this process, the creation of an IT, is repeated $t$ times in order to create the IF. &lt;p&gt; &lt;video width=&quot;100%&quot; autoplay&gt; &lt;source src=&quot;./images/isolationforests/split.mp4&quot; type=&quot;video/mp4&quot;&gt; &lt;/video&gt; &lt;/p&gt; In order to perform anomaly detection (*e.g.* observation scoring) we will then use the IT equivalent of the BST unsuccessful search heuristics. An external node termination in an IT is equivalent to a BST unsuccessful search. Given an observation $x$, our goal is then to calculate the score for this observation, given our defined subsampling size, that is, $s(x,\\psi)$. This technique amounts to partitioning the feature space randomly until feature points are &ldquo;isolated&rdquo;. Intuitively, points in high density regions will need more partitioning steps, whereas anomalies (by definition away from high density regions) will need fewer splits. Since the building of the ITs is performed in a randomised fashion and using a subsample of the data, this density predictor can be average over a number of ITs, the *Isolation Forest*. Intuitively, this could be done by calculating the average path length for our $\\mathcal{T}n, n=1,\\dots,t$ ITs, $\\overline{h}(x)$.&#10;However, as pointed in Liu[^Liu] *et. al* (2012), a problem with calculating this is that maximum possible height of each $\\mathcal{T}_n$ grows as $\\mathcal{O}(\\log(\\psi))$. To compare $h(x)$ given different subsampling sizes, a normalisation factor, $c(\\psi)$ must be established. This can be calculated by $$&#10;c(\\psi) = \\begin{cases}&#10;2H(\\psi-1)-2\\frac{\\psi-1}{n},\\text{if}\\ \\psi &gt;2,\\\\&#10;1, \\text{if}\\ \\psi=2,\\\\&#10;0, \\text{otherwise}, \\end{cases}&#10;$$ where $H(i)$ is the harmonic number estimated by $H(i)\\approx\\log(i) + e$. Denoting $h_{max}$ as the tree height limit and e as the *current path length*, initialised as $e=0$ we can then calculate $h(x)$ recursively as: $$&#10;h(x,\\mathcal{T},h_{max},e) = \\begin{cases}&#10;h(x,\\mathcal{T}_{n,left},h_{max},e+1) \\text{if}\\ x_a &lt; q_{\\mathcal{T}} \\\\&#10;h(x,\\mathcal{T}_{n,right},h_{max},e+1) \\text{if}\\ x_a \\geq q_{\\mathcal{T}} \\\\&#10;e+c(\\mathcal{T_{n,s}}) \\text{if}\\ \\mathcal{T} \\text{is a terminal node or}\\ e \\geq h_{max}.&#10;\\end{cases}&#10;$$ Given these quantities we can then, finally, calculate the anomaly score, $s$ as $$&#10;s(x,\\psi) = 2^{-\\frac{\\text{E}[h(x)]}{c(\\psi)}}&#10;$$ with $\\text{E}[h(x)]$ being the average $h(x)$ for a collection of ITs. ## Parameters As mentioned in Liu[^Liu] *et. al* (2012), the empirical subsampling size $\\psi=2^8$ is typically enough to perform anomaly detection in a wide range of data. Regarding the number of trees, $t$ no considerable accuracy gain is usually observed with $t&gt;100$. In the plots below, we can see the score calculation for two point in our data, namely an outlier ($x_o=[3.10, -12.69])$ and a normal observation ($x_n=[8.65, 9.71]$) with a varying number of trees and $\\psi=2^8$ (*left*) and a varying subsample size and $t=100$ (*right*). We can see that the score value stabilised quite early on when using $\\psi=2^8$ and that very low subsampling sizes can lead to problems when classifying anomalies. ![](./images/isolationforests/avg_score.png) Now that we know how to implement an IF algorithm and calculate an anomaly score, we will try to visualise the anomaly score distribution in the vicinity of the simulated data. To do so, we simply create a two dimensional lattice enclosing our data an iteratively calculate $s(\\cdot, \\psi)$. The result is show below: ![](./images/isolationforests/score_field.png) The above steps fully define a naive isolation forest algorithm, which when applied to the previously simulated data, result in 88% of the anomalies being correctly identified. ![](./images/isolationforests/detection.png) Thanks for reading! If you have any questions or comments, please let me know on [Mastodon](https://mastodon.social/@ruivieira) or [Twitter](https://twitter.com/ruimvieira). [^Liu]: Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2012). *Isolation-Based Anomaly Detection.* ACM Transactions on Knowledge Discovery from Data, 6(1), 1&ndash;39. https://doi.org/10.1145/2133360.2133363&#10;");
docs.push({"id":33,"title":"Introduction to Isolation Forests","url":"/introduction-to-isolation-forests.html"});
index.add(34, "# Java Completable Futures&#10;## Running in parallel&#10;```java&#10;import java.util.concurrent.CompletableFuture; CompletableFuture&lt;String&gt; future1 = CompletableFuture.supplyAsync(() -&gt; &quot;Hello&quot;);&#10;CompletableFuture&lt;String&gt; future2 = CompletableFuture.supplyAsync(() -&gt; &quot;Beautiful&quot;);&#10;CompletableFuture&lt;String&gt; future3 = CompletableFuture.supplyAsync(() -&gt; &quot;World&quot;); CompletableFuture&lt;Void&gt; combinedFuture = CompletableFuture.allOf(future1, future2, future3); CompletableFuture&lt;String&gt; result = combinedFuture.thenApply(v -&gt; future1.join() + future2.join() + future3.join()); System.out.println(result.get());&#10;``` ```&#10;HelloBeautifulWorld ``` ## Waiting for all Lets assume we have a completable future, $f$. This future, in turn, create $N$ additional completable futures, $f_1, f_2, \\dots, f_N$.&#10;How can we set $f$ to complete only when _all_ $f_1, f_2, \\dots, f_N$ are also completed? The answer is to use a combination of `allOf`[^allOf] with `thenRun`[^thenRun]. According to the documentation, `allOf` returns a new `CompletableFuture` that is completed when all of the given `CompletableFuture`s complete. In turn, `thenRun` will execute the given action.&#10;Let&apos;s look at an example: [^allOf]: [https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#allOf-java.util.concurrent.CompletableFuture...-](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#allOf-java.util.concurrent.CompletableFuture...-)&#10;[^thenRun]: [https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#thenRun-java.lang.Runnable-](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#thenRun-java.lang.Runnable-)&#10;```java&#10;CompletableFuture&lt;String&gt; f = new CompletableFuture&lt;&gt;(); int N = 10; CompletableFuture&lt;String&gt; f1 = CompletableFuture.completedFuture(&quot;f1&quot;);&#10;CompletableFuture&lt;String&gt; f2 = CompletableFuture.completedFuture(&quot;f2&quot;);&#10;CompletableFuture&lt;String&gt; f3 = CompletableFuture.completedFuture(&quot;f3&quot;); ExecutorService executor = Executors.newSingleThreadExecutor(); executor.submit(() -&gt; { CompletableFuture.allOf(f1, f2, f3) .thenRun(() -&gt; f.complete(&quot;f1,f2,f3 completed.\\nProceed to finish f.&quot;)); }); f.thenAccept(v -&gt; { System.out.println(v);&#10;}); Thread.sleep(100);&#10;executor.shutdown();&#10;``` ```&#10;f1,f2,f3 completed.&#10;Proceed to finish f. ``` ```java ```&#10;");
docs.push({"id":34,"title":"Java Completable Futures","url":"/java-completable-futures.html"});
index.add(35, "# Java consumer&#10;## Introduction ### Applying&#10;Introduced in Java 8, the `Consumer` interface aims at providing additional functional programming capabilities for Java. `Consumer` defined functions do not return any value and they consist mainly of two methods: ```java&#10;void accept(T t);&#10;default Consumer&lt;T&gt; andThen(Consumer&lt;? super T&gt; after);&#10;```&#10;Let&apos;s look at an example:&#10;```java&#10;import java.util.ArrayList; import java.util.LinkedList; import java.util.List; import java.util.function.Consumer; Consumer&lt;String&gt; say = a -&gt; System.out.println(&quot;Hello, &quot; + a + &quot;!&quot;); say.accept(&quot;World&quot;); ``` ```&#10;Hello, World! ``` A `Consumer` can be applied in a functional way, since applying a consumer is equivalent to applying the `accept` method.&#10;For instance:&#10;```java&#10;List&lt;String&gt; musketeers = new ArrayList&lt;String&gt;();&#10;musketeers.add(&quot;D&apos;Artagnan&quot;);&#10;musketeers.add(&quot;Athos&quot;);&#10;musketeers.add(&quot;Aramis&quot;);&#10;musketeers.add(&quot;Porthos&quot;);&#10;musketeers.stream().forEach(say)&#10;``` ```&#10;Hello, D&apos;Artagnan!&#10;Hello, Athos!&#10;Hello, Aramis!&#10;Hello, Porthos! ``` `Consumer` functions can also modify reference objects. For instance:&#10;```java&#10;List&lt;Double&gt; numbers = new ArrayList&lt;Double&gt;(); numbers.add(1d); numbers.add(2d); numbers.add(3d); Consumer&lt;List&lt;Double&gt;&gt; square = list -&gt; { for (int i = 0; i &lt; list.size(); i++) { double x = list.get(i); list.set(i, x*x); };&#10;}; System.out.println(numbers); square.accept(numbers); System.out.println(numbers);&#10;``` ```&#10;[1.0, 2.0, 3.0]&#10;[1.0, 4.0, 9.0] ``` ### Composing Let&apos;s now look at how to create a chain of `Consumer`s by composing them with the `andThen` method. Let&apos;s first create a consumer which converts a string to uppercase in-place:&#10;```java&#10;Consumer&lt;List&lt;String&gt;&gt; upperCaseConsumer = list -&gt; { for(int i=0; i&lt; list.size(); i++){ String value = list.get(i).toUpperCase(); list.set(i, value); } }; Consumer&lt;List&lt;String&gt;&gt; sayAll = list -&gt; list.stream().forEach(say);&#10;``` We will now create a chain by first applying `upperCaseConsumer` and the `say` to our list.&#10;```java&#10;upperCaseConsumer.andThen(sayAll).accept(musketeers);&#10;``` ```&#10;Hello, D&apos;ARTAGNAN!&#10;Hello, ATHOS!&#10;Hello, ARAMIS!&#10;Hello, PORTHOS! ``` ```java ```&#10;");
docs.push({"id":35,"title":"Java consumer","url":"/java-consumer.html"});
index.add(36, "# Java Notes on Java. * [Java Completable Futures](java-completable-futures.html)&#10;* [Java consumer](java-consumer.html) ## Reference ### Get user home directory ```java&#10;System.getProperty(&quot;user.home&quot;);&#10;``` ### List files recursively ```java&#10;try (Stream&lt;Path&gt; walk = Files.walk(Paths.get(input))) { List&lt;String&gt; result = walk.filter(Files::isRegularFile) .map(x -&gt;x.toString()) .collect(Collectors.toList()); result.forEach(System.out::println); } catch (IOException e) { e.printStackTrace(); }&#10;``` In case we want the file subset with a specific extension, `txt` we can filter the stream with ```java List&lt;String&gt; result = walk.filter(Files::isRegularFile) .filter(x -&gt; x.toString().endsWith(&quot;.txt&quot;)) .map(x -&gt; x.toString()) .collect(Collectors.toList()); ```");
docs.push({"id":36,"title":"Java","url":"/java.html"});
index.add(37, "# K-means clustering&#10;## Introduction _K_-means is still one of the fundamental clustering algorithms. It is used in such diverse fields as Natural Language Processing (NLP), social sciences and medical sciences. The core idea behind _K_-means is that we want to group data in _clusters_. Data points will be assigned to a specific cluster depending on it&apos;s distance to a cluster&apos;s center, usually called the _centroid_. It is important to note that typically, the mean distance to a centroid is used to partition the clusters, however, difference distances can be used and different pivot points. An example is the _K_-medoids clustering algorithm. We will define the two main steps of a generic _K_-means clustering algorithm, namely the data assignement and the centroid update step.&#10;### Data assignement&#10;The criteria to determine whether a point is closer to one centroid is typically an [Euclidean distance](distance-metrics.html#Euclidean distance L2) ($L^2$) .&#10;If we consider a set of $n$ centroids $C$, such that $$&#10;C = \\lbrace c_1, c_2, \\dots, c_n \\rbrace&#10;$$ We assign each data point in $\\mathcal{D}=\\lbrace x_1, x_2, \\dots, x_n \\rbrace$ to the nearest centroid according to its distance, such that $$&#10;\\underset{c_i \\in C}{\\arg\\min} \\; dist(c_i,x)^2&#10;$$&#10;As mentioned previously $dist(\\cdot)$ is typically the standard ($L^2$) [Euclidean distance](distance-metrics.html#Euclidean distance L2).&#10;We define the subset of points assigned to a centroid $i$ as $S_i$.&#10;### Centroid update step&#10;This step corresponds to updating the centroids using the mean of add points assign to a cluster, $S_i$. That is $$&#10;c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i} x_i&#10;$$&#10;# Partitioning&#10;Different algorithms can be used for cluster partitioning, for instance: - PAM&#10;- CLARA&#10;- CLARANS&#10;## PAM&#10;To illustrate the PAM partitioning method, we will use a synthetic dataset created along the guidelines in [synthetic data generation](synthetic-data-generation.html#Separability).&#10;# Elbow method&#10;In order to use the &quot;Elbow method&quot; we calculate the [Within-Cluster Sum of Squares (WCSS)](distance-metrics.html#Within-cluster sum of squares (WCSS)) for a varying number of clusters, $K$. ```python&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;import pandas as pd import sklearn import warnings&#10;warnings.filterwarnings(&quot;ignore&quot;)&#10;``` ```python&#10;dataset = pd.read_csv(&apos;data/mall-customers.zip&apos;)&#10;X = dataset.iloc[:, [3, 4]].values&#10;``` ```python&#10;from sklearn.cluster import KMeans&#10;wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters = i, init = &apos;k-means++&apos;, random_state = 42) kmeans.fit(X) wcss.append(kmeans.inertia_)&#10;``` ```python&#10;from plotutils import * plt.plot(range(1, 11), wcss)&#10;plt.xlabel(&apos;Number of clusters&apos;)&#10;plt.ylabel(&apos;WCSS&apos;) plt.show()&#10;``` ![k-means-clustering_1](./images/k-means-clustering_1.png)&#10;```python&#10;kmeans = KMeans(n_clusters = 5, init = &quot;k-means++&quot;, random_state = 42)&#10;y_kmeans = kmeans.fit_predict(X)&#10;``` ```python&#10;ps = 30&#10;plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = ps, c = colours[0], label = &apos;Cluster1&apos;)&#10;plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = ps, c = colours[1], label = &apos;Cluster2&apos;)&#10;plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = ps, c = colours[2], label = &apos;Cluster3&apos;)&#10;plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = ps, c = colours[3], label = &apos;Cluster4&apos;)&#10;plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = ps, c = colours[4], label = &apos;Cluster5&apos;) plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = &apos;black&apos;, label = &apos;Centroids&apos;)&#10;plt.xlabel(&apos;Annual Income (k$)&apos;) plt.ylabel(&apos;Spending Score (1-100)&apos;) plt.legend() plt.show()&#10;``` ![k-means-clustering_2](./images/k-means-clustering_2.png)&#10;```python ```&#10;");
docs.push({"id":37,"title":"K-means clustering","url":"/k-means-clustering.html"});
index.add(38, "---&#10;date: 2016-06-23T21:55:00+01:00&#10;draft: false&#10;url: &quot;/langtons-ant.html&quot;&#10;--- # Langton&apos;s Ant &lt;script src=&apos;/assets/ants.js&apos; type=&quot;text/javascript&quot;&gt;&lt;/script&gt; Last week, at the [North East Functional Programming](https://twitter.com/FP_North_East) meet up, we were given a code Kata consisting of the [Langton&apos;s ant](https://en.wikipedia.org/wiki/Langton%27s_ant) algorithm. I&apos;ve had a go at Scala but decided later on to put a live version in this blog. I considered several implementation options, such as scala.js and Elm, but in the end decided to implement it in plain Javascript. &lt;div style=&quot;margin-left: 10%;margin-right: 10%;&quot;&gt; &lt;canvas id=&quot;ants&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/canvas&gt;&#10;&lt;/div&gt; &lt;button type=&quot;button&quot; class=&quot;btn btn-secondary btn-sm&quot; id=&quot;addAntButton&quot;&gt;Add ant&lt;/button&gt;");
docs.push({"id":38,"title":"Langton's Ant","url":"/langtons-ant.html"});
index.add(39, "# Linux admin Notes on Linux admin.");
docs.push({"id":39,"title":"Linux admin","url":"/linux-admin.html"});
index.add(40, "---&#10;date: 2015-09-06T11:56:00+01:00&#10;draft: false&#10;url: &quot;/mcmc-notifications.html&quot;&#10;--- # MCMC notifications It is said that patience is a virtue but the truth is that no one likes waiting (especially *waiting around*: [this interesting article](http://www.nytimes.com/2012/08/19/opinion/sunday/why-waiting-in-line-is-torture.html?\\_r=0) explores why people prefer walking 8 minutes to the airport&rsquo;s baggage claim and having the bags ready rather than waiting the same amount of time entirely in the claim area). Anyone performing computationally heavy work, such as Monte Carlo methods, will know that these are usually computationally expensive algorithms which, even in modern hardware, can result in waiting times in the magnitude of hours, days and even weeks. These long running times coupled with the fact that in certain cases it is not easy to accurately predict how long a certain number of iterations will take, usually leads to a tiresome behaviour of constantly checking for good (or bad) news. Although it is perfectly possible to specify that your simulation should stop after a certain amount of time (especially valid for very long simulations), this doesn&rsquo;t seem to be the standard practice. In this post I&rsquo;ll detail my current setup for being notified *exactly* of when simulations are finished. To implement this setup, the following stack is required: * A [JDK](http://openjdk.java.net)&#10;* [Apache Maven](http://maven.apache.org)&#10;* A messaging service [Pushbullet](https://www.pushbullet.com)&#10;* A smartphone, tablet, smartwatch (or any other internet enabled device) To start, we can create an account in Pushbullet, which will involve, in the simplest case, signing up using some authentication service such as Google. Next, we will install the client application (available for [Android](https://play.google.com/store/apps/details?id=com.pushbullet.android&amp;amp;hl=en\\_GB), [iOS](https://itunes.apple.com/gb/app/pushbullet/id810352052) and [most modern browsers](https://www.pushbullet.com/apps) after which we can enable notifications (at least in the Android client, I&rsquo;m not familiar with the iPhone version). Since my current work started as a plain Java project which in time evolved mainly to &lt;a href\\=&quot;http://scala-lang.org&quot;\\&gt;Scala&lt;/a\\&gt;, it consists of an unholy mixture of Maven as a build tool for Scala code. This shouldn&apos;t be a problem for other setups, but I&rsquo;ll just go through my specific setup (*i.e.* using Maven dependencies to a Scala project). To implement communication between the code and the messaging service, we can use a simple library such as [jpushbullet](https://github.com/silk8192/jpushbullet). The library works well enough, although at the time of writing it only supports Pushbullet&rsquo;s v1 API but not the [newer v2 API](https://docs.pushbullet.com/#api). Since the project, unfortunately, is not in Maven central, you should build it from scratch. Fortunately, in a sensibly configured machine, this is trivial. In the machine where you plan to perform the simulations, clone and build `jpushbullet`. ```bash&#10;git clone git@github.com:silk8192/jpushbullet.git&#10;mvn clean install&#10;``` Once the build is complete, you can add it as a local dependency in your project&rsquo;s `pom.xml`: ```xml&#10;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.shakethat.jpushbullet&lt;/groupId&gt; &lt;artifactId&gt;jpushbullet&lt;/artifactId&gt; &lt;/dependency&gt;&#10;&lt;/dependencies&gt;&#10;``` For the purpose of this example, lets assume that you have the following `Object` as the entry point of your simulation. The next step is to add a call to the Pushbullet service before the exit point. Please keep in mind that it is *very bad* practice to include your personal API key in your committed code. I *strongly* suggest you keep this information in a separate file (*e.g.* in `resources`), read it at runtime and add it to `.gitignore`. That being said, place the messaging code as such: ```java package benchmarks import com.shakethat.jpushbullet.PushbulletClient object MCMC { def main(args:Array[String]):Unit = { // Your MCMC code val client = new PushbulletClient(api\\_key) val devices = client.getDevices val title = &quot;MCMC simulation finished&quot; val body = &quot;Some summary can be included&quot; // n is the preferred device number client.sendNote(true, devices .getDevices .get(n) .getIden(), title, body) } }&#10;``` Usually, I would call this code via `ssh` into my main machine from Maven (using [Scala Maven](http://scala-tools.org/mvnsites/maven-scala-plugin/) as: ```bash&#10;$ nohup mvn scala:run -DmainClass=benchmarks.MCMC &amp;&#10;``` Finally, when the simulation is completed, you will be notified in the client devices (you can select which ones by issuing separate `sendNote` calls) and include a result summary, as an example. ![](./images/pushbullet.png) My current setup generates an R script from a template which is run by `Rscript` in order to produce a PDF report. However, be careful, since file quotas in Pushbullet are limited, so text notifications should be used without worry of going over the free usage tier.&gt; Keep in mind, that there are other alternatives to `jpushbullet`, such as [send-notification](https://github.com/jcgay/send-notification), a general notification library for Java for which the setup is quite similar. Hope this was helpful.");
docs.push({"id":40,"title":"MCMC notifications","url":"/mcmc-notifications.html"});
index.add(41, "---&#10;date: 2018-08-02T23:05:00+01:00&#10;draft: false&#10;url: &quot;/mcmc-performance-on-substrate-vm.html&quot;&#10;--- # MCMC performance on Substrate VM Recently I&apos;ve been following (but not very closely, I admit) the development of the [GraalVM](https://www.graalvm.org/) project. The project has many interesting goals (such as [Project Metropolis](http://openjdk.java.net/projects/metropolis/), increased JIT performance and others). However, having dabbled with projects such as [Scala native](https://github.com/scala-native/scala-native) and [Kotlin native](https://kotlinlang.org/docs/reference/native-overview.html), one of the aspects of GraalVM that caught my attention was the [SubstrateVM](https://github.com/oracle/graal/tree/master/substratevm), which allegedly allows for a simple, straight-forward compilation of any Java bytecode into a native binary. I specifically wanted to compare the performance and memory consumption of simple scientific computing tasks when using the JVM and native executables.&#10;To do this, I picked two simple numerical simulations in the form of toy Gibbs samplers, in order to keep the cores busy for a while. ## Binomial-Beta case The first problem chosen was the one of sampling from a Beta-Binomial distribution where we have $$&#10;X \\sim \\text{Binom}\\left(n,\\theta\\right) \\\\\\\\&#10;\\theta \\sim \\text{B}\\left(a,b\\right).&#10;$$ Since we know that $$&#10;\\pi\\left(\\theta|x\\right) \\propto \\theta^x \\left(1-\\theta\\right)^{n-x}\\theta^{a-1}\\left(1-\\theta\\right)^{b-1},&#10;$$ We calculate the joint density $$&#10;p(x,\\theta) = \\begin{pmatrix} n \\\\\\\\ x \\end{pmatrix} \\theta^x \\left(1-\\theta\\right)^{n-x}\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}\\left(1-\\theta\\right)^{b-1}&#10;$$ The marginal distribution is a Binomial-Beta: $$&#10;p\\left(x\\right)=\\begin{pmatrix} n \\\\\\\\ x \\end{pmatrix}\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma(a)\\Gamma(b)}\\frac{\\Gamma\\left(a+b\\right)\\Gamma\\left(b+n-x\\right)}{\\Gamma\\left(a+b+n\\right)},\\qquad x=0,1,\\cdots,n.&#10;$$ The code for this simulation is available [here](https://github.com/ruivieira/benchmark-gibbs). The project is setup so that Maven produces an assembly Jar file, since I&apos;ve found that to be the easier artifact we can offer to the GraalVM&apos;s native compiler. To enable assembly Jars we add the `maven-assembly-plugin` to `pom.xml` and specify a main class. The assembly can then be produced simply by executing ```bash&#10;mvn package&#10;``` An assembly Jar should be available in the `target` folder and named `benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar`. Both the Jar and the native executable allow to specify how many iterations the Gibbs sampler should run for (as well as the thinning factor). If nothing is specified, the default will be used, which is $50000$ iterations thinned by $100$. This particular Gibbs sampler was implemented in two variants. One variant stores the samples draws of $x$ and $\\theta$ in arrays `double[]` while the other one simply calculates the Gibbs steps by using the previous value, that is $x_i=f(x_{i-1},\\theta_{i-1})$ and then discarding the previous values. The latter has a constant memory cost in $\\mathcal{O}(1)$ in terms of number of iterations, while the former clearly doesn&apos;t. We can them proceed with the first test, first benchmarking it under the JVM by running (for both sample history variants): ```shell&#10;$ /usr/bin/time -v java -jar target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar store 50000 100&#10;$ /usr/bin/time -v java -jar target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar nostore 50000 100&#10;``` (It is important to note that the `time` command is the executable under `/usr/bin` and not your shell&apos;s builtin.)&#10;The next step is to build the native image using GraalVM&apos;s compiler. This is also quite straight-forward and simply a matter of calling: ```shell&#10;$GRAALVM_BIN/native-image target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar&#10;``` where `$GRAALVM_BIN` is simply the location where you installed the GraalVM binaries. If the compilation is successful, you should see some information about the compilation steps, such as parsing, inlining, compiling and writing the image. Finally, if using the default, you should have a native executable available in your current directory. Again, the benchmark command is similar to the JVM step, that is: ```shell&#10;$ /usr/bin/time -v ./benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies store 50000 100&#10;$ /usr/bin/time -v ./benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies nostore 50000 100&#10;``` The results from the runs which saved the sampling history on both platforms (JVM and native) were consistent as we can see from the plots below: ![](./images/gibbs_jvm_native.png) The (peak) memory consumption and execution time for each version is presented in the table below: | | Time(s) | Peak (Mb) |&#10;|----------------------------|---------|-----------|&#10;| JVM (no sample history) | 110.09 | 320.913 |&#10;| native (no sample history) | 130.52 | 273.747 |&#10;| JVM (sample history) | 112.51 | 324.796 |&#10;| native (sample history) | 130.62 | 274.239 | ## Another bivariate case The second problem chosen is another bivariate model, [previously detailed](a-gibbs-sampler-in-crystal.html) in this blog.&#10;The code is included in the same repositoty as the Beta-Binomial case and the setup for the benchmarks is similar. The only step needed to run this example is to change the main class in the assemply plugin section of the `pom.xml` from `BinomialBet`a to `Bivariate`. The benchmark results are in the table below: | | Time(s)| Peak (Mb)|&#10;|------|--------|----------|&#10;|JVM |106.92 |176.541 |&#10;|native|121.29 |273.383 | Now, in this case, the results are much more interesting. The JVM version outperforms the native version in *both* execution time and memory consumption. I don&apos;t have an explanation for this, but if you think you have (or have any other questions) please let me know on [Mastodon](https://mastodon.social/@ruivieira) or [Twitter](https://twitter.com/ruimvieira). Thanks for reading!");
docs.push({"id":41,"title":"MCMC performance on Substrate VM","url":"/mcmc-performance-on-substrate-vm.html"});
index.add(42, "# Machine Learning Notes on machine learning. ## Topics * [Synthetic data generation](synthetic-data-generation.html) * Using [Gaussian copulas](synthetic-data-with-sdv-and-gaussian-copulas.html), [CTGAN](synthetic-data-with-sdv-and-ctgan.html) and [CopulaGAN](synthetic-data-with-sdv-and-copulagan.html)&#10;* [Explainability](explainability.html)&#10;* [K-means clustering](k-means-clustering.html) ### Metrics * [Error metrics](error-metrics.html)&#10;* [Distance metrics](distance-metrics.html) ### Optimisation&#10;* [Gradient-free optimisation](gradient-free-optimisation.html) ## Frameworks * [Cookiecutter Data Science](cookiecutter-data-science.html)&#10;* [Scikit-learn](scikit-learn.html)");
docs.push({"id":42,"title":"Machine Learning","url":"/machine-learning.html"});
index.add(43, "---&#10;date: 2019-04-03T23:05:00+01:00&#10;draft: false&#10;url: &quot;/monotonic-cubic-spline-interpolation-with-some-rust.html&quot;&#10;--- # Monotonic Cubic Spline interpolation (with some Rust) Monotonic Cubic Spline interpolation (MCSI) is a popular and useful method which fits a smooth, continuous function through discrete data. MCSI has several applications in the field of computer vision and trajectory fitting. MCSI further guarantees monotonicity of the smoothed approximation, something which a cubic spline approximation alone cannot.&#10;In this post I&apos;ll show how to implement the method developed by F. N. Fritsch and R. E. Carlson [^Fritsch2005] in the [Rust](https://www.rust-lang.org/) programming language. # Rust&#10;*Why* Rust? Definitely this is a type of solution so simple that it can be implemented in practically any programming language we can think of. However, I do find that the best way to get acquainted with a new language and its concepts is precisely to try to implement a simple and well-know solution. Although this post does not intend to be an introduction to the Rust language, some of the fundamentals will be presented as we go along. Idiomatic Rust Object-Oriented Programming (OOP) has several characteristics which differ significantly from &quot;traditional&quot; OOP languages.&#10;Rust achieves data and behaviour encapsulation by means of defining data structure blueprints (called `struct`) and then defining their behaviour though a concrete implementation (through `impl`). As an example, a simple &quot;class&quot; `Foo` would consist of: ```rust&#10;struct Foo {&#10;} impl Foo { fn new() -&gt; Foo { return Foo {}; } fn method(&amp;mut self) {} fn static_method() {}&#10;} pub fn main() { let mut f = Foo::new(); f.method(); Foo::static_method();&#10;}&#10;``` The &quot;constructor&quot; is defined typically as `new()`, but any &quot;static&quot; method which returns an initialised `struct` *can* be a constructor and &quot;object&quot; methods include the passing of the `self` instance not unlike languages such as Python. The `&amp;mut self` refers to the control or exclusive access to `self` and it is not directly related to `mut` mutability control. These concepts touch on Rust&apos;s borrowing and ownership model which, unfortunately, are *way* beyond the scope of this blog post. A nice introduction is provided by the &quot;*Rust programming book*&quot; available [here](https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html).&#10;Our implementation aims at building a MCSI class `MonotonicCubicSpline` by splitting the algorithm into the slope calculation at *construction* time, a *Hermite interpolation* function and a *partial application function generator*. This will follow the general structure ```rust&#10;pub struct MonotonicCubicSpline { m_x: Vec&lt;f64&gt;, m_y: Vec&lt;f64&gt;, m_m: Vec&lt;f64&gt;&#10;} impl MonotonicCubicSpline { pub fn new(x : &amp;Vec&lt;f64&gt;, y : &amp;Vec&lt;f64&gt;) -&gt; MonotonicCubicSpline { // ... } pub fn hermite(point: f64, x : (f64, f64), y: (f64, f64), m: (f64, f64)) -&gt; f64 { // ... } pub fn interpolate(&amp;mut self, point : f64) -&gt; f64 { // ... } fn partial(x: Vec&lt;f64&gt;, y: Vec&lt;f64&gt;) -&gt; impl Fn(f64) -&gt; f64 { // ... }&#10;}&#10;``` `Vec` is a vector, a typed growable collection available in Rust&apos;s standard library with documentation available [here](https://doc.rust-lang.org/std/vec/struct.Vec.html). ## Monotonic Cubic Splines MCSI hinges on the concept of cubic Hermite interpolators. The Hermite interpolation for the unit interval for a generic interval $(x_k,x_{k+1})$ is $$&#10;p(x)=p_k h_{00}(t)+ h_{10}(t)(x_{k+1}-x_k)m_k + \\\\ h_{01}(t)p_{k+1} + h_{11}(t)(x_{k+1}-x_{k})m_{k+1}.&#10;$$ The $h_{\\star}$ functions are usually called the *Hermite basis functions* in the literature and here we will use the factorised forms of: $$&#10;\\begin{aligned}&#10;h_{00}(t) &amp;= (1+2t)(1-t)^2 \\\\&#10;h_{10}(t) &amp;= t(1-t)^2 \\\\&#10;h_{01}(t) &amp;= t^2 (3-2t) \\\\&#10;h_{11}(t) &amp;= t^2 (t-1).&#10;\\end{aligned}&#10;$$ This can be rewritten as $$&#10;p(x) = (p_k(1 + 2t) + \\Delta x_k m_k t)(1-t)(1-t) + \\\\ (p_{k+1} (3 -2t) + \\Delta x_k m_{k+1} (t-1))t^2&#10;$$ where $$&#10;\\begin{aligned}&#10;\\Delta x_k &amp;= x_{k+1} - x_k \\\\&#10;t &amp;= \\frac{x-x_k}{h}.&#10;\\end{aligned}&#10;$$ This associated Rust method is the above mentioned &quot;static&quot; `MonotonicCubicSpline::hermite()`: ```rust&#10;pub fn hermite(point: f64, x : (f64, f64), y: (f64, f64), m: (f64, f64)) -&gt; f64 { let h = x.1 - x.0; let t = (point - x.0) / h; return (y.0 (1.0 + 2.0 t) + h m.0 t) (1.0 - t) (1.0 - t) + (y.1 (3.0 - 2.0 t) + h m.1 (t - 1.0)) t t;&#10;}&#10;``` where the tuples correspond to $x \\to (x_k, x_{k+1})$, $t \\to (y_k, y_{k+1})$ and $m \\to (m_k, m_{k+1})$ For a series of data points $(x_k, y_k)$ with $k=1,\\dots,n$ we then calculate the slopes of the secant lines between consecutive points, that is: $$&#10;\\Delta_k = \\frac{\\Delta y_{k}}{\\Delta x_k},\\qquad \\text{for}\\ k=1,\\dots,n-1&#10;$$ with $Delta y_k = y_{k+1}-y_k$ and $\\Delta x_k$ as defined previously. ![](./images/splines/secants.gif) Since the data is represented by the vectors `x : Vec&lt;f64&gt;` and `y : Vec&lt;f64&gt;` we implement this in the &quot;constructor&quot;: ```rust&#10;let mut secants = vec![0.0 ; n - 1];&#10;let mut slopes = vec![0.0 ; n];&#10;for i in 0..(n-1) { let dx = x[i + 1] - x[i]; let dy = y[i + 1] - y[i]; secants[i] = dy / dx;&#10;}&#10;``` The next step is to average the secants in order to get the tangents, such that $$&#10;m_k = \\frac{\\Delta_{k-1}+\\Delta_k}{2},\\qquad \\text{for}\\ k=2,\\dots,n-1.&#10;$$ This is achieved by the code: ```rust&#10;slopes[0] = secants[0];&#10;for i in 1..(n-1) { slopes[i] = (secants[i - 1] + secants[i]) * 0.5;&#10;}&#10;slopes[n - 1] = secants[n - 2];&#10;``` By definition, we want to ensure monotonicity of the interpolated points, but to guarantee this we must avoid the interpolation spline to go too far from a certain radius of the control points. If we define $\\alpha_k$ and $\\beta_k$ as $$&#10;\\begin{aligned}&#10;\\alpha_k &amp;= \\frac{m_k}{\\Delta_k} \\\\&#10;\\beta_k &amp;= \\frac{m_{k+1}}{\\Delta_k},&#10;\\end{aligned}&#10;$$ to ensure the monotonicity of the interpolation we can impose the following constraint on the above quantities: $$&#10;\\phi(\\alpha, \\beta) = \\alpha - \\frac{(2\\alpha+\\beta-3)^2}{3(\\alpha+\\beta-2)}\\geq 0,&#10;$$ that is $$&#10;\\alpha + 2\\beta - 3 \\leq 0, \\text{or}\\ 2\\alpha+\\beta-3 \\leq 0&#10;$$ Typically the vector $(\\alpha_k, \\beta_k)$ is restricted to a circle of radius 3, that is $$&#10;\\alpha^2_l + \\beta_k^2&gt;9,&#10;$$ and then setting $$&#10;m_{k+1} = t\\beta_k\\Delta_k,&#10;$$ where $$&#10;\\begin{aligned}&#10;h &amp;amp;= \\sqrt{\\alpha^2_k + \\beta^2_k} \\\\&#10;t &amp;amp;= \\frac{3}{h}.&#10;\\end{aligned}&#10;$$ One of the ways in which Rust implements polymorphism is through method dispatch. The `f64` primitive [provides](https://doc.rust-lang.org/std/primitive.f64.html#method.hypot) a shorthand for the quantity $\\sqrt{\\alpha^2_k + \\beta^2_k}$ as $\\alpha.\\text{hypot}(\\beta)$. The relevant Rust code will then be: ```rust&#10;for i in 0..(n-1) { if secants[i] == 0.0 { slopes[i] = 0.0; slopes[i + 1] = 0.0; } else { let alpha = slopes[i] / secants[i]; let beta = slopes[i + 1] / secants[i]; let h = alpha.hypot(beta); if h &gt; 3.0 { let t = 3.0 / h; slopes[i] = t * alpha * secants[i]; slopes[i + 1] = t * beta * secants[i]; } }&#10;}&#10;``` We are now able to define a &quot;smooth function&quot; generator using MCSI. We generate a smooth function $g(\\cdot)$ given a set of $(x_k, y_k)$ points, such that $$&#10;f(x_k, y_k, p) \\to g(p).&#10;$$ ## Partial application Before anything, it is important to recall the difference between partial application and *currying*, since the two are (incorrectly) used interchangeably quite often.&#10;*Function currying* allows to factor functions with multiple arguments into a chain of single-argument functions, that is $$&#10;f(x, y, z) = h(x)(y)(z)&#10;$$ The concept is prevalent in functional programming, since its initial formalisation [^Curry1958]. Partial application, however, generally aims at using an existing function conditioned on some argument as a basis to build functions with a reduced arity. In this case this would be useful since ultimately we want to create a smooth, continuous function based on the control points $(x_k, y_k)$. The partial application implementation is done in Rust as ```rust&#10;pub fn partial(x: Vec&lt;f64&gt;, y: Vec&lt;f64&gt;) -&gt; impl Fn(f64) -&gt; f64 { move |p| { let mut spline = MonotonicCubicSpline::new(&amp;x, &amp;y); spline.interpolate(p) }&#10;}&#10;``` An example of how to generate a concrete smoothed continuous function from a set of control points can be: ```rust&#10;let x = vec![0.0, 2.0, 3.0, 10.0];&#10;let y = vec![1.0, 4.0, 8.0, 10.5];&#10;let g = partial(x, y); // calculate an interpolated point&#10;let point = g(0.39);&#10;``` ![](./images/splines/interpolation.png) The full code can be found [here](https://gitlab.com/ruivieira/mentat/blob/master/src/lib.rs). [^Fritsch2005]: Fritsch, F. N., &amp;amp; Carlson, R. E. (2005). Monotone Piecewise Cubic Interpolation. *SIAM Journal on Numerical Analysis*. https://doi.org/10.1137/0717021&#10;[^Curry1958]: Curry, Haskell; Feys, Robert (1958). Combinatory logic. I (2 ed.). Amsterdam, Netherlands: North-Holland Publishing Company.");
docs.push({"id":43,"title":"Monotonic Cubic Spline interpolation (with some Rust)","url":"/monotonic-cubic-spline-interpolation-with-some-rust.html"});
index.add(44, "# OOB score in random forests In the Random Forest algorithm, we build a decision tree (DT) based on a certain training dataset. This tree will be split in order to minimise some criteria function. However, it is not desirable that individual DTs get too large with too many splits, so a common approach is to train each tree with a *subset of the training data* (sampled with replacement). This will ensure that individual tree maintain a manageable size, while the variance of the tree ensemble is reduced and the overall bias is not altered. This *training subset* is usually called the **bootstrap samples**. In the image below, we can see an illustration of the sampling with replacement. ![img](./images/diagrams/rf_sampling_replacement.png)&#10;```python ```&#10;");
docs.push({"id":44,"title":"OOB score in random forests","url":"/oob-score-in-random-forests.html"});
index.add(45, "```java&#10;%%loadFromPOM&#10;&lt;dependency&gt; &lt;groupId&gt;org.optaplanner&lt;/groupId&gt; &lt;artifactId&gt;optaplanner-core&lt;/artifactId&gt; &lt;version&gt;8.3.0.Final&lt;/version&gt;&#10;&lt;/dependency&gt;&#10;``` Define the Lesson:&#10;```java&#10;import java.time.DayOfWeek;&#10;import java.time.LocalTime; import org.optaplanner.core.api.domain.lookup.PlanningId; public class Person { @PlanningId private Long id; private double height; private double weight; // No-arg constructor required for Hibernate public Timeslot() { } public Timeslot(DayOfWeek dayOfWeek, LocalTime startTime, LocalTime endTime) { this.dayOfWeek = dayOfWeek; this.startTime = startTime; this.endTime = endTime; } public Timeslot(long id, DayOfWeek dayOfWeek, LocalTime startTime) { this(dayOfWeek, startTime, startTime.plusMinutes(50)); this.id = id; } @Override public String toString() { return dayOfWeek + &quot; &quot; + startTime; } // ************************************************************************ // Getters and setters // ************************************************************************ public Long getId() { return id; } public DayOfWeek getDayOfWeek() { return dayOfWeek; } public LocalTime getStartTime() { return startTime; } public LocalTime getEndTime() { return endTime; } }&#10;``` ```java&#10;import org.optaplanner.core.api.domain.lookup.PlanningId; public class Room { @PlanningId private Long id; private String name; // No-arg constructor required for Hibernate public Room() { } public Room(String name) { this.name = name.trim(); } public Room(long id, String name) { this(name); this.id = id; } @Override public String toString() { return name; } // ************************************************************************ // Getters and setters // ************************************************************************ public Long getId() { return id; } public String getName() { return name; } } ``` ```java&#10;import org.optaplanner.core.api.domain.entity.PlanningEntity;&#10;import org.optaplanner.core.api.domain.lookup.PlanningId;&#10;import org.optaplanner.core.api.domain.variable.PlanningVariable; @PlanningEntity&#10;public class Lesson { @PlanningId private Long id; private String subject; private String teacher; private String studentGroup; @PlanningVariable(valueRangeProviderRefs = &quot;timeslotRange&quot;) private Timeslot timeslot; @PlanningVariable(valueRangeProviderRefs = &quot;roomRange&quot;) private Room room; // No-arg constructor required for Hibernate and OptaPlanner public Lesson() { } public Lesson(String subject, String teacher, String studentGroup) { this.subject = subject.trim(); this.teacher = teacher.trim(); this.studentGroup = studentGroup.trim(); } public Lesson(long id, String subject, String teacher, String studentGroup, Timeslot timeslot, Room room) { this(subject, teacher, studentGroup); this.id = id; this.timeslot = timeslot; this.room = room; } @Override public String toString() { return subject + &quot;(&quot; + id + &quot;)&quot;; } // ************************************************************************ // Getters and setters // ************************************************************************ public Long getId() { return id; } public String getSubject() { return subject; } public String getTeacher() { return teacher; } public String getStudentGroup() { return studentGroup; } public Timeslot getTimeslot() { return timeslot; } public void setTimeslot(Timeslot timeslot) { this.timeslot = timeslot; } public Room getRoom() { return room; } public void setRoom(Room room) { this.room = room; } } ``` ```java ```&#10;");
docs.push({"id":45,"title":"OptaPlanner","url":"/optaplanner.html"});
index.add(46, "# Optimising random forest hyperparameters Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following: - [The number of decision trees](optimising-random-forest-hyperparamaters.html#Number of decision trees) in a random forest&#10;- The split criteria&#10;- Maximum depth of individual trees&#10;- Minimum samples per internal node&#10;- Maximum number of leaf nodes&#10;- Random features per split&#10;- Number of samples in bootstrap dataset We will look at each of these hyper-parameters individually with examples of how to select them. ## Data To understand how we can optimise the hyperparameters in a random forest model, we will use [scikit-learn&apos;s](scikit-learn.html) `RandomForestClassifier` and a subset of *Titanic*[^titanic] dataset. [^titanic]: Titanic Dataset - [https://www.kaggle.com/c/titanic-dataset/data](https://www.kaggle.com/c/titanic-dataset/data) First, we will import the features and labels using [Pandas](pandas.html).&#10;```python&#10;import pandas as pd train_features = pd.read_csv(&quot;data/svm-hyperparameters-train-features.csv&quot;)&#10;train_label = pd.read_csv(&quot;data/svm-hyperparameters-train-label.csv&quot;)&#10;``` Let&apos;s look at a random sample of entries from this dataset, both for features and labels.&#10;```python&#10;train_features.sample(10)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;517&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;30.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;24.1500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;538&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;30.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;14.5000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;735&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;28.5&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;16.1000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;25.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.6500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;395&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;22.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.7958&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;560&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;30.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.7500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;875&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;15.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.2250&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;579&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;32.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.9250&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;701&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;35.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;26.2875&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;707&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;42.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;26.2875&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;Some of the available features are: - `Pclass`, ticket class&#10;- `Sex`&#10;- `Age`, age in years&#10;- `Sibsp`, number of siblings/spouses aboard&#10;- `Parch`, number of parents/children aboard&#10;- `Fare`, passenger fare&#10;```python&#10;train_label.sample(10)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Survived&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;342&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;436&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;414&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;379&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;847&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;397&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;82&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;671&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;317&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;681&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;The outcome label indicates whether a passenger survived the disaster. As part of the typical initial steps for model training, we will prepare the data by splitting it into a training and testing subset.&#10;```python&#10;from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.33, random_state=23)&#10;``` ## Naive model&#10;First we will train a &quot;naive&quot; model, that is a model using the defaults provided by `RandomForestClassifier`[^rf]. These defaults are: - `n_estimators = 10`&#10;- `criterion=&rsquo;gini&rsquo;`&#10;- `max_depth=None`&#10;- `min_samples_split=2`&#10;- `min_samples_leaf=1`&#10;- `min_weight_fraction_leaf=0.0`&#10;- `max_features=&rsquo;auto&rsquo;`&#10;- `max_leaf_nodes=None`&#10;- `min_impurity_decrease=0.0`&#10;- `min_impurity_split=None`&#10;- `bootstrap=True`&#10;- `oob_score=False`&#10;- `n_jobs=1`&#10;- `random_state=None`&#10;- `verbose=0`&#10;- `warm_start=False`&#10;- `class_weight=None`&#10;[^rf]: [https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html) We will instantiate a random forest classifier:&#10;```python&#10;from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier()&#10;``` And training it using the `X_train` and `y_train` subsets using the appropriate `fit` method[^fit]. [^fit]: [https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit](https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit)&#10;```python&#10;true_labels = train_label.values.ravel() rf.fit(X_train, y_train.values.ravel())&#10;``` We can now evaluate trained naive model&apos;s score.&#10;```python&#10;from sklearn.metrics import precision_score&#10;``` ```python&#10;predicted_labels = rf.predict(X_test) precision_score(y_test, predicted_labels)&#10;``` ## Hyperparameter search&#10;A simple example of a generic hyperparameter search using the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) method in `scikit-learn`. The score used to measure the &quot;best&quot; model is the `mean_test_score`, but other metrics could be used, such as the [Out-of-bag (OOB)](oob-score-in-random-forests.html) error.&#10;```python&#10;parameters = { &quot;n_estimators&quot;:[5,10,50,100,250], &quot;max_depth&quot;:[2,4,8,16,32,None] }&#10;``` ```python&#10;rfc = RandomForestClassifier()&#10;``` ```python&#10;from sklearn.model_selection import GridSearchCV&#10;``` ```python&#10;cv = GridSearchCV(rfc,parameters,cv=5)&#10;cv.fit(X_train, y_train.values.ravel())&#10;``` ```python&#10;def display(results): print(f&apos;Best parameters are: {results.best_params_}&apos;) print(&quot;\\n&quot;) mean_score = results.cv_results_[&apos;mean_test_score&apos;] std_score = results.cv_results_[&apos;std_test_score&apos;] params = results.cv_results_[&apos;params&apos;] for mean,std,params in zip(mean_score,std_score,params): print(f&apos;{round(mean,3)} + or -{round(std,3)} for the {params}&apos;)&#10;``` ```python&#10;display(cv)&#10;``` ```&#10;Best parameters are: {&apos;max_depth&apos;: 8, &apos;n_estimators&apos;: 50} 0.795 + or -0.02 for the {&apos;max_depth&apos;: 2, &apos;n_estimators&apos;: 5}&#10;0.775 + or -0.024 for the {&apos;max_depth&apos;: 2, &apos;n_estimators&apos;: 10}&#10;0.774 + or -0.025 for the {&apos;max_depth&apos;: 2, &apos;n_estimators&apos;: 50}&#10;0.779 + or -0.02 for the {&apos;max_depth&apos;: 2, &apos;n_estimators&apos;: 100}&#10;0.78 + or -0.018 for the {&apos;max_depth&apos;: 2, &apos;n_estimators&apos;: 250}&#10;0.814 + or -0.024 for the {&apos;max_depth&apos;: 4, &apos;n_estimators&apos;: 5}&#10;0.81 + or -0.028 for the {&apos;max_depth&apos;: 4, &apos;n_estimators&apos;: 10}&#10;0.807 + or -0.022 for the {&apos;max_depth&apos;: 4, &apos;n_estimators&apos;: 50}&#10;0.804 + or -0.018 for the {&apos;max_depth&apos;: 4, &apos;n_estimators&apos;: 100}&#10;0.817 + or -0.025 for the {&apos;max_depth&apos;: 4, &apos;n_estimators&apos;: 250}&#10;0.815 + or -0.017 for the {&apos;max_depth&apos;: 8, &apos;n_estimators&apos;: 5}&#10;0.815 + or -0.017 for the {&apos;max_depth&apos;: 8, &apos;n_estimators&apos;: 10}&#10;0.822 + or -0.019 for the {&apos;max_depth&apos;: 8, &apos;n_estimators&apos;: 50}&#10;0.82 + or -0.014 for the {&apos;max_depth&apos;: 8, &apos;n_estimators&apos;: 100}&#10;0.82 + or -0.016 for the {&apos;max_depth&apos;: 8, &apos;n_estimators&apos;: 250}&#10;0.807 + or -0.028 for the {&apos;max_depth&apos;: 16, &apos;n_estimators&apos;: 5}&#10;0.812 + or -0.028 for the {&apos;max_depth&apos;: 16, &apos;n_estimators&apos;: 10}&#10;0.814 + or -0.028 for the {&apos;max_depth&apos;: 16, &apos;n_estimators&apos;: 50}&#10;0.807 + or -0.028 for the {&apos;max_depth&apos;: 16, &apos;n_estimators&apos;: 100}&#10;0.812 + or -0.026 for the {&apos;max_depth&apos;: 16, &apos;n_estimators&apos;: 250}&#10;0.779 + or -0.036 for the {&apos;max_depth&apos;: 32, &apos;n_estimators&apos;: 5}&#10;0.807 + or -0.027 for the {&apos;max_depth&apos;: 32, &apos;n_estimators&apos;: 10}&#10;0.814 + or -0.021 for the {&apos;max_depth&apos;: 32, &apos;n_estimators&apos;: 50}&#10;0.802 + or -0.023 for the {&apos;max_depth&apos;: 32, &apos;n_estimators&apos;: 100}&#10;0.797 + or -0.021 for the {&apos;max_depth&apos;: 32, &apos;n_estimators&apos;: 250}&#10;0.789 + or -0.027 for the {&apos;max_depth&apos;: None, &apos;n_estimators&apos;: 5}&#10;0.795 + or -0.033 for the {&apos;max_depth&apos;: None, &apos;n_estimators&apos;: 10}&#10;0.804 + or -0.023 for the {&apos;max_depth&apos;: None, &apos;n_estimators&apos;: 50}&#10;0.802 + or -0.014 for the {&apos;max_depth&apos;: None, &apos;n_estimators&apos;: 100}&#10;0.815 + or -0.021 for the {&apos;max_depth&apos;: None, &apos;n_estimators&apos;: 250} ``` ## Parameters ### Number of decision trees This is specified using the `n_estimators` hyper-parameter on the random forest initialisation. Typically, a higher number of trees will lead to greater accuracy at the expense of model size and training time.&#10;```python&#10;cv = GridSearchCV(rfc,{&quot;n_estimators&quot;:[2, 4, 8, 16, 32, 64, 128, 256, 512]},cv=5)&#10;cv.fit(X_train, y_train.values.ravel())&#10;``` ```python&#10;results = pd.DataFrame({&quot;n_estimators&quot;: [param[&quot;n_estimators&quot;] for param in cv.cv_results_[&apos;params&apos;]], &quot;mean_score&quot;: list(cv.cv_results_[&apos;mean_test_score&apos;]), &quot;std_score&quot;: cv.cv_results_[&apos;std_test_score&apos;]})&#10;results&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;n_estimators&lt;/th&gt; &lt;th&gt;mean_score&lt;/th&gt; &lt;th&gt;std_score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0.766793&lt;/td&gt; &lt;td&gt;0.016062&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;0.773515&lt;/td&gt; &lt;td&gt;0.029848&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;0.796989&lt;/td&gt; &lt;td&gt;0.034546&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;16&lt;/td&gt; &lt;td&gt;0.803669&lt;/td&gt; &lt;td&gt;0.011729&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;0.800308&lt;/td&gt; &lt;td&gt;0.030873&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;64&lt;/td&gt; &lt;td&gt;0.807073&lt;/td&gt; &lt;td&gt;0.021654&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;128&lt;/td&gt; &lt;td&gt;0.805378&lt;/td&gt; &lt;td&gt;0.018606&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;7&lt;/th&gt; &lt;td&gt;256&lt;/td&gt; &lt;td&gt;0.812101&lt;/td&gt; &lt;td&gt;0.022103&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;8&lt;/th&gt; &lt;td&gt;512&lt;/td&gt; &lt;td&gt;0.807059&lt;/td&gt; &lt;td&gt;0.020457&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=&apos;factor(n_estimators)&apos;, y=&apos;mean_score&apos;)) + geom_errorbar(aes(x=&apos;factor(n_estimators)&apos;, ymin=&apos;mean_score - std_score&apos;, ymax=&apos;mean_score + std_score&apos;)) + theme_classic() + xlab(&apos;Number of trees&apos;) + ylab(&apos;Mean score&apos;)&#10;)&#10;``` ![optimising-random-forest-hyperparamaters_1](./images/optimising-random-forest-hyperparamaters_1.png)&#10;### The split criteria&#10;At each node, a random forest decides, according to a specific algorithm, which feature and value split the tree.&#10;Therefore, the choice of splitting algorithm is crucial for the random forest&apos;s performance. Since, in this example, we are dealing with a classification problem, the choices of split algorithm are, for instance: - Gini&#10;- Entropy If we were dealing with a random forest for regression, other methods (such as [MSE](error-metrics.html)) would be a possible choice.&#10;We will now compare both split algorithms as specified above, in training a random forest with our data:&#10;```python&#10;rfc = RandomForestClassifier(n_estimators=256) cv = GridSearchCV(rfc,{&quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]},cv=5)&#10;cv.fit(X_train, y_train.values.ravel())&#10;``` ```python&#10;results = pd.DataFrame({&quot;criterion&quot;: [param[&quot;criterion&quot;] for param in cv.cv_results_[&apos;params&apos;]], &quot;mean_score&quot;: list(cv.cv_results_[&apos;mean_test_score&apos;]), &quot;std_score&quot;: cv.cv_results_[&apos;std_test_score&apos;]})&#10;results&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;criterion&lt;/th&gt; &lt;th&gt;mean_score&lt;/th&gt; &lt;th&gt;std_score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;gini&lt;/td&gt; &lt;td&gt;0.802003&lt;/td&gt; &lt;td&gt;0.023607&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;entropy&lt;/td&gt; &lt;td&gt;0.812087&lt;/td&gt; &lt;td&gt;0.013550&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;### Maximum depth of individual trees&#10;In theory, the &quot;longer&quot; the tree, the more splits it can have and better accommodate the data. However, at the tree level can this can lead to overfitting.&#10;Although this is a problem for decision trees, it is not necessarily a problem for the ensemble, the random forest.&#10;Although the key is to strike a balance between trees that aren&apos;t too large or too short, there&apos;s no universal heuristic to determine the size.&#10;Let&apos;s try a few option for maximum depth:&#10;```python&#10;rfc = RandomForestClassifier(n_estimators=256, criterion=&quot;entropy&quot;) cv = GridSearchCV(rfc,{&apos;max_depth&apos;: [2, 4, 8, 16, 32, None]},cv=5)&#10;cv.fit(X_train, y_train.values.ravel())&#10;``` ```python&#10;results = pd.DataFrame({&quot;max_depth&quot;: [param[&quot;max_depth&quot;] for param in cv.cv_results_[&apos;params&apos;]], &quot;mean_score&quot;: list(cv.cv_results_[&apos;mean_test_score&apos;]), &quot;std_score&quot;: cv.cv_results_[&apos;std_test_score&apos;]})&#10;results = results.dropna()&#10;results&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;max_depth&lt;/th&gt; &lt;th&gt;mean_score&lt;/th&gt; &lt;th&gt;std_score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;0.770154&lt;/td&gt; &lt;td&gt;0.018592&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;0.817185&lt;/td&gt; &lt;td&gt;0.022074&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;8.0&lt;/td&gt; &lt;td&gt;0.812087&lt;/td&gt; &lt;td&gt;0.008403&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;16.0&lt;/td&gt; &lt;td&gt;0.805378&lt;/td&gt; &lt;td&gt;0.019350&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;32.0&lt;/td&gt; &lt;td&gt;0.813768&lt;/td&gt; &lt;td&gt;0.018610&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=&apos;factor(max_depth)&apos;, y=&apos;mean_score&apos;)) + theme_classic() + xlab(&apos;Max tree depth&apos;) + ylab(&apos;Mean score&apos;)&#10;)&#10;``` ![optimising-random-forest-hyperparamaters_2](./images/optimising-random-forest-hyperparamaters_2.png)&#10;### Maximum number of leaf nodes&#10;This hyperparameter can be of importance to other topics, such as [Explainability](explainability.html). It is specified in `scikit-learn` using the `max_leaf_nodes` parameter. Let&apos;s try a few different values:&#10;```python&#10;rfc = RandomForestClassifier(n_estimators=256, criterion=&quot;entropy&quot;, max_depth=8) cv = GridSearchCV(rfc,{&apos;max_leaf_nodes&apos;: [2**i for i in range(1, 8)]},cv=5)&#10;cv.fit(X_train, y_train.values.ravel())&#10;``` ```python&#10;results = pd.DataFrame({&quot;max_leaf_nodes&quot;: [param[&quot;max_leaf_nodes&quot;] for param in cv.cv_results_[&apos;params&apos;]], &quot;mean_score&quot;: list(cv.cv_results_[&apos;mean_test_score&apos;]), &quot;std_score&quot;: cv.cv_results_[&apos;std_test_score&apos;]})&#10;results = results.dropna()&#10;results&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;max_leaf_nodes&lt;/th&gt; &lt;th&gt;mean_score&lt;/th&gt; &lt;th&gt;std_score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0.755014&lt;/td&gt; &lt;td&gt;0.024759&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;0.773487&lt;/td&gt; &lt;td&gt;0.024367&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;0.817185&lt;/td&gt; &lt;td&gt;0.026706&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;16&lt;/td&gt; &lt;td&gt;0.812115&lt;/td&gt; &lt;td&gt;0.014137&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;0.813782&lt;/td&gt; &lt;td&gt;0.019204&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;64&lt;/td&gt; &lt;td&gt;0.812073&lt;/td&gt; &lt;td&gt;0.008712&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;128&lt;/td&gt; &lt;td&gt;0.813754&lt;/td&gt; &lt;td&gt;0.013498&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=&apos;factor(max_leaf_nodes)&apos;, y=&apos;mean_score&apos;)) + geom_errorbar(aes(x=&apos;factor(max_leaf_nodes)&apos;, ymin=&apos;mean_score - std_score&apos;, ymax=&apos;mean_score + std_score&apos;)) + theme_classic() + xlab(&apos;Maximum leaf nodes&apos;) + ylab(&apos;Mean score&apos;)&#10;)&#10;``` ![optimising-random-forest-hyperparamaters_3](./images/optimising-random-forest-hyperparamaters_3.png)&#10;### Random features per split&#10;This is an important hyperparameter that will depend on how noisy the original data is.&#10;Typically, if the data is not very noisy, the number of used random features can be kept low. Otherwise, it needs to be kept high. An important consideration is also the following trade-off: - A low number of random features decrease the forest&apos;s overall variance&#10;- A low number of random features increases the bias&#10;- A high number of random features increases computational time In `scikit-learn` this is specified with the `max_features` parameter. Assuming $N_f$ is the total number of features,&#10;some possible values for this parameter are: - `sqrt`, this will take the `max_features` as the rounded $\\sqrt{N_f}$&#10;- `log2`, as above, takes the $\\log_2(N_f)$&#10;- The actual maximum number of features can be directly specified Let&apos;s try a simple benchmark, even though our data does not have many features to begin with:&#10;```python&#10;rfc = RandomForestClassifier(n_estimators=256, criterion=&quot;entropy&quot;, max_depth=8) cv = GridSearchCV(rfc,{&apos;max_features&apos;: [&quot;sqrt&quot;, &quot;log2&quot;, 1, 2, 3, 4, 5, 6]},cv=5)&#10;cv.fit(X_train, y_train.values.ravel())&#10;``` ```python&#10;results = pd.DataFrame({&quot;max_features&quot;: [param[&quot;max_features&quot;] for param in cv.cv_results_[&apos;params&apos;]], &quot;mean_score&quot;: list(cv.cv_results_[&apos;mean_test_score&apos;]), &quot;std_score&quot;: cv.cv_results_[&apos;std_test_score&apos;]})&#10;results&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;max_features&lt;/th&gt; &lt;th&gt;mean_score&lt;/th&gt; &lt;th&gt;std_score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;sqrt&lt;/td&gt; &lt;td&gt;0.820476&lt;/td&gt; &lt;td&gt;0.018793&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;log2&lt;/td&gt; &lt;td&gt;0.810420&lt;/td&gt; &lt;td&gt;0.015345&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0.820504&lt;/td&gt; &lt;td&gt;0.017704&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0.820490&lt;/td&gt; &lt;td&gt;0.017064&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;0.815420&lt;/td&gt; &lt;td&gt;0.009488&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;0.823838&lt;/td&gt; &lt;td&gt;0.011681&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;0.828880&lt;/td&gt; &lt;td&gt;0.016225&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;7&lt;/th&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;0.828880&lt;/td&gt; &lt;td&gt;0.013361&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=&apos;factor(max_features)&apos;, y=&apos;mean_score&apos;)) + geom_errorbar(aes(x=&apos;factor(max_features)&apos;, ymin=&apos;mean_score - std_score&apos;, ymax=&apos;mean_score + std_score&apos;)) + theme_classic() + xlab(&apos;Maximum number of features&apos;) + ylab(&apos;Mean score&apos;)&#10;)&#10;``` ![optimising-random-forest-hyperparamaters_4](./images/optimising-random-forest-hyperparamaters_4.png)&#10;### Bootstrap dataset size&#10;This hyperparameter relates to the proportion of the training data to be used by decision trees. It is specified in `scikit-learn` by `max_samples` and can take the value of either: - `None`, take the entirety of the samples&#10;- An integer, representing the actual number of samples&#10;- A float, representing a proportion between `0` and `1` or the samples to take. Let&apos;s try a hyperparameter search with some values:&#10;```python&#10;rfc = RandomForestClassifier(n_estimators=256, criterion=&quot;entropy&quot;, max_depth=8, max_features=6) cv = GridSearchCV(rfc,{&apos;max_samples&apos;: [i/10.0 for i in range(1, 10)]},cv=5)&#10;cv.fit(X_train, y_train.values.ravel())&#10;``` ```python&#10;results = pd.DataFrame({&quot;max_samples&quot;: [param[&quot;max_samples&quot;] for param in cv.cv_results_[&apos;params&apos;]], &quot;mean_score&quot;: list(cv.cv_results_[&apos;mean_test_score&apos;]), &quot;std_score&quot;: cv.cv_results_[&apos;std_test_score&apos;]})&#10;results&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;max_samples&lt;/th&gt; &lt;th&gt;mean_score&lt;/th&gt; &lt;th&gt;std_score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.1&lt;/td&gt; &lt;td&gt;0.805434&lt;/td&gt; &lt;td&gt;0.021459&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.2&lt;/td&gt; &lt;td&gt;0.817171&lt;/td&gt; &lt;td&gt;0.019534&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;0.3&lt;/td&gt; &lt;td&gt;0.815448&lt;/td&gt; &lt;td&gt;0.010392&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.4&lt;/td&gt; &lt;td&gt;0.818796&lt;/td&gt; &lt;td&gt;0.016416&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;0.832227&lt;/td&gt; &lt;td&gt;0.019042&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;0.6&lt;/td&gt; &lt;td&gt;0.827199&lt;/td&gt; &lt;td&gt;0.015364&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;6&lt;/th&gt; &lt;td&gt;0.7&lt;/td&gt; &lt;td&gt;0.825518&lt;/td&gt; &lt;td&gt;0.016115&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;7&lt;/th&gt; &lt;td&gt;0.8&lt;/td&gt; &lt;td&gt;0.820490&lt;/td&gt; &lt;td&gt;0.011031&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;8&lt;/th&gt; &lt;td&gt;0.9&lt;/td&gt; &lt;td&gt;0.827199&lt;/td&gt; &lt;td&gt;0.014415&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=&apos;factor(max_samples)&apos;, y=&apos;mean_score&apos;)) + geom_errorbar(aes(x=&apos;factor(max_samples)&apos;, ymin=&apos;mean_score - std_score&apos;, ymax=&apos;mean_score + std_score&apos;)) + theme_classic() + xlab(&apos;Proportion bootstrap samples&apos;) + ylab(&apos;Mean score&apos;)&#10;)&#10;``` ![optimising-random-forest-hyperparamaters_5](./images/optimising-random-forest-hyperparamaters_5.png)&#10;```python ```&#10;");
docs.push({"id":46,"title":"Optimising random forest hyperparamaters","url":"/optimising-random-forest-hyperparamaters.html"});
index.add(47, "# Pandas basics&#10;## Column operations&#10;### Renaming columns&#10;```python&#10;import pandas as pd&#10;import numpy as np&#10;import warnings warnings.filterwarnings(&apos;ignore&apos;) df = pd.DataFrame({ &apos;a&apos;:np.random.randn(6), &apos;b&apos;:np.random.choice( [5,7,np.nan], 6), &apos;c&apos;:np.random.choice( [&apos;foo&apos;,&apos;bar&apos;,&apos;baz&apos;], 6), })&#10;``` ```python&#10;df.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;a&lt;/th&gt; &lt;th&gt;b&lt;/th&gt; &lt;th&gt;c&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.549838&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.658684&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;-0.784545&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.204787&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;1.206179&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;df.rename(columns={&quot;a&quot;: &quot;new_name&quot;}, inplace=True)&#10;df.columns&#10;``` Using a mapping function. In this case `str.upper()`:&#10;```python&#10;df.rename(columns=str.upper, inplace=True)&#10;df.columns&#10;``` We can also use a lambda. For instance, using `lambda x: x.capitalize()` would result:&#10;```python&#10;df.rename(columns=lambda x: x.capitalize(), inplace=True)&#10;df.columns&#10;``` A list of column names can be passed directly to columns.&#10;```python&#10;df.columns = [&quot;first&quot;, &quot;second&quot;, &quot;third&quot;]&#10;df.columns&#10;``` ### Dropping columns&#10;A column can be dropped using the `.drop()` method along with the `column` keyword. For instance in the dataframe `df`:&#10;```python&#10;df&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;first&lt;/th&gt; &lt;th&gt;second&lt;/th&gt; &lt;th&gt;third&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.549838&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.658684&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;-0.784545&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.204787&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;1.206179&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;-0.898500&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;We can drop the `second` column using:&#10;```python&#10;df.drop(columns=&apos;second&apos;)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;first&lt;/th&gt; &lt;th&gt;third&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.549838&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.658684&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;-0.784545&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.204787&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;1.206179&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;-0.898500&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;The `del` keyword is also a possibility. However, `del` changes the dataframe **in-place**, therefore we will make a copy of the dataframe first.&#10;```python&#10;df_copy = df.copy()&#10;df_copy&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;first&lt;/th&gt; &lt;th&gt;second&lt;/th&gt; &lt;th&gt;third&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.549838&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.658684&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;-0.784545&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.204787&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;1.206179&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;-0.898500&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;del df_copy[&apos;second&apos;]&#10;df_copy&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;first&lt;/th&gt; &lt;th&gt;third&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.549838&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.658684&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;-0.784545&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.204787&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;1.206179&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;-0.898500&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;Yet another possibility is to drop the column by index. For instance:&#10;```python&#10;df.drop(columns=df.columns[1])&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;first&lt;/th&gt; &lt;th&gt;third&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.549838&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;0.658684&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;-0.784545&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.204787&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;1.206179&lt;/td&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;-0.898500&lt;/td&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;Or we could use ranges, for instance:&#10;```python&#10;df.drop(columns=df.columns[0:2])&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;third&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;foo&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;5&lt;/th&gt; &lt;td&gt;baz&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python ```&#10;");
docs.push({"id":47,"title":"Pandas basics","url":"/pandas-basics.html"});
index.add(48, "# Pandas * [Basics](pandas-basics.html)");
docs.push({"id":48,"title":"Pandas","url":"/pandas.html"});
index.add(49, "# Portuguese Christmas recipes This year we spent Christmas in &quot;lockdown&quot; and we tried to make ourselves our full traditional Portuguese Christmas recipes from scratch -- while not being in Portugal. Herein lies the first issue: there are many different &quot;traditions&quot;, but these are the ones that me and my partner are used to. Traditionally, Christmas celebrations in Portugal start on the night of Christmas eve and carry on during Christmas day. The main meals are then dinner on the 24&lt;sup&gt;th&lt;/sup&gt; December and lunch on the 25&lt;sup&gt;th&lt;/sup&gt; December. ## Christmas eve dinner&#10;As mentioned, we will follow two separate traditions. From my family&apos;s side, originally from the [north of Portugal (Porto)](https://en.wikipedia.org/wiki/Porto), we typically have boiled salted cod with vegetables (cabbage, onion and carrots), seasoned with olive oil, vinegar and garlic.&#10;From my partner&apos;s side, the meal typically consists of octopus rice, accompanied with salted cod fishcakes (&quot;*bolinhos de bacalhau*&quot;) and pan-fried octopus (&quot;*filetes de polvo*&quot;). ### Salted cod Salted cod is a staple from Portuguese cuisine, with possible origins in the cod salt-curing methods of Basque fishermen that ventured into Newfoundland in the 1500s[^basque]. Going through the centuries, even as recently as 1884, Portuguese writer E&ccedil;a de Queiroz wrote in a letter to Oliveira Martins about his love of a &quot;bacalhau de cebolada&quot;[^queiroz]. [^basque]: Silva, Ant&oacute;nio Jos&eacute; Marques da (2015), [&quot;The fable of the cod and the promised sea: About Portuguese traditions of bacalhau&quot;](https://www.academia.edu/15680102/The_fable_of_the_cod_and_the_promised_sea._About_portuguese_traditions_of_bacalhau), in Barata, Filipe Themudo; Rocha, Jo&atilde;o Magalh&atilde;es (eds.), _Heritages and Memories from the Sea_, &Eacute;vora, Portugal: 1st International Conference of the UNESCO Chair in Intangible Heritage and Traditional Know-How: Linking Heritage [^queiroz]: &quot;*A comida como h&aacute;bito e identidade: o bacalhau e os portugueses*&quot;, (ISCTE-IUL, Departamento de Antropologia, Escola de Ci&ecirc;ncias Humanas e Sociais), em 28 de fevereiro de 2013 ![](/images/IMG_5821.jpg)&#10;The salted cod needs to be soaked in water for _at least_ four days to remove the excess salt (picture above, on the left). ![](/images/IMG_5817.jpg)&#10;It&apos;s a really simple dish: just put everything on a pot and let it boil for approximately one hour. Since the salted cod has a really firm fleshy texture, it won&apos;t fall apart like fresh fish when boiled for a long time. Usually there&apos;s no need to add salt, since the cod will probably still have quite a lot of salt in it, but it never hurts to double check. We will cook around two to three times the amount of cod and vegetables that we need for the actual meal. The reason for this is that the starter for the next day ([Portuguese Christmas recipes](portuguese-christmas-recipes.html#Roupa-velha)) is made from the left-overs of the Christmas eve&apos;s dinner. So essentially, we have to make sure we have plenty of left-overs! ![](/images/IMG_5830.jpg)&#10;And here it is: ready to tuck in. As you can see, I like my salted cod with a _very generous_ amount of olive oil and vinegar.&#10;### Octopus rice Octopus is another northern Portuguese tradition, especially in the [Minho](https://en.wikipedia.org/wiki/Alto_Minho) and [Tr&aacute;s-os-Montes](https://en.wikipedia.org/wiki/Alto_Tr%C3%A1s-os-Montes), possibly due to the proximity with Galiza ([Galicia](https://en.wikipedia.org/wiki/Galicia_(Spain))) where octopus fishing has been historically a very important activity. ![](/images/IMG_5820.jpg)&#10;Next it&apos;s the octopus rice. Boil the octopus with just some salt for seasoning. Knowing when the octopus is ready is really an art. Make sure its not undercooked, but don&apos;t overcook it either since it will be quite chewy. Brown chopped onions in olive oil and add the water from boiling the octopus along with rice, the octopus and chopped parsley. The rice should have a fair amount of water and not end up dry. ![](/images/IMG_5826.jpg)&#10;Part of the octopus goes into the rice and the rest is pan-fried (&quot;*filetes de polvo*&quot;). They are battered, with eggs and flour, and deep-fried. ![](/images/IMG_5825.jpg)&#10;We then proceed to the cod fishcakes (&quot;*bolinhos de bacalhau*&quot;). These are done by shredding some salted code and mixing it with mashed potato, salt and parsley and then deep-fried. ## Christmas day lunch ### Roupa-velha ![](/images/IMG_5837.jpg)&#10;The reason why we cook way more quantities than we need for the salted cod, is to make something called &quot;*roupa-velha*&quot; (literal translation &quot;*old clothes*&quot;) as a starter on the 25&lt;sup&gt;th&lt;/sup&gt;. This a left-over dish and we use all the left-overs from the Christmas Eve dinner. ![](/images/IMG_5838.jpg)&#10;Start by shredding the cooked salted cod and removing all the fish bones and skin. ![](/images/IMG_5841.jpg)&#10;We then add a good amount of garlic (two or three cloves _at least_), and prepare a pan with some olive oil. We put first the garlic and let it brown. ![](/images/IMG_5842.jpg)&#10;When the garlic is brown we add all the left-overs (potato, sliced egg, carrot, cabbage and shredded code). We stir it for at least 15 minutes and add vinegar. _Lots of vinegar_. ![](/images/IMG_5843.jpg) And here it is. Must be eaten while pipping hot. ### Lamb roast&#10;Usually on the 25th of December we eat a roast (turkey, lamb, goat or pork).&#10;We went for a lamb roast. It was seasoned with lemon, rosemary, garlic, paprika, olive oil and salt for four days. ![](/images/IMG_0881.jpg) It is accompanied by roast potatoes and carrots and (optionally) some white rice. ![](/images/IMG_0885.jpg)&#10;And here it is! ## Desserts ### Aletria and arroz doce *Aletria* is a typical Christmas dessert which is quite similar to rice pudding in taste, but instead of rice, it is done with *vermicelli* pasta. ![](/images/IMG_5800.jpg) The preparation is quite similar to rice pudding, but adding some lemon peels to the milk mix. ![](/images/IMG_5844.jpg)&#10;A cinnamon decoration is a must, here shown with a festive &quot;*Feliz Natal*&quot; (Merry Christmas). ![](/images/IMG_5831.jpg)&#10;&quot;*Arroz doce*&quot; (literal translation *Sweet Rice*) is very similar to rice pudding, also with the addition of some lemon. ### Filh&oacute;s Filh&oacute;s are a type of slightly sweet doughy pancake, usually sprinkled with sugar and cinnamon, traditional during Christmas. These are specific type of filh&oacute; called &quot;*Filh&oacute; tendida no joelho*&quot;, traditional from the [Beiras](https://en.wikipedia.org/wiki/Beiras_e_Serra_da_Estrela) Portuguese region, where the dough is stretched on top of the knee. ![](/images/IMG_5801.jpg)&#10;The dough has to be proven at a certain temperature. Here is the contraption we&apos;ve used: a heating fan, heater and an [Hibernate](https://hibernate.org/) (!) book. ![](/images/IMG_5807.jpg)&#10;The dough must be proved (in our case at least 10 hours) so it can be stretched and fried lightly in olive oil on both sides. After draining any excess oil, they are sprinkled with a sugar and cinnamon mix. ### Rabanadas&#10;&quot;Rabanadas&quot; are in essence very similar to &quot;French toast&quot;.&#10;The way to prepare them is to leave dried bread soaking in milk and drained before deep-frying. After draining, until they are mostly dry and without much excess oil, they are sprinkled (generously) with a mixture of sugar and cinnamon. ![](/images/IMG_5834.jpg) ");
docs.push({"id":49,"title":"Portuguese Christmas recipes","url":"/portuguese-christmas-recipes.html"});
index.add(50, "# Python Pandas ## Subsetting and indexing ### Indexing performance Let&apos;s assume the case where you a column `BOOL` with values `Y` or `N` that you want to replace with an integer `1` or `0` value. The inital[^pythonic] instinct would be to do something like: [^pythonic]: and Pythonic? ```python&#10;df[&quot;BOOL&quot;] = df[&quot;BOOL&quot;].eq(&quot;Y&quot;).mul(1)&#10;``` This will result in the warning ```text&#10;SettingWithCopyWarning:&#10;A value is trying to be set on a copy of a slice from a DataFrame.&#10;Try using .loc[row_indexer,col_indexer] = value instead&#10;``` [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy) recommends the usage of the following idiom, since it can be considerably faster: ```python&#10;df.loc[:, (&quot;BOOL&quot;)] = df.loc[:, (&quot;BOOL&quot;)].eq(&quot;Y&quot;).mul(1)&#10;```");
docs.push({"id":50,"title":"Python Pandas","url":"/python-pandas.html"});
index.add(51, "# Python Pweave ## Installing Installing `pweave` is a matter of simply running[^1]: ```shell&#10;pip3 install pweave&#10;``` [^1]: I recommend using a separate `pyenv` for this. ## Editor support At the moment of writing, the editor which, IMO, has the best support for `pweave` is Atom (using Hydrogen). To install the necessary packages run: ```shell&#10;apm install language-weave Hydrogen&#10;apm install language-markdown atom-html-preview pdf-view&#10;``` ## Chunk options ");
docs.push({"id":51,"title":"Python Pweave","url":"/python-pweave.html"});
index.add(52, "# Python environments ## Interpreters To install different [Python](python.html) interpreters I strongly recommend `asdf`[^asdf]. [^asdf]: https://asdf-vm.com/ Let&apos;s look at to install Python in two different OSes, macOS and Fedora. ### macOS To install `asdf` on a macOS, first install the general dependencies with ```shell&#10;$ brew install coreutils curl git&#10;``` then install `asdf` itself with ```shell&#10;$ brew install asdf&#10;``` Add to the shell, in our case [[zsh]] with: ```shell&#10;$ echo -e &quot;\\n. $(brew --prefix asdf)/asdf.sh&quot; &gt;&gt; ~/.zshrc&#10;``` Add a plugin, in our case [Python](python.html), with ```shell&#10;$ asdf plugin add Python&#10;``` You can list all available versions with ```shell&#10;$ asdf list all Python&#10;``` Install a specific version, say, ```shell&#10;$ asdf install Python 3.9.0&#10;``` ### Fedora To install `asdf` on a Fedora, first install the general dependencies ```shell&#10;$ sudo dnf install curl git&#10;``` Clone the repository ```shell&#10;$ git clone https://github.com/asdf-vm/asdf.git \\ ~/.asdf --branch v0.8.0&#10;``` Add to `zsh` with ```shell&#10;$ . $HOME/.asdf/asdf.sh`&#10;``` ## pyenv ### Compiling on macOS `pyenv` can be notoriously problematic on macOS. For instance, running `pyenv doctor` on my laptop[^1] will result in: [^1]: I&apos;m running Big Sur at the moment of writing ```text&#10;Cloning ~/.pyenv/plugins/pyenv-doctor/bin/.....&#10;Installing python-pyenv-doctor...&#10;python-build: use readline from homebrew&#10;python-build: use zlib from xcode sdk BUILD FAILED (OS X 10.15.7 using python-build 20180424) Inspect or clean up the working tree at /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/python-build.20210128094523.17091&#10;Results logged to /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/python-build.20210128094523.17091.log Last 10 log lines:&#10;checking readline/readline.h, presence... no&#10;checking for readline/readline.h,... no&#10;checking readline/rlconf.h usability... yes&#10;checking readline/rlconf.h presence... yes&#10;checking for readline/rlconf.h... yes&#10;checking for SSL_library_init in -lssl... no&#10;configure: WARNING: OpenSSL &lt;1.1 not installed. Checking v1.1 or beyond...&#10;checking for OPENSSL_init_ssl in -lssl... no&#10;configure: error: OpenSSL is not installed.&#10;make: *** No targets specified and no makefile found. Stop.&#10;Problem(s) detected while checking system. See https://github.com/pyenv/pyenv/wiki/Common-build-problems for known solutions.&#10;``` The problem in this case is that `pyenv` can&apos;t find the relevant C headers for compilation of new versions. This can be fixed by using: ```bash&#10;$ CFLAGS=&quot;-I$(brew --prefix openssl)/include \\ -I$(brew --prefix readline)/include \\ -I$(xcrun --show-sdk-path)/usr/include&quot; \\ LDFLAGS=&quot;-L$(brew --prefix openssl)/lib \\ -L$(brew --prefix readline)/lib \\ -L$(xcrun --show-sdk-path)/usr/lib&quot; \\ pyenv doctor&#10;``` and the output will be: ```text&#10;Cloning ~/.pyenv/plugins/pyenv-doctor/bin/.....&#10;Installing python-pyenv-doctor...&#10;python-build: use readline from homebrew&#10;python-build: use zlib from xcode sdk&#10;Installed python-pyenv-doctor to /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/pyenv-doctor.20210128095003.18889/prefix Congratulations! You are ready to build pythons!&#10;``` ## Poetry ### Poetry as Jupyter kernel To register a `poetry` environment (named `foo`) as a Jupyter kernel, run: ```shell&#10;poetry run python -m ipykernel install --user --name foo&#10;``` ## venv Create a new `venv` with the command: ```bash&#10;$ virtualenv venv&#10;``` and activate it using (under Bash or zsh) with: ```bash&#10;$ source venv/bin/activate&#10;``` ## Anaconda&#10;First download [Anaconda](https://www.anaconda.com/products/individual) (or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)). Once installed you can proceed to create environments[^anaconda]. ### Creating environments An environment `foo` can be created using ```shell&#10;conda create --name foo&#10;``` One it is created, it can be activated using ```shell&#10;conda activate foo&#10;``` [^anaconda]: The remainder will assume that you have installed `Anaconda`, rather than `Miniconda`.");
docs.push({"id":52,"title":"Python environments","url":"/python-environments.html"});
index.add(53, "# Python grammar of graphics&#10;```python&#10;import pandas as pd&#10;import warnings&#10;warnings.filterwarnings(&apos;ignore&apos;)&#10;warnings.simplefilter(&apos;ignore&apos;) mpg = pd.read_csv(&quot;data/mpg.csv&quot;)&#10;``` ```python&#10;mpg.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;mpg&lt;/th&gt; &lt;th&gt;cylinders&lt;/th&gt; &lt;th&gt;displacement&lt;/th&gt; &lt;th&gt;horsepower&lt;/th&gt; &lt;th&gt;weight&lt;/th&gt; &lt;th&gt;acceleration&lt;/th&gt; &lt;th&gt;model_year&lt;/th&gt; &lt;th&gt;origin&lt;/th&gt; &lt;th&gt;name&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;18.0&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;307.0&lt;/td&gt; &lt;td&gt;130&lt;/td&gt; &lt;td&gt;3504&lt;/td&gt; &lt;td&gt;12.0&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;chevrolet chevelle malibu&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;15.0&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;350.0&lt;/td&gt; &lt;td&gt;165&lt;/td&gt; &lt;td&gt;3693&lt;/td&gt; &lt;td&gt;11.5&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;buick skylark 320&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;18.0&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;318.0&lt;/td&gt; &lt;td&gt;150&lt;/td&gt; &lt;td&gt;3436&lt;/td&gt; &lt;td&gt;11.0&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;plymouth satellite&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;16.0&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;304.0&lt;/td&gt; &lt;td&gt;150&lt;/td&gt; &lt;td&gt;3433&lt;/td&gt; &lt;td&gt;12.0&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;amc rebel sst&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;17.0&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;302.0&lt;/td&gt; &lt;td&gt;140&lt;/td&gt; &lt;td&gt;3449&lt;/td&gt; &lt;td&gt;10.5&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;ford torino&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from plotnine import *&#10;from plotnine.data import * ggplot(data=mpg) +\\&#10;geom_point(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;)) + theme_classic()&#10;``` ![python-grammar-of-graphics_1](./images/python-grammar-of-graphics_1.png)&#10;```python&#10;ggplot(data=mpg) +\\&#10;geom_point(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;, color=&quot;class&quot;)) + theme_classic()&#10;``` ![python-grammar-of-graphics_2](./images/python-grammar-of-graphics_2.png)&#10;```python&#10;ggplot(data=mpg) +\\&#10;geom_point(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;, size=&quot;class&quot;)) + theme_classic()&#10;``` ![python-grammar-of-graphics_3](./images/python-grammar-of-graphics_3.png)&#10;```python&#10;# Left&#10;ggplot(data=mpg) +\\&#10;geom_point(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;, alpha=&quot;manufacturer&quot;)) + theme_classic()&#10;``` ![python-grammar-of-graphics_4](./images/python-grammar-of-graphics_4.png)&#10;```python&#10;# Right&#10;ggplot(data=mpg) +\\&#10;geom_point(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;, shape=&quot;manufacturer&quot;)) + theme_classic()&#10;``` ![python-grammar-of-graphics_5](./images/python-grammar-of-graphics_5.png)&#10;```python&#10;ggplot(data=mpg) +\\&#10;geom_point(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;), color=&quot;blue&quot;) + theme_classic()&#10;``` ![python-grammar-of-graphics_6](./images/python-grammar-of-graphics_6.png)&#10;```python&#10;ggplot(data=mpg) +\\&#10;geom_smooth(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;)) + theme_classic()&#10;``` ![python-grammar-of-graphics_7](./images/python-grammar-of-graphics_7.png)&#10;```python&#10;ggplot(data=mpg) +\\&#10;geom_smooth(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;, linetype=&quot;drv&quot;)) + theme_classic()&#10;``` ![python-grammar-of-graphics_8](./images/python-grammar-of-graphics_8.png)&#10;```python&#10;ggplot(data=mpg) +\\&#10;geom_point(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;)) +\\&#10;geom_smooth(mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;)) + theme_classic()&#10;``` ![python-grammar-of-graphics_9](./images/python-grammar-of-graphics_9.png)&#10;```python&#10;ggplot(data=mpg, mapping=aes(x=&quot;displ&quot;, y=&quot;hwy&quot;)) +\\&#10;geom_point(mapping=aes(color=&quot;class&quot;)) +\\&#10;geom_smooth() + theme_classic()&#10;``` ![python-grammar-of-graphics_10](./images/python-grammar-of-graphics_10.png)");
docs.push({"id":53,"title":"Python grammar of graphics","url":"/python-grammar-of-graphics.html"});
index.add(54, "--- date: 2019-04-02T23:05:00+01:00&#10;draft: false&#10;url: &quot;/python-monkey-patching-for-readability.html&quot;&#10;--- # Python monkey patching (for readability) When preparing a [Jupyter](https://jupyter.org/) notebook for a workshop on recommendation engines which I&apos;ve presented with a colleague, I was faced with the following problem: &gt; &quot;How to break a large class definition into several cells so it can be presented step-by-step.&quot; Having the ability to declare a rather complex (and large) [Python](python.html) class in separate cells has several advantages, the obvious one being the ability to fully document each method&apos;s functionality with Markdown, rather than comments.&#10;Python does allow for functionality to be added to classes after their declaration via the assignment of methods through attributes. This is commonly known as &quot;monkey patching&quot; and hinges on the concepts of *bound* and *unbound* methods. I will show a quick and general overview of the methods that Python puts at our disposal for dynamic runtime object manipulation, but for a more in-depth please consult the official [Python documentation](https://docs.python.org/3/). ## Bound and unbound methods Let&apos;s first look at bound methods. If we assume a class called `Class` and an instance `instance`, with an instance method `bound` and class method `unbound` such that ```python&#10;class Class: def bound(self): pass @staticmethod def unbound(): pass instance = Class()&#10;``` Then `foo` is a bound method and `bar` is an unbound method.&#10;This definition, in practice, can be exemplified by the standard way of calling `.foo()`, which is ```python&#10;instance.bound()&#10;``` which in turn is equivalent to ```python&#10;Class.bound(instance)&#10;``` The standard way of calling `unbound` is , similarly ```python&#10;instance.unbound()&#10;``` This, however, is equivalent to ```python&#10;Class.unbound()&#10;``` In the unbound case, we can see there&apos;s no need to pass the class instance. `unbound` is *not bound* to the class instance. As mentioned before, Python allow us to change the class attributes at runtime. If we consider a method such as ```python&#10;def newBound(self): pass&#10;``` we can then add it to the class, even after declaring it. For instance: ```python&#10;Class.newBound = newBound&#10;instance = Class()&#10;instance.newBound() # Class.newBound(instance)&#10;``` It is interesting to note that any type of function definition will work, since functions are first class objects in Python. As such, if the method can be written as a single statement, a `lambda` could also be used, *i.e.* ```python&#10;Class.newBound = lambda self: print(&quot;I&apos;m a lambda&quot;)&#10;``` A limitation of the &quot;monkey patching&quot; method, is that attributes can only be changed at the class definition level.&#10;As an example, although possible, it is not trivial to add the `.newBound()` method to `instance`. A solution is to either call the descriptor methods (which allow for instance attribute manipulation), or declare the instance attribute as a `MethodType`. To illustrate this in our case: ```python&#10;import types&#10;instance.newBound = types.MethodType(newBound, instance)&#10;instance.newBound() # Prints &quot;I&apos;m a lambda&quot;&#10;``` This method is precisely, as mentioned, to change attributes for a specific instance, so in this case, if we try to access the bound method from another instance `anotherInstance`, it would fail ```python&#10;anotherInstance = Class()&#10;anotherInstance.newBound() # fails with AttributeError&#10;``` ## Abstract classes Python supports abstract classes, *i.e.* the definition of &quot;blueprint&quot; classes for which we delegate the concrete implementation of abstract methods to subclasses. In Python 3.x this is done via the `@abstractmethod` annotation. If we declare a class such as ```python&#10;from abc import ABC, abstractmethod&#10;class AbstractClass(ABC): @abstractmethod def abstractMethod(self): pass&#10;``` we can then implement `abstractMethod` in all of `AbstractClass`&apos;s subclasses: ```python&#10;class ConcreteClass(AbstractClass): def abstractMethod(self): print(&quot;Concrete class abstract method&quot;)&#10;``` We could, obviously, do this in Python *without* abstract classes, but this mechanism allows for a greater safety, since implementation of abstract methods is mandatory in this case. With regular classes, not implementing `abstractMethod` would simply assume we were using the parent&apos;s definition. Unfortunately, monkey patching of abstract methods is not supported in Python. We *could* monkey patch the concrete class: ```python&#10;ConcreteClass.newBound = lambda self: print(&quot;New &apos;child&apos; bound&quot;)&#10;c = ConcreteClass()&#10;c.newBound() # prints &quot;New &apos;child&apos; bound&quot;&#10;``` And we could even add a new bound method to the superclass, which will be available to all subclasses: ```python&#10;AbstractClass.newBound = lambda self: print(&quot;New &apos;parent&apos; bound&quot;)&#10;c = ConcreteClass()&#10;c.newBound() # prints &quot;New &apos;parent&apos; bound&quot;&#10;``` However, we can&apos;t add abstract methods with monkey patching. This is [a documented exception](https://docs.python.org/3/library/abc.html#abc.abstractmethod) of this functionality with the specific warning that &gt; Dynamically adding abstract methods to a class, or attempting to modify the abstraction status of a method or class once it is created, are not supported. The abstractmethod() only affects subclasses derived using regular inheritance; &quot;virtual subclasses&quot; registered with the ABC&apos;s register() method are not affected.&lt;/p&gt; ## Private methods We can dynamically add and replace inner methods, such as: ```python&#10;class Class: def _inner(self): print(&quot;Inner bound&quot;) def __private(self): print(&quot;Private bound&quot;) def callNewPrivate(self): self.__newPrivate() Class._newInner = lambda self: print(&quot;New inner bound&quot;)&#10;c = Class()&#10;c._inner() # prints &quot;Inner bound&quot;&#10;c._newInner() # prints &quot;New inner bound&quot;&#10;``` However, private methods behave differently. Python enforces name mangling for private methods. As specified in the documentation: &gt; Since there is a valid use-case for class-private members (namely to avoid name clashes of names with names defined by subclasses), there is limited support for such a mechanism, called name mangling. Any identifier of the form `__spam` (at least two leading underscores, at most one trailing underscore) is textually replaced with `_classname__spam`, where classname is the current class name with leading underscore(s) stripped. This mangling is done without regard to the syntactic position of the identifier, as long as it occurs within the definition of a class. We can then still access the private methods (although we probably shouldn&apos;t), but monkey patching won&apos;t work as before due to the above. ```python&#10;c._Class__private() # Private bound&#10;Class.__newPrivate = lambda self: print(&quot;New private bound&quot;)&#10;c = Class()&#10;c._Class__newPrivate() # fails with AttributeError&#10;``` We have defined a new method called `__newPrivate()` but interestingly, this method is *not* private. We can see this by calling it directly (which is allowed) and by calling the new &quot;private&quot; method from inside the class as `self.__newPrivate()`: ```python&#10;c.__newPrivate() # prints &quot;New private bound&quot;&#10;c.callNewPrivate() # fails with AttributeError (can&apos;t find _Class_NewPrivate)&#10;``` It is possible to perform some OOP abuse and declare the private method by mangling the name ourselves. In this case we could then do: ```python&#10;Class._Class__newPrivate = lambda self: print(&quot;New private bound&quot;)&#10;c = Class()&#10;c._Class__newPrivate() # prints &quot;New private bound&quot;&#10;c.callNewPrivate() # prints &quot;New private bound&quot;&#10;``` ## Builtins Is it possible to monkey patch builtin classes in Python, *e.g.* `int` or `float`?&#10;In short, yes, it is. Although the usefulness is arguable and I *strongly* urge not to do this in any production scenario, we&apos;ll look at how to achieve this, for the sake of completeness. A very interesting and educational read is available from the [Forbidden Fruit](https://github.com/clarete/forbiddenfruit) Python module. Primitive (or *builtin*) classes in Python are typically written in C and as such some of these meta-programming facilities require jumping through extra hoops (as well as being a Very Bad Idea&trade;).&#10;Let&apos;s first look at the integer class representation, `int`. A `int` doesn&apos;t allow bound methods to be added dynamically as previously. For instance: ```python&#10;p = 5&#10;type(p) # int&#10;``` We can try to add a method to `int` to square the value of the instance: ```python&#10;int.square = lambda self: self ** 2&#10;``` This fails with the error `TypeError: can&apos;t set attributes of built-in/extension type &apos;int&apos;`.&#10;The solution (as presented in Forbidden Fruit) is to first create classes to hold the `ctype` information of a builtin (C) class. We subclass `ctypes` Python representation of a C `struct` in native byte order and hold the `signed int` size and pointer to `PyObject`. ```python&#10;import ctypes&#10;class PyObject(ctypes.Structure): pass PyObject.fields = [ (&apos;ob_refcnt&apos;, ctypes.c_int), (&apos;ob_type&apos;, ctypes.POINTER(PyObject)),&#10;]&#10;``` Next we create a holder for Python objects slots, containing a reference to the `ctype` structure: ```python&#10;class SlotsProxy(PyObject): _fields_ = [(&apos;dict&apos;, ctypes.POINTER(PyObject))]&#10;``` The final step is extract the `PyProxyDict` from the object referenced by the pointer.&#10;Ideally, we should get the builtin&apos;s namespace so we can freely set attributes as we did previously. A helper function to retrieve the builtins (mutable) namespace can then be: ```python&#10;def patch(klass): name = klass.__name__ target = klass.__dict__ proxy_dict = SlotsProxy.from_address(id(target)) namespace = {} ctypes.pythonapi.PyDict_SetItem( ctypes.py_object(namespace), ctypes.py_object(name), proxy_dict.dict, ) return namespace[name]&#10;``` We can now easily patch builtin classes. Let&apos;s try to add the square method again by first retrieving the namespace (stored below in `d`) and setting it directly ```python&#10;d = patch(int)&#10;d[&quot;square&quot;] = lambda self: self ** 2 p.square() # 25&#10;``` All future instance of `int` will also contain the square method now: ```python&#10;(2 + p).square() # 49&#10;``` # Conclusion&#10;&quot;Monkey patching&quot; is usually, and rightly so, considered a code smell, due to the increased indirection and potential source of unwanted surprises.&#10;However, having the ability to &quot;monkey patch&quot; classes in Python allows us to write Jupyter notebooks in a more literate, fluid way rather than presenting the user with a &quot;wall of code&quot;.&#10;Thank you for reading. If you have any comments or suggestions please drop me a message on [Mastodon](https://mastodon.technology/@ruivieira).");
docs.push({"id":54,"title":"Python monkey patching (for readability)","url":"/python-monkey-patching-for-readability.html"});
index.add(55, "# Python Main page for all things Python.&#10;Other pages cover specific topics, such as: * [Python environments](python-environments.html)&#10;* [[Python collections]]&#10;* [How to setup Pweave](python-pweave.html)&#10;* [Pandas](pandas.html)&#10;* Notes on [Python&apos;s grammar of graphics](python-grammar-of-graphics.html) ## Language changes * In 2021, the Python steering council [accepted the proposal](https://lwn.net/ml/python-dev/61D540B9-2FE5-4CC8-8038-5654B1D325C7@python.org/) to add a pattern-matching primitive to the language.&#10;The proposal consists of [PEP634](https://www.python.org/dev/peps/pep-0634/) along with [PEP635](https://www.python.org/dev/peps/pep-0635/) and [PEP636](https://www.python.org/dev/peps/pep-0636/). ## Modules ### Relative import in Python 3 If a relative import is present inside a Python 3 file (*e.g.* `file1`) inside a module (*e.g.* `mymod`), say&#10;```python&#10;from .foo import bar&#10;``` We will encounter the error ```&#10;ImportError: attempted relative import with no known parent package&#10;``` A possible solution is to include the following in your module&apos;s `__init__.py`:&#10;```python&#10;import os, sys sys.path.append(os.path.dirname(os.path.realpath(__file__)))&#10;``` ## Ternary operator Ternary operators help reduce the amount of very small `if-else` blocks.&#10;Python does not have a ternary operator [like](https://en.wikipedia.org/wiki/%3F:#C) [other](https://en.wikipedia.org/wiki/%3F:#JavaScript) [languages](https://en.wikipedia.org/wiki/%3F:#Ruby). However, conditionals can be used to the same effect: ```python&#10;y = 7&#10;x = 0 if (y == 1) else 1&#10;print(x)&#10;``` ```&#10;1 ``` ## for ... else `for-else` blocks allow to capture if a condition was met inside a `for-loop`. For instance, consider the following `for-loop`: ```python&#10;locations = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;f&apos;]&#10;treasure = False&#10;for location in locations: if location == &apos;x&apos;: treasure = True break&#10;if not treasure: print(&quot;X marks the spot, but not found&quot;)&#10;``` ```&#10;X marks the spot, but not found ``` We can simplify the above logic using a `for-else` loop: ```python&#10;for location in locations: if location == &apos;x&apos;: break&#10;else: print(&quot;X marks the spot, but not found&quot;)&#10;``` ```&#10;X marks the spot, but not found ``` ## Boolean unravelling ### and unravelling the `and` boolean operator. The operation can be rewritten as the&#10;function `u_and`: ```python&#10;def u_and(a, b): result = a if a: result = b return result&#10;``` For instance: ```python&#10;a = True ; b = None&#10;print(a and b, u_and(a, b))&#10;a = True ; b = True&#10;print(a and b, u_and(a, b))&#10;a = False ; b = True&#10;print(a and b, u_and(a, b))&#10;``` ```&#10;None None&#10;True True&#10;False False ``` None None True True False False&#10;### or On the other hand, `or` cand be unravelled as: ```python&#10;def u_or(a, b): result = a if not a: result = b return result&#10;``` As an example: ```python&#10;a = True ; b = None&#10;print(a or b, u_or(a, b))&#10;a = True ; b = True&#10;print(a or b, u_or(a, b))&#10;a = False ; b = True&#10;print(a or b, u_or(a, b))&#10;``` ```&#10;True True&#10;True True&#10;True True ``` True True True True True True&#10;## The many faces of `print` ### Concatenating arguments ```python&#10;var1 = &quot;Foo&quot;&#10;var2 = &quot;Bar&quot;&#10;print(&quot;I am &quot;, var1, &quot; not &quot;, var2)&#10;``` ```&#10;I am Foo not Bar ``` I am Foo not Bar ```python&#10;var1 = &quot;Foo&quot;&#10;var2 = &quot;Bar&quot;&#10;print(&quot;I am &quot;, var1, &quot; not &quot;, var2)&#10;``` ```&#10;I am Foo not Bar ``` I am Foo not Bar&#10;It is also possible to use separators by using the `sep` argument: ```python&#10;var1 = &quot;Foo&quot;&#10;var2 = &quot;Bar&quot;&#10;print(&quot;I am&quot;, var1, &quot;not&quot;, var2, sep=&quot;!&quot;)&#10;``` ```&#10;I am!Foo!not!Bar ``` I am!Foo!not!Bar&#10;### String termination The `end` argument allows to specify the suffix of the whole string. ```python&#10;print(&quot;This is on radio&quot;, end=&quot; (over)&quot;)&#10;``` ```&#10;This is on radio (over)&#10;``` This is on radio (over) ## Filesystem operations ### Get home directory For Python +3.5: ```python&#10;from pathlib import Path home = str(Path.home())&#10;``` ### List files recursively For Python +3.5, use `glob`: ```python&#10;import glob # root_dir with trailing slash (i.e. /root/dir/)&#10;root_dir = &quot;./tmp&quot;&#10;for filename in glob.iglob(root_dir + &apos;**/*.md&apos;, recursive=True): print(filename)&#10;```&#10;");
docs.push({"id":55,"title":"Python","url":"/python.html"});
index.add(56, "# Rust exercises ## Variables ### Problem 1&#10;```rust&#10;fn main() { x = 5; println!(&quot;x has the value {}&quot;, x);&#10;}&#10;``` ### Solution 1&#10;```rust&#10;fn main() { let x = 5; println!(&quot;x has the value {}&quot;, x);&#10;}&#10;main()&#10;``` ```&#10;x has the value 5 ``` ### Problem 2&#10;```rust&#10;fn main() { let x; if x == 10 { println!(&quot;Ten!&quot;); } else { println!(&quot;Not ten!&quot;); }&#10;}&#10;``` ### Solution 2&#10;```rust&#10;fn main() { let x = 10; if x == 10 { println!(&quot;Ten!&quot;); } else { println!(&quot;Not ten!&quot;); }&#10;}&#10;main()&#10;``` ```&#10;Ten! ``` ### Problem 3&#10;```rust&#10;fn main() { let x = 3; println!(&quot;Number {}&quot;, x); x = 5; // don&apos;t change this line println!(&quot;Number {}&quot;, x);&#10;}&#10;``` ### Solution 3&#10;```rust&#10;fn main() { let mut x = 3; println!(&quot;Number {}&quot;, x); x = 5; // don&apos;t change this line println!(&quot;Number {}&quot;, x);&#10;}&#10;main()&#10;``` ```&#10;Number 3&#10;Number 5 ``` ### Problem 4&#10;```rust&#10;fn main() { let x: i32; println!(&quot;Number {}&quot;, x);&#10;}&#10;``` ### Solution 4&#10;```rust&#10;fn main() { let x: i32 = 1; println!(&quot;Number {}&quot;, x);&#10;}&#10;main()&#10;``` ```&#10;Number 1 ``` ### Problem 5&#10;```rust&#10;fn main() { let number = &quot;T-H-R-E-E&quot;; println!(&quot;Spell a Number : {}&quot;, number); number = 3; println!(&quot;Number plus two is : {}&quot;, number + 2);&#10;}&#10;``` ### Solution 5&#10;```rust&#10;fn main() { let mut number = &quot;T-H-R-E-E&quot;.len(); println!(&quot;Spell a Number : {}&quot;, number); number = 3; println!(&quot;Number plus two is : {}&quot;, number + 2);&#10;}&#10;main()&#10;``` ```&#10;Spell a Number : 9&#10;Number plus two is : 5 ``` ### Problem 6&#10;```rust&#10;const NUMBER = 3;&#10;fn main() { println!(&quot;Number {}&quot;, NUMBER);&#10;}&#10;``` ### Solution 6&#10;```rust&#10;const NUMBER :i32 = 3;&#10;fn main() { println!(&quot;Number {}&quot;, NUMBER);&#10;}&#10;main()&#10;``` ```&#10;Number 3 ``` ```rust ```&#10;");
docs.push({"id":56,"title":"Rust exercises","url":"/rust-exercises.html"});
index.add(57, "# Rust ## Install ```shell&#10;curl --proto &apos;=https&apos; --tlsv1.2 -sSf https://sh.rustup.rs | sh&#10;source $HOME/.cargo/env&#10;``` Create a new project using ```shell&#10;$ cargo new hello_world --bin # for a binary&#10;$ cargo new hello_world # for a library&#10;``` ## Exercises * [Rust exercises](rust-exercises.html), resolution of the [rustlings](https://github.com/rust-lang/rustlings) exercises ## Reference ### List folders recursively Using the `glob` crate: ```rust&#10;use glob::glob; fn main() { for entry in glob(&quot;./**/*.md&quot;).expect(&quot;Failed to read glob pattern&quot;) { match entry { Ok(path) =&gt; println!(&quot;{:?}&quot;, path.display()), Err(e) =&gt; println!(&quot;{:?}&quot;, e), } }&#10;}&#10;```");
docs.push({"id":57,"title":"Rust","url":"/rust.html"});
index.add(58, "# SSH # Configuration ## Config file To add a known server to the `config` file, use the following syntax: ```properties&#10;Host mymachine HostName 127.0.0.1 User root Port 7654&#10;``` There is no method to specify or provide on the command line the password in a non-interactive manner for ssh authentication using a OpenSSH built-in mechanism.");
docs.push({"id":58,"title":"SSH","url":"/ssh.html"});
index.add(59, "# Scikit-learn Collection of notes on [Python&apos;s](python.html) `scikit-learn` machine learning library. * [Optimising random forest hyperparamaters](optimising-random-forest-hyperparamaters.html)");
docs.push({"id":59,"title":"Scikit-learn","url":"/scikit-learn.html"});
index.add(60, "---&#10;date: 2020-04-13T15:21:00+01:00&#10;draft: false&#10;url: &quot;/serving-models-with-seldon.html&quot;&#10;--- # Serving models with Seldon Deploying machine learning models in production comes with several requirements.&#10;We must manage the model lifecycle. We need reproducibility and typically use containerised workflows. [Seldon](https://github.com/SeldonIO/seldon-core) is a tool which aims at providing a production workflow for machine learning models, allowing to build model serving containers which expose well-defined APIs. In this post, I&apos;ll show how to create a simple model and how to deploy it with Seldon. The model is a customer segmentation one. The goal is to classify a customer according to a segment (0, 1 or 2), according to its age, income, whether they engaged with previous campaigns and the campaign type. Once we train the model, we deploy it with Seldon in a container orchestration&#10;platform such as [Kubernetes](https://kubernetes.io/) and [OpenShift](https://www.openshift.com/). ## Create data We use the Python&apos;s [`scikit-learn`](https://scikit-learn.org/stable/) to train our model.&#10;However, we must first simulate some data to train it.&#10;We start by simulating the users age ($a$) and income ($c$). We assume income is correlated with age. $$&#10;c|a \\sim \\mathcal{N}\\left(a + 20, 100\\right) \\\\\\\\&#10;a|k \\sim \\mathcal{U}\\left(A_k, B_k\\right),\\quad A=\\left\\lbrace16, 25, 50, 61\\right\\rbrace,B=\\left\\lbrace24, 49, 60, 90\\right\\rbrace \\\\\\\\&#10;k \\sim \\mathcal{M}\\left(4, \\left\\lbrace 0.15, 0.4, 0.2, 0.25\\right\\rbrace\\right)&#10;$$ ![](./images/seldon_segments.png) Let&apos;s assume we have eight distinct events ($e=\\left(0, 1, \\dots, 7\\right)$). We sample them from a multinomial&#10;distribution and also assume that two different age bands have different distributions, just to add some variation. $$&#10;e = \\begin{cases} \\mathcal{M}\\left(7, \\left\\lbrace 0.026, 0.195, 0.156, 0.208, 0.130, 0.205, 0.078 \\right\\rbrace\\right) &amp; \\text{if}\\ a &lt; 50 \\\\\\\\&#10;\\mathcal{M}\\left(7, \\left\\lbrace 0.052, 0.143, 0.169, 0.182, 0.164, 0.182, 0.104 \\right\\rbrace\\right) &amp; \\text{if}\\ a \\geq 50&#10;\\end{cases}&#10;$$ ![](./images/seldon_hist_event_income.png) The responses are calculated as `0` or `1`, representing &quot;true&quot; or &quot;false&quot;, and sampled from Bernoulli&#10;distributions, with different distributions depending on the event, again just to add some variation. $$&#10;r = \\begin{cases}&#10;\\text{Bernoulli}\\left(0.6\\right) &amp; \\text{if}\\ e \\in \\left(2, 3, 4, 6\\right) \\\\\\\\&#10;\\text{Bernoulli}\\left(0.4\\right) &amp; \\text{if}\\ e \\in \\left(1, 5, 7\\right)&#10;\\end{cases}&#10;$$ To predict the response of a customer, we use a logistic model, with coefficients $\\beta_{age}=-0.0004$ and $\\beta_{income}=0.0001$. For the customer level, we use a negative binomial model with coefficients $\\beta_{age}=-0.0233$ and $\\beta_{income}=0.0054$.&#10;This results in the following distribution of customer levels: ![](./images/seldon_level.png) Finally, we create the response according to negative binomial model with coefficients $\\beta_{level}=0.1862$ and $\\beta_{response}=0.2076$. We get the following segments, stratified by age and income: ![](./images/seldon_segments.png) ## Train model Now that we have our simulated data, we can train a model.&#10;Generally, it is straightforward to train model data when in `pandas` data frame format.&#10;Let&apos;s proceed with creating a data frame with the data we&apos;ve just generated: ```python&#10;import pandas as pd data = { &apos;age&apos;: age, &apos;income&apos;: income, &apos;class&apos;: _class, &apos;response&apos;: response, &apos;segment&apos;: segment, &apos;events&apos;: events } df = pd.DataFrame(data)&#10;``` We now create the training and testing datasets. The first thing is to define the classifier&apos;s `inputs` and `outputs` and then splitting each of them into training and testing. Here I have used a split of 60%/40% for training and testing respectively. ```python&#10;from sklearn.model_selection import train_test_split cols = [&apos;age&apos;, &apos;income&apos;, &apos;response&apos;, &apos;events&apos;]&#10;inputs = df[cols]&#10;outputs = df[&apos;segment&apos;] # split dataset&#10;X_train, X_test, y_train, y_test = \\ train_test_split(inputs, outputs, test_size=0.4, random_state=23)&#10;``` We use a Random Forest classifier as the underlying algorithm for our model.&#10;These are available in `sciki-learn` with the [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class.&#10;However, `scikit-learn` does not support categorical variables out of the box [^2].&#10;To deal with them, we build a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which allows to chain multiple transformations to our data, including a categorical variable processor, such as [`OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) [^3].&#10;We use [`DataFrameMapper`](https://github.com/scikit-learn-contrib/sklearn-pandas) to apply the encoder to the `response` and `events` columns and leave the remaining unchanged. [^2]: As of the time of writing.&#10;[^3]: Other encoders are available in `scikit-learn`. I recommend you experiment with some of them. ```python&#10;from sklearn.ensemble import RandomForestClassifier&#10;from sklearn import preprocessing&#10;from sklearn.pipeline import Pipeline def build_RF_pipeline(inputs, outputs, rf=None): if not rf: rf = RandomForestClassifier() pipeline = Pipeline([ (&quot;mapper&quot;, DataFrameMapper([ ([&apos;response&apos;, &apos;events&apos;], preprocessing.OrdinalEncoder()), ([&apos;age&apos;, &apos;income&apos;], None) ])), (&quot;classifier&quot;, rf) ]) pipeline.fit(inputs, outputs) return pipeline&#10;``` The actual training involves a simple hyper-parameter estimation using&#10;[`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). This method performs a type of parameter grid search but restricting the search to only the specified values. For the scope of this post, it is not&#10;necessary to perform an exhaustive hyperparameter estimation.&#10;The `RF_estimation` function returns the best-fitted model after searching with the test dataset. ```python&#10;def RF_estimation(inputs, outputs, estimator_steps=10, depth_steps=10, min_samples_split=None, min_samples_leaf=None): # hyper-parameter estimation n_estimators = [int(x) for x in np.linspace(start=50, stop=100, num=estimator_steps)] max_depth = [int(x) for x in np.linspace(3, 10, num=depth_steps)] max_depth.append(None) if not min_samples_split: min_samples_split = [1, 2, 4] if not min_samples_leaf: min_samples_leaf = [1, 2, 4] bootstrap = [True, False] random_grid = {&apos;n_estimators&apos;: n_estimators, &apos;max_depth&apos;: max_depth, &apos;min_samples_split&apos;: min_samples_split, &apos;min_samples_leaf&apos;: min_samples_leaf, &apos;bootstrap&apos;: bootstrap} rf_random = RandomizedSearchCV( estimator=RandomForestClassifier(), param_distributions=random_grid, n_iter=100, scoring=&apos;neg_mean_absolute_error&apos;, cv=3, verbose=1, random_state=42, n_jobs=-1) rf_random.fit(inputs, outputs) best_random = rf_random.best_estimator_ return best_random&#10;``` After applying the parameter estimation, we take the best scoring model and calculate the MSE. Unsurprisingly (given the simple model and simulated data), we get a very good fit. ```python&#10;rf_predictions = random_forest_pipeline.predict(X_test)&#10;print(f&quot;MSE: {random_forest_pipeline.score(X_test, y_test)*100}%&quot;)&#10;# MSE: 99.95%&#10;``` The final step is serialising the model. Serialisation is necessary since we only serve the pre-trained model.&#10;To do so, we use the [`joblib`](https://github.com/joblib/joblib) library and save the model to a `model.pkl` file. ```python&#10;import joblib #save mode in filesystem&#10;joblib.dump(random_forest_pipeline, &apos;model.pkl&apos;)&#10;``` ## Deploy model It is important to note that we don&apos;t need the model training code included in the Seldon server. The purpose of Seldon is not to *train* models, but to *deploy* them and manage their lifecycle.&#10;This workflow means that a typical Seldon deployment would only include the prediction endpoint implementation and&#10;a serialised model. This provision is made by firstly create a *wrapper* for our model which implements the Seldon endpoints. ### Simple model We create a Python script called `Model.py` [^1].&#10;The primary prediction endpoint uses the following signature: [^1]: You can use any file name, as long as it&apos;s consistent with `.s2i/environment`, which we&apos;ll look at soon. ```python&#10;def predict(self, X: np.ndarray, names: Iterable[str], meta: Dict = None)&#10;``` The wrapper is straightforward, in this example.&#10;We use the `joblib` library again, to load the serialised model `model.pkl`, and then pass through any JSON payload as inputs (`X`) to the model to get a prediction as well as using Python&apos;s default [logging](https://docs.python.org/3/library/logging.html)&#10;to provide some feedback. ```python&#10;import joblib&#10;import logging class Model(object): def __init__(self): logger.info(&quot;Initializing.&quot;) logger.info(&quot;Loading model.&quot;) self.model = joblib.load(&apos;model.pkl&apos;) def predict(self, X, features_names): return self.model.predict_proba(X)&#10;``` We now build the model using the `s2i` ([source-to-image](https://github.com/openshift/source-to-image)).&#10;As the name implies, `s2i`&apos;s allow to create a container image from source code, taking care of any necessary intermediate steps.&#10;Seldon support several types of builds (such as Python, R and Java) [^4]. [^4]: More information can be found [here](https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/s2i.html). Typically `s2i`&apos;s rely on certain conventions (over configuration) on your application structure. A requirement when building a Seldon model using its `s2i` is to provide some specific environment variables. These are usually stored in a file located in `$REPO/.s2i/environment`. For instance, for this model we use: ```ini&#10;MODEL_NAME=Model&#10;API_TYPE=REST&#10;SERVICE_TYPE=MODEL&#10;PERSISTENCE=0&#10;``` The `MODEL_NAME` corresponds to the script we&apos;ve created previously, `Model.py` and instructs Seldon to use it as the REST endpoint provider. `API_TYPE` defines the endpoint interface. We use the REST interface, other possibilities include gRPC, for instance. ![](./images/diagram.png) To build the container image using the `s2i`, assuming you want an image named `$NAME` and tagged with `$TAG`, we simply need to run: ```bash&#10;$ s2i build $REPO \\ seldonio/seldon-core-s2i-python36:0.18 \\ $NAME:$TAG&#10;``` You can provide the location of your source code either by specifying a remote Git repository or by passing a local one.&#10;Once the container image builds, you can now run it using, for instance: ```shell&#10;docker run -i --rm -p 5000:5000 $NAME:$TAG&#10;``` Let&apos;s get a prediction from the model: ```shell&#10;$ curl --header &quot;Content-Type: application/json&quot; \\ --request POST \\ --data &apos;{&quot;data&quot;:{&quot;ndarray&quot;:[[34.0, 100.0, 1, 2]]}}&apos; \\ http://localhost:5000/predict&#10;``` This will return a prediction: ```json&#10;{ &quot;data&quot;: { &quot;names&quot;: [&quot;t:0&quot;,&quot;t:1&quot;,&quot;t:2&quot;], &quot;ndarray&quot;: [[0.0,0.9980208571211083,0.00197914287889168]]}, &quot;meta&quot;: {}&#10;}&#10;``` This response corresponds to the probability of each segment (`0`, `1` and `2`), respectively.&#10;We can see that a customer with this profile is classified as a segment `1` with an associated probability of 99.8%. ### With metrics Seldon provides basic metrics by default, covering service, predictor and model name, version and image. However, you can directly add custom metrics. Going back to our `Model` wrapper class, we add a new method called `metrics` which returns custom metrics. The metrics are compatible with Prometheus and, therefore, the metric type should be familiar if you have dealt with Prometheus before. These include, for instance: - Counters&#10;- Gauges&#10;- Timers Let&apos;s add to the wrapper: ```python&#10;import joblib&#10;import logging class Model(object): def __init__(self): logger.info(&quot;Initializing.&quot;) logger.info(&quot;Loading model.&quot;) self.model = joblib.load(&apos;model.pkl&apos;) def predict(self, X, features_names): return self.model.predict_proba(X) # new custom metrics endpoint def metrics(self): return [ # a counter which will increase by the given value {&quot;type&quot;: &quot;COUNTER&quot;, &quot;key&quot;: &quot;mycounter&quot;, &quot;value&quot;: 1}, # a gauge which will be set to given value {&quot;type&quot;: &quot;GAUGE&quot;, &quot;key&quot;: &quot;mygauge&quot;, &quot;value&quot;: 10}, # a timer which will add sum and count metrics - assumed millisecs {&quot;type&quot;: &quot;TIMER&quot;, &quot;key&quot;: &quot;mytimer&quot;, &quot;value&quot;: 1.1}, ]&#10;``` If we now request a new prediction, as previously, we can see the custom metrics included in the model&apos;s response. ```json&#10;{ &quot;data&quot;: { &quot;names&quot;: [&quot;t:0&quot;,&quot;t:1&quot;,&quot;t:2&quot;], &quot;ndarray&quot;:[[0.0,0.9980208571211083,0.00197914287889168]]}, &quot;meta&quot;: { &quot;metrics&quot;: [ {&quot;key&quot;:&quot;mycounter&quot;,&quot;type&quot;:&quot;COUNTER&quot;,&quot;value&quot;:1}, {&quot;key&quot;:&quot;mygauge&quot;,&quot;type&quot;:&quot;GAUGE&quot;,&quot;value&quot;:10}, {&quot;key&quot;:&quot;mytimer&quot;,&quot;type&quot;:&quot;TIMER&quot;,&quot;value&quot;:1.1}] }&#10;}&#10;``` These values are available via the Prometheus endpoint. The model can also be easily deployed in a container platform, for instance, OpenShift. Assuming you are logged to a cluster and your image is a registry accessible by OpenShift, you can simply deploy it using: ```shell&#10;$ oc new-app $NAME:$TAG&#10;``` I hope this was useful to you.&#10;Happy coding!");
docs.push({"id":60,"title":"Serving models with Seldon","url":"/serving-models-with-seldon.html"});
index.add(61, "# Shell configurations According to Bash&apos;s `man`: * `/bin/bash` * The bash executable&#10;* `/etc/profile` * The system-wide initialization file, executed for login shells&#10;* `~/.bash_profile` * The personal initialization file, executed for login shells&#10;* `~/.bashrc` * The individual per-interactive-shell startup file&#10;* `~/.bash_logout` * The individual login shell cleanup file, executed when a login shell exits&#10;* `~/.inputrc` * Individual readline initialization file With `zsh`, `.zshrc` is always read for an interactive shell, whether it&apos;s a login one or not.");
docs.push({"id":61,"title":"Shell configurations","url":"/shell-configurations.html"});
index.add(62, "# Shell tricks ## Reset cursor&#10;Sometimes, especially when using ANSI escape code heavy applications, your terminal state might get mangled.&#10;If that&apos;s the case, it&apos;s just a matter of performing a [VT320 escape sequence](https://en.wikipedia.org/wiki/ANSI_escape_code#CSI_sequences) to reset the state.&#10;For instance, in [[zsh]], using the &apos;unhide&apos; command: ```shell&#10;echo -en &quot;\\e[?25h&quot;&#10;```");
docs.push({"id":62,"title":"Shell tricks","url":"/shell-tricks.html"});
index.add(63, "---&#10;date: 2016-06-23T21:55:00+01:00&#10;draft: false&#10;url: &quot;/langtons-ant.html&quot;&#10;description: Technical details about this site, ruivieira.dev&#10;--- # Site details ## Assets This site&apos;s CSS takes 2796 bytes.&#10;This, however, does not include the following dependencies: * LaTeX processor ([KaTeX](https://katex.org/))&#10;* custom monospaced font ([Julia Mono](https://juliamono.netlify.app/)). The minimal CSS is heavily inspired by [58 bytes of css to look great nearly everywhere](https://jrl.ninja/etc/1/). The site is generated from a set of Markdown files using a [Go](go.html) pre-processor, which mainly converts the Markdown to HTML and gathers some metadata (such as picking the backlinks for each page). The generator also automatically detects whether a page includes mathematical notation and if so, the proper dependencies are added. ## Search The site is searchable from [here](/search.html).&#10;The search page also allows for query string searches using the `q` keyword, for instance: ```&#10;/search.html?q=statistics&#10;``` This allows you to add a custom search engine to most modern browsers, such as Firefox or Chrome. Search is done 100% client-side, so there&apos;s absolutely no information collected regarding your search queries. Highlighting of terms is also possible just adding the query parameter `?h=...` to *any page*. For instance, to highlight the term `privacy` on this page, simply go to ```&#10;/site-details.html?h=privacy&#10;``` or click [here](/site-details.html?h=privacy).&#10;## Privacy No cookies are used on this site. All site traffic statistics are captured using [GoatCounter](https://www.goatcounter.com/). Goatcounter is an open-source, privacy-friendly analytics site, which** doesn&apos;t use cookies** and collects minimal information, just enough to produce a few useful summaries. In line with the desire for total transparency for all visitors, I&apos;ve made the realtime stats dashboard for this site **public** and available at [https://ruivieira-dev.goatcounter.com/](https://ruivieira-dev.goatcounter.com/?hl-period=year). You can see for yourself which data is collected. Please let me know if you have any concerns about this site&apos;s privacy policy by dropping me a message at [@ruivieira@mastodon.technology](https://mastodon.technology/@ruivieira). As mentioned previously, search is done 100% client-side, so no information is collected about your search queries. ## Javascript All pages *will work* the same with or without Javascript, except the following: * The [link graph page](/graph.html)&#10;* The [search page](/search.html) So if you want to disable Javascript for this site, please do! It won&apos;t affect any of the content. ## Text mode Apart from the obvious lack of images, the vast majority of this site will also work with a text-based browser (such as [lynx](https://invisible-island.net/lynx/)). Even the code examples are quite readable (for instance, `pandas` dataframes are properly rendered as text tables). Give it a go by installing lynx and running `lynx https://ruivieira.dev`. ## Keeping up to date If you want to keep up to date and be notified when new content is added to this site, at the moment the best way is to follow the [Mesozoic Mastodon bot](https://botsin.space/@mesozoic). This bot is part of a [Git](git.html) `pre-push` hook and will &quot;toot&quot; whenever changes to the site&apos;s source are made. ");
docs.push({"id":63,"title":"Site details","url":"/site-details.html"});
index.add(64, "# Synthetic Data Generation with SDV * [Synthetic data with SDV and Gaussian copulas](synthetic-data-with-sdv-and-gaussian-copulas.html)&#10;* [Synthetic data with SDV and CTGAN](synthetic-data-with-sdv-and-ctgan.html)&#10;* [Synthetic data with SDV and CopulaGAN](synthetic-data-with-sdv-and-copulagan.html)");
docs.push({"id":64,"title":"Synthetic Data Generation with SDV","url":"/synthetic-data-generation-with-sdv.html"});
index.add(65, "# Generating synthetic data Synthetic data will be used mainly for these scenarios: - Regression&#10;- Classification Here we will mainly look at the methods provided by `scikit-learn` to generate synthetic datasets. For more advanced methods, such as using the SDV library please check the [SDV page](synthetic-data-generation-with-sdv.html). It support methods such as [Gaussian copulas](synthetic-data-with-sdv-and-gaussian-copulas.html), [CTGAN](synthetic-data-with-sdv-and-ctgan.html) and [CopulaGAN](synthetic-data-with-sdv-and-copulagan.html).&#10;## Regression data What does a regression consist of? For this section we will mainly use `scikit-learn`&apos;s [`make_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) method. For reproducibility, we will set a `random_state`.&#10;```python&#10;import warnings&#10;warnings.filterwarnings(&apos;ignore&apos;)&#10;warnings.simplefilter(&apos;ignore&apos;) random_state = 23&#10;``` We will create a dataset using `make_regression`&apos;s random linear regression model with input features $x=(f_1,f_2,f_3,f_4)$ and an output $y$.&#10;```python&#10;import matplotlib.pyplot as plt&#10;from plotnine import *&#10;from plotnine.data import *&#10;import numpy as np&#10;import pandas as pd&#10;from sklearn.datasets import make_regression&#10;from scipy.stats import linregress N_FEATURES = 4&#10;N_TARGETS = 1&#10;N_SAMPLES = 100 dataset = make_regression( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=2, n_targets=N_TARGETS, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=random_state,&#10;) print(dataset[0][:10])&#10;print(dataset[1][:10])&#10;``` ```&#10;[[ 0.87305874 -1.63096187 0.52538404 -0.19035824] [ 1.00698671 0.79834941 -0.04057655 -0.31358605] [-0.61464273 1.65110321 0.75791487 -0.0039844 ] [-1.08536678 1.82337823 0.4612592 -1.72325306] [-1.67774847 -0.54401341 0.86347869 -0.30250463] [-0.02427254 0.75537599 -0.04644972 -0.85153564] [-0.48085576 0.82100952 -0.9390196 -0.25870492] [-0.66772841 -2.46244005 -0.19855095 -1.85756579] [-0.29810663 -0.02239635 0.25363492 -1.22688366] [ 1.48146924 0.38269965 -1.18208819 -1.31062148]]&#10;[ 20.00449025 -30.41054677 52.65371365 -119.26376184 33.78805456 -78.12189078 -88.41673748 -177.21674804 -90.13920313 -197.90799195] ``` Let&apos;s turn this dataset into a Pandas `DataFrame`:&#10;```python&#10;df = pd.DataFrame(data=dataset[0], columns=[f&quot;f{i+1}&quot; for i in range(N_FEATURES)]) df[&quot;y&quot;] = dataset[1]&#10;``` ```python&#10;df.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;f1&lt;/th&gt; &lt;th&gt;f2&lt;/th&gt; &lt;th&gt;f3&lt;/th&gt; &lt;th&gt;f4&lt;/th&gt; &lt;th&gt;y&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;0.873059&lt;/td&gt; &lt;td&gt;-1.630962&lt;/td&gt; &lt;td&gt;0.525384&lt;/td&gt; &lt;td&gt;-0.190358&lt;/td&gt; &lt;td&gt;20.004490&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;1.006987&lt;/td&gt; &lt;td&gt;0.798349&lt;/td&gt; &lt;td&gt;-0.040577&lt;/td&gt; &lt;td&gt;-0.313586&lt;/td&gt; &lt;td&gt;-30.410547&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;-0.614643&lt;/td&gt; &lt;td&gt;1.651103&lt;/td&gt; &lt;td&gt;0.757915&lt;/td&gt; &lt;td&gt;-0.003984&lt;/td&gt; &lt;td&gt;52.653714&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;-1.085367&lt;/td&gt; &lt;td&gt;1.823378&lt;/td&gt; &lt;td&gt;0.461259&lt;/td&gt; &lt;td&gt;-1.723253&lt;/td&gt; &lt;td&gt;-119.263762&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;-1.677748&lt;/td&gt; &lt;td&gt;-0.544013&lt;/td&gt; &lt;td&gt;0.863479&lt;/td&gt; &lt;td&gt;-0.302505&lt;/td&gt; &lt;td&gt;33.788055&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;Let&apos;s plot the data:&#10;```python&#10;from plotutils import * def plot_regression(df, size): for i in range(size): fit = np.polyfit(df[df.columns[i]], df[&quot;y&quot;], 1) fit_fn = np.poly1d(fit) plt.subplot(2, 2, i + 1) plt.xlabel(&quot;y&quot;) plt.ylabel(f&quot;f{i+1}&quot;) plt.scatter(df[df.columns[i]], df[&quot;y&quot;], s=30, c=colours[1], edgecolor=edges[1]) plt.plot( df[df.columns[i]], fit_fn(df[df.columns[i]]), ls=&quot;--&quot;, c=colours[0], lw=1 ) plot_regression(df, N_FEATURES)&#10;``` ![synthetic-data-generation_1](./images/synthetic-data-generation_1.png)&#10;### Changing the Gaussian noise level&#10;The `noise` parameter in `make_regression` allows to adjust the scale of the data&apos;s gaussian centered noise.&#10;```python&#10;dataset = make_regression( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=2, n_targets=N_TARGETS, bias=0.0, effective_rank=None, tail_strength=0.5, noise=2.0, shuffle=True, coef=False, random_state=random_state,&#10;) df = pd.DataFrame(data=dataset[0], columns=[f&quot;f{i+1}&quot; for i in range(N_FEATURES)]) df[&quot;y&quot;] = dataset[1]&#10;``` ```python&#10;plot_regression(df, N_FEATURES)&#10;``` ![synthetic-data-generation_2](./images/synthetic-data-generation_2.png)&#10;### Visualising increasing noise Let&apos;s increase the noise by $10^i$, for $i=1, 2, 3$ and see what the data looks like.&#10;```python&#10;df = pd.DataFrame(data=np.zeros((N_SAMPLES, 1))) def create_noisy_data(noise): return make_regression( n_samples=N_SAMPLES, n_features=1, n_informative=1, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=noise, shuffle=True, coef=False, random_state=random_state, ) for i in range(3): data = create_noisy_data(10 ** i) df[f&quot;f{i+1}&quot;] = data[0] df[f&quot;y{i+1}&quot;] = data[1]&#10;``` ```python&#10;for i in range(3): fit = np.polyfit(df[f&quot;f{i+1}&quot;], df[f&quot;y{i+1}&quot;], 1) fit_fn = np.poly1d(fit) plt.subplot(1, 3, i + 1) plt.scatter(df[f&quot;f{i+1}&quot;], df[f&quot;y{i+1}&quot;], s=30, c=colours[1], edgecolor=edges[1]) plt.plot( df[f&quot;f{i+1}&quot;], fit_fn(df[f&quot;f{i+1}&quot;]), ls=&quot;--&quot;, color=colours[0], lw=1, ) plt.xlabel(f&quot;f{i+1}&quot;) plt.ylabel(f&quot;y{i+1}&quot;)&#10;``` ![synthetic-data-generation_3](./images/synthetic-data-generation_3.png)&#10;(data:classification)=&#10;## Classification data To generate data for classification we will use the [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) method.&#10;```python&#10;from sklearn.datasets import make_classification N = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=random_state,&#10;) df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(N)]) df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;df.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;f1&lt;/th&gt; &lt;th&gt;f2&lt;/th&gt; &lt;th&gt;f3&lt;/th&gt; &lt;th&gt;f4&lt;/th&gt; &lt;th&gt;y&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;-3.216&lt;/td&gt; &lt;td&gt;-0.416&lt;/td&gt; &lt;td&gt;-1.295&lt;/td&gt; &lt;td&gt;-1.882&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;-1.426&lt;/td&gt; &lt;td&gt;-1.257&lt;/td&gt; &lt;td&gt;-1.734&lt;/td&gt; &lt;td&gt;-1.804&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;2.798&lt;/td&gt; &lt;td&gt;-3.010&lt;/td&gt; &lt;td&gt;-1.085&lt;/td&gt; &lt;td&gt;-3.134&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;0.633&lt;/td&gt; &lt;td&gt;2.502&lt;/td&gt; &lt;td&gt;-1.553&lt;/td&gt; &lt;td&gt;1.625&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;1.494&lt;/td&gt; &lt;td&gt;0.912&lt;/td&gt; &lt;td&gt;-1.887&lt;/td&gt; &lt;td&gt;-1.457&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from itertools import combinations&#10;from math import ceil lst_var = list(combinations(df.columns[:-1], 2))&#10;len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[&quot;y&quot;].apply(lambda y: colours[y]), edgecolor=df[&quot;y&quot;].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2)&#10;``` ![synthetic-data-generation_4](./images/synthetic-data-generation_4.png)&#10;### Cluster separation According to the docs[^1], `class_sep` is the factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier. [^1]: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html&#10;```python&#10;N_FEATURES = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=3.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None,&#10;) df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(N_FEATURES)]) df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;from itertools import combinations&#10;from math import ceil lst_var = list(combinations(df.columns[:-1], 2))&#10;len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[&quot;y&quot;].apply(lambda y: colours[y]), edgecolor=df[&quot;y&quot;].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2)&#10;``` ![synthetic-data-generation_5](./images/synthetic-data-generation_5.png)&#10;We can make the cluster separability more difficult, by decreasing the value of `class_sep`.&#10;```python&#10;N_FEATURES = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None,&#10;) df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(N_FEATURES)]) df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;from itertools import combinations&#10;from math import ceil lst_var = list(combinations(df.columns[:-1], 2))&#10;len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[&quot;y&quot;].apply(lambda y: colours[y]), edgecolor=df[&quot;y&quot;].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2)&#10;``` ![synthetic-data-generation_6](./images/synthetic-data-generation_6.png)&#10;### Noise level According to the documentation[^1], `flip_y` is the fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder.&#10;```python&#10;N_FEATURES = 4 for i in range(6): data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.1 * i, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(N_FEATURES)]) df[&quot;y&quot;] = data[1] plt.subplot(2, 3, i + 1) plt.title(f&quot;flip_y={round(0.1*i,2)}&quot;) plt.scatter( df[&quot;f1&quot;], df[&quot;f2&quot;], s=50, c=df[&quot;y&quot;].apply(lambda y: colours[y]), edgecolor=df[&quot;y&quot;].apply(lambda y: edges[y]), )&#10;plt.tight_layout(pad=3.0)&#10;``` ![synthetic-data-generation_7](./images/synthetic-data-generation_7.png)&#10;```python&#10;df = pd.DataFrame(data=np.zeros((N_SAMPLES, 1))) for i in range(3): data = make_classification( n_samples=N_SAMPLES, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0, class_sep=i + 0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=random_state, ) df[f&quot;f{i+1}1&quot;] = data[0][:, 0] df[f&quot;f{i+1}2&quot;] = data[0][:, 1] df[f&quot;t{i+1}&quot;] = data[1] for i in range(3): plt.subplot(1, 3, i + 1) plt.scatter( df[f&quot;f{i+1}1&quot;], df[f&quot;f{i+1}2&quot;], s=50, c=df[f&quot;t{i+1}&quot;].apply(lambda y: colours[y]), edgecolor=df[f&quot;t{i+1}&quot;].apply(lambda y: edges[y]), )&#10;``` ![synthetic-data-generation_8](./images/synthetic-data-generation_8.png)&#10;It is noteworthy that many paremeters in `scikit-learn` for synthetic data generation allow inputs per feature or cluster.&#10;To do so, we simple pass the parameter value as an array.&#10;For instance, to ```python&#10;N = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=random_state,&#10;) df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(N)]) df[&quot;y&quot;] = data[1]&#10;``` ## Separability&#10;```python&#10;from sklearn.datasets import make_blobs N_FEATURE = 4 data = make_blobs( n_samples=60, n_features=N_FEATURE, centers=3, cluster_std=1.0, center_box=(-5.0, 5.0), shuffle=True, random_state=None,&#10;)&#10;df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(N_FEATURE)])&#10;df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;from itertools import combinations&#10;from math import ceil lst_var = list(combinations(df.columns[:-1], 2))&#10;len_var = len(lst_var)&#10;for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[&quot;y&quot;].apply(lambda y: colours[y]), edgecolor=df[&quot;y&quot;].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2)&#10;``` ![synthetic-data-generation_9](./images/synthetic-data-generation_9.png)&#10;To make a cluster more separable we can change `cluster_std`.&#10;```python&#10;data = make_blobs( n_samples=60, n_features=N_FEATURES, centers=3, cluster_std=0.3, center_box=(-5.0, 5.0), shuffle=True, random_state=None,&#10;)&#10;df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(N_FEATURES)])&#10;df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;from itertools import combinations&#10;from math import ceil lst_var = list(combinations(df.columns[:-1], 2))&#10;len_var = len(lst_var)&#10;for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[&quot;y&quot;].apply(lambda y: colours[y]), edgecolor=df[&quot;y&quot;].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2)&#10;``` ![synthetic-data-generation_10](./images/synthetic-data-generation_10.png)&#10;By decreasing `cluster_std` we make them less separable.&#10;```python&#10;data = make_blobs( n_samples=60, n_features=N_FEATURES, centers=3, cluster_std=2.5, center_box=(-5.0, 5.0), shuffle=True, random_state=None,&#10;)&#10;df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(N_FEATURES)])&#10;df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;from itertools import combinations&#10;from math import ceil lst_var = list(combinations(df.columns[:-1], 2))&#10;len_var = len(lst_var)&#10;for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[&quot;y&quot;].apply(lambda y: colours[y]), edgecolor=df[&quot;y&quot;].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2)&#10;``` ![synthetic-data-generation_11](./images/synthetic-data-generation_11.png)&#10;### Anisotropic data&#10;```python&#10;data = make_blobs(n_samples=50, n_features=2, centers=3, cluster_std=1.5)&#10;``` ```python&#10;transformation = [[0.5, -0.5], [-0.4, 0.8]]&#10;``` ```python&#10;data_0 = np.dot(data[0], transformation)&#10;df = pd.DataFrame(data_0, columns=[f&quot;f{i}&quot; for i in range(1, 3)])&#10;df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;plt.scatter( df[&quot;f1&quot;], df[&quot;f2&quot;], c=df[&quot;y&quot;].apply(lambda y: colours[y]), s=50, edgecolors=df[&quot;y&quot;].apply(lambda y: edges[y]),&#10;)&#10;plt.xlabel(&quot;f1&quot;)&#10;plt.ylabel(&quot;f2&quot;)&#10;plt.show()&#10;``` ![synthetic-data-generation_12](./images/synthetic-data-generation_12.png)&#10;## Concentric clusters&#10;Sometimes we might be interested in creating a non-separable cluster. The simples way is to create concentric clusters with the `make_circles` method.&#10;```python&#10;from sklearn.datasets import make_circles data = make_circles( n_samples=N_SAMPLES, shuffle=True, noise=None, random_state=random_state, factor=0.6&#10;)&#10;df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(2)])&#10;df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;plt.scatter( df[&quot;f1&quot;], df[&quot;f2&quot;], c=df[&quot;y&quot;].apply(lambda y: colours[y]), s=50, edgecolors=df[&quot;y&quot;].apply(lambda y: edges[y]),&#10;)&#10;plt.xlabel(&quot;f1&quot;)&#10;plt.ylabel(&quot;f2&quot;)&#10;plt.show()&#10;``` ![synthetic-data-generation_13](./images/synthetic-data-generation_13.png)&#10;### Adding noise The `noise` parameter allows to create a concentric noisy dataset.&#10;```python&#10;data = make_circles( n_samples=N_SAMPLES, shuffle=True, noise=0.15, random_state=random_state, factor=0.6&#10;)&#10;df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(2)])&#10;df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;plt.scatter( df[&quot;f1&quot;], df[&quot;f2&quot;], c=df[&quot;y&quot;].apply(lambda y: colours[y]), s=50, edgecolors=df[&quot;y&quot;].apply(lambda y: edges[y]),&#10;)&#10;plt.xlabel(&quot;f1&quot;)&#10;plt.ylabel(&quot;f2&quot;)&#10;plt.show()&#10;``` ![synthetic-data-generation_14](./images/synthetic-data-generation_14.png)&#10;## Moon clusters A shape that can be useful to other methos (such as [counterfactuals](counterfactuals.html), for instance) is the one generated by the `make_moons` method.&#10;```python&#10;from sklearn.datasets import make_moons data = make_moons( n_samples=N_SAMPLES, shuffle=True, noise=None, random_state=random_state&#10;)&#10;df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(2)])&#10;df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;plt.scatter( df[&quot;f1&quot;], df[&quot;f2&quot;], c=df[&quot;y&quot;].apply(lambda y: colours[y]), s=50, edgecolors=df[&quot;y&quot;].apply(lambda y: edges[y]),&#10;)&#10;plt.xlabel(&quot;f1&quot;)&#10;plt.ylabel(&quot;f2&quot;)&#10;plt.show()&#10;``` ![synthetic-data-generation_15](./images/synthetic-data-generation_15.png)&#10;### Adding noise As usual, the `noise` parameter allows to control the noise.&#10;```python&#10;data = make_moons( n_samples=N_SAMPLES, shuffle=True, noise=0.1, random_state=random_state&#10;)&#10;df = pd.DataFrame(data[0], columns=[f&quot;f{i+1}&quot; for i in range(2)])&#10;df[&quot;y&quot;] = data[1]&#10;``` ```python&#10;plt.scatter( df[&quot;f1&quot;], df[&quot;f2&quot;], c=df[&quot;y&quot;].apply(lambda y: colours[y]), s=50, edgecolors=df[&quot;y&quot;].apply(lambda y: edges[y]),&#10;)&#10;plt.xlabel(&quot;f1&quot;)&#10;plt.ylabel(&quot;f2&quot;)&#10;plt.show()&#10;``` ![synthetic-data-generation_16](./images/synthetic-data-generation_16.png)&#10;```python ```&#10;");
docs.push({"id":65,"title":"Synthetic Data Generation","url":"/synthetic-data-generation.html"});
index.add(66, "# Synthetic data with SDV and CTGAN&#10;```python&#10;import pandas as pd&#10;import warnings warnings.filterwarnings(&apos;ignore&apos;) data = pd.read_csv(&quot;data/svm-hyperparameters-train-features.csv&quot;)&#10;``` ```python&#10;data.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;22.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.2500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;38.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;71.2833&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;26.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.9250&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;35.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;53.1000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;35.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;8.0500&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;data.describe(include=&apos;all&apos;)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;count&lt;/th&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;mean&lt;/th&gt; &lt;td&gt;2.308642&lt;/td&gt; &lt;td&gt;0.647587&lt;/td&gt; &lt;td&gt;29.758889&lt;/td&gt; &lt;td&gt;0.523008&lt;/td&gt; &lt;td&gt;0.381594&lt;/td&gt; &lt;td&gt;32.204208&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;std&lt;/th&gt; &lt;td&gt;0.836071&lt;/td&gt; &lt;td&gt;0.477990&lt;/td&gt; &lt;td&gt;13.002570&lt;/td&gt; &lt;td&gt;1.102743&lt;/td&gt; &lt;td&gt;0.806057&lt;/td&gt; &lt;td&gt;49.693429&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;min&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.420000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;25%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;22.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;7.910400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;50%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;30.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;14.454200&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;35.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;31.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;max&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;80.000000&lt;/td&gt; &lt;td&gt;8.000000&lt;/td&gt; &lt;td&gt;6.000000&lt;/td&gt; &lt;td&gt;512.329200&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from sdv.tabular import CTGAN&#10;``` ```python&#10;model = CTGAN()&#10;``` ```python&#10;model.fit(data)&#10;``` ```python&#10;new_data = model.sample(200)&#10;``` ```python&#10;new_data.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;2.842574&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;28.927918&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;47.380061&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;122.939126&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;42.536188&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;36.907182&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;28.853204&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;37.291651&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;35.857498&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;77.988031&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;new_data.describe(include=&apos;all&apos;)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;count&lt;/th&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;mean&lt;/th&gt; &lt;td&gt;1.310000&lt;/td&gt; &lt;td&gt;0.255000&lt;/td&gt; &lt;td&gt;27.552343&lt;/td&gt; &lt;td&gt;1.170000&lt;/td&gt; &lt;td&gt;1.250000&lt;/td&gt; &lt;td&gt;104.137308&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;std&lt;/th&gt; &lt;td&gt;0.858829&lt;/td&gt; &lt;td&gt;0.436955&lt;/td&gt; &lt;td&gt;14.735829&lt;/td&gt; &lt;td&gt;1.730919&lt;/td&gt; &lt;td&gt;1.399031&lt;/td&gt; &lt;td&gt;137.597823&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;min&lt;/th&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;-6.115006&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;-85.821346&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;25%&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;19.907786&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;27.131476&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;50%&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;29.180020&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;56.792348&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;36.793752&lt;/td&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;102.704251&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;max&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;64.004638&lt;/td&gt; &lt;td&gt;8.000000&lt;/td&gt; &lt;td&gt;6.000000&lt;/td&gt; &lt;td&gt;747.924987&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from sdv.evaluation import evaluate evaluate(new_data, data)&#10;``` ```python&#10;model = CTGAN( epochs=500, batch_size=100, generator_dim=(256, 256, 256), discriminator_dim=(256, 256, 256)&#10;)&#10;``` ```python&#10;model.fit(data)&#10;``` ```python&#10;new_data = model.sample(200)&#10;``` ```python&#10;new_data.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;29.551254&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;257.363295&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;49.643384&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;-1&lt;/td&gt; &lt;td&gt;53.014717&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-14.547607&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0.694069&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.065562&lt;/td&gt; &lt;td&gt;5&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;19.361019&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;28.684305&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;-256.489558&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;new_data.describe(include=&apos;all&apos;)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;count&lt;/th&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;mean&lt;/th&gt; &lt;td&gt;1.330000&lt;/td&gt; &lt;td&gt;0.215000&lt;/td&gt; &lt;td&gt;28.184757&lt;/td&gt; &lt;td&gt;1.200000&lt;/td&gt; &lt;td&gt;1.015000&lt;/td&gt; &lt;td&gt;153.406430&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;std&lt;/th&gt; &lt;td&gt;0.880327&lt;/td&gt; &lt;td&gt;0.411853&lt;/td&gt; &lt;td&gt;24.932518&lt;/td&gt; &lt;td&gt;2.682532&lt;/td&gt; &lt;td&gt;1.737779&lt;/td&gt; &lt;td&gt;278.082600&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;min&lt;/th&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;-17.001736&lt;/td&gt; &lt;td&gt;-4.000000&lt;/td&gt; &lt;td&gt;-1.000000&lt;/td&gt; &lt;td&gt;-433.103355&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;25%&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;12.779717&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;13.491768&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;50%&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;28.262440&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;45.898936&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;34.990247&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;215.423687&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;max&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;93.451393&lt;/td&gt; &lt;td&gt;13.000000&lt;/td&gt; &lt;td&gt;8.000000&lt;/td&gt; &lt;td&gt;1104.455566&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;evaluate(new_data, data)&#10;``` ```python ```&#10;");
docs.push({"id":66,"title":"Synthetic data with SDV and CTGAN","url":"/synthetic-data-with-sdv-and-ctgan.html"});
index.add(67, "# Synthetic data with SDV and CopulaGAN&#10;```python&#10;import pandas as pd&#10;import warnings warnings.filterwarnings(&apos;ignore&apos;) data = pd.read_csv(&quot;data/svm-hyperparameters-train-features.csv&quot;)&#10;``` ```python&#10;data.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;22.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.2500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;38.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;71.2833&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;26.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.9250&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;35.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;53.1000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;35.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;8.0500&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;data.describe(include=&apos;all&apos;)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;count&lt;/th&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;mean&lt;/th&gt; &lt;td&gt;2.308642&lt;/td&gt; &lt;td&gt;0.647587&lt;/td&gt; &lt;td&gt;29.758889&lt;/td&gt; &lt;td&gt;0.523008&lt;/td&gt; &lt;td&gt;0.381594&lt;/td&gt; &lt;td&gt;32.204208&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;std&lt;/th&gt; &lt;td&gt;0.836071&lt;/td&gt; &lt;td&gt;0.477990&lt;/td&gt; &lt;td&gt;13.002570&lt;/td&gt; &lt;td&gt;1.102743&lt;/td&gt; &lt;td&gt;0.806057&lt;/td&gt; &lt;td&gt;49.693429&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;min&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.420000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;25%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;22.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;7.910400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;50%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;30.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;14.454200&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;35.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;31.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;max&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;80.000000&lt;/td&gt; &lt;td&gt;8.000000&lt;/td&gt; &lt;td&gt;6.000000&lt;/td&gt; &lt;td&gt;512.329200&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from sdv.tabular import CopulaGAN&#10;``` ```python&#10;model = CopulaGAN()&#10;``` ```python&#10;model.fit(data)&#10;``` ```python&#10;new_data = model.sample(200)&#10;``` ```python&#10;new_data.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;23.113076&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;29.937752&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;29.918377&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;5.958775&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;76.710166&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;202.889201&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;46.581959&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;89.833194&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;33.135297&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;118.551008&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;new_data.describe(include=&apos;all&apos;)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;count&lt;/th&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.00000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;mean&lt;/th&gt; &lt;td&gt;1.960000&lt;/td&gt; &lt;td&gt;0.540000&lt;/td&gt; &lt;td&gt;37.131076&lt;/td&gt; &lt;td&gt;1.10500&lt;/td&gt; &lt;td&gt;0.975000&lt;/td&gt; &lt;td&gt;69.046022&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;std&lt;/th&gt; &lt;td&gt;0.831781&lt;/td&gt; &lt;td&gt;0.499648&lt;/td&gt; &lt;td&gt;17.131095&lt;/td&gt; &lt;td&gt;1.03408&lt;/td&gt; &lt;td&gt;1.196467&lt;/td&gt; &lt;td&gt;70.354667&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;min&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;3.409883&lt;/td&gt; &lt;td&gt;0.00000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;3.135404&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;25%&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;27.213419&lt;/td&gt; &lt;td&gt;0.00000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;10.772323&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;50%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;31.788050&lt;/td&gt; &lt;td&gt;1.00000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;42.555842&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;49.398770&lt;/td&gt; &lt;td&gt;2.00000&lt;/td&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;105.964913&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;max&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;78.841709&lt;/td&gt; &lt;td&gt;5.00000&lt;/td&gt; &lt;td&gt;5.000000&lt;/td&gt; &lt;td&gt;294.906651&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from sdv.evaluation import evaluate evaluate(new_data, data)&#10;``` ```python&#10;model = CopulaGAN( field_transformers={ &apos;Pclass&apos;: &apos;categorical&apos;, &apos;Sex&apos;: &apos;categorical&apos;, &apos;Age&apos;: &apos;float&apos;, &apos;SibSp&apos;: &apos;boolean&apos;, &apos;Parch&apos;: &apos;integer&apos;, &apos;Fare&apos;: &apos;float&apos; }, field_distributions={ &apos;Fare&apos;: &apos;truncated_gaussian&apos; }&#10;)&#10;``` ```python&#10;model.fit(data)&#10;``` ```python&#10;new_data = model.sample(200)&#10;``` ```python&#10;new_data.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;55.113425&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-9.755683e-08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;42.257149&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;7.252636e+00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;30.128989&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.708430e+00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;30.838961&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-1.184872e-07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;30.370534&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;3.530599e+00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;new_data.describe(include=&apos;all&apos;)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;count&lt;/th&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;200.000000&lt;/td&gt; &lt;td&gt;2.000000e+02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;mean&lt;/th&gt; &lt;td&gt;2.265000&lt;/td&gt; &lt;td&gt;0.630000&lt;/td&gt; &lt;td&gt;33.075478&lt;/td&gt; &lt;td&gt;0.715000&lt;/td&gt; &lt;td&gt;0.915000&lt;/td&gt; &lt;td&gt;2.587770e+01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;std&lt;/th&gt; &lt;td&gt;0.726231&lt;/td&gt; &lt;td&gt;0.484016&lt;/td&gt; &lt;td&gt;13.472204&lt;/td&gt; &lt;td&gt;0.452547&lt;/td&gt; &lt;td&gt;1.189474&lt;/td&gt; &lt;td&gt;3.599977e+01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;min&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;-1.122344&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;-1.191555e-07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;25%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;26.064943&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;6.589631e+00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;50%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;30.706679&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;1.182216e+01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;41.977015&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;2.797455e+01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;max&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;74.046067&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;5.000000&lt;/td&gt; &lt;td&gt;2.014112e+02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;evaluate(new_data, data)&#10;``` ```python ```&#10;");
docs.push({"id":67,"title":"Synthetic data with SDV and CopulaGAN","url":"/synthetic-data-with-sdv-and-copulagan.html"});
index.add(68, "# Synthetic data with SVD and Gaussian copulas&#10;```python&#10;import pandas as pd data = pd.read_csv(&quot;data/svm-hyperparameters-train-features.csv&quot;)&#10;``` ```python&#10;data.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;22.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.2500&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;38.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;71.2833&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;26.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;7.9250&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;35.0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;53.1000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;35.0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;8.0500&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;data.describe(include=&quot;all&quot;)&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;count&lt;/th&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;td&gt;891.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;mean&lt;/th&gt; &lt;td&gt;2.308642&lt;/td&gt; &lt;td&gt;0.647587&lt;/td&gt; &lt;td&gt;29.758889&lt;/td&gt; &lt;td&gt;0.523008&lt;/td&gt; &lt;td&gt;0.381594&lt;/td&gt; &lt;td&gt;32.204208&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;std&lt;/th&gt; &lt;td&gt;0.836071&lt;/td&gt; &lt;td&gt;0.477990&lt;/td&gt; &lt;td&gt;13.002570&lt;/td&gt; &lt;td&gt;1.102743&lt;/td&gt; &lt;td&gt;0.806057&lt;/td&gt; &lt;td&gt;49.693429&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;min&lt;/th&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.420000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;25%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;22.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;7.910400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;50%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;30.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;14.454200&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;35.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;31.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;max&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;80.000000&lt;/td&gt; &lt;td&gt;8.000000&lt;/td&gt; &lt;td&gt;6.000000&lt;/td&gt; &lt;td&gt;512.329200&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from sdv.tabular import GaussianCopula&#10;``` ```python&#10;model = GaussianCopula()&#10;``` ```python&#10;model.fit(data)&#10;``` ```python&#10;N_SAMPLES = 1000&#10;``` ```python&#10;new_df = model.sample(N_SAMPLES)&#10;``` ```python&#10;new_df.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;35.675009&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;96.720466&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;17.487054&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-1.952414&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;30.713002&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-1&lt;/td&gt; &lt;td&gt;-12.755545&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;17.839183&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;33.991250&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;27.495970&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;18.383220&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;new_df.describe()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;count&lt;/th&gt; &lt;td&gt;1000.000000&lt;/td&gt; &lt;td&gt;1000.000000&lt;/td&gt; &lt;td&gt;1000.000000&lt;/td&gt; &lt;td&gt;1000.000000&lt;/td&gt; &lt;td&gt;1000.00000&lt;/td&gt; &lt;td&gt;1000.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;mean&lt;/th&gt; &lt;td&gt;2.305000&lt;/td&gt; &lt;td&gt;0.684000&lt;/td&gt; &lt;td&gt;29.767560&lt;/td&gt; &lt;td&gt;0.540000&lt;/td&gt; &lt;td&gt;0.39300&lt;/td&gt; &lt;td&gt;32.009626&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;std&lt;/th&gt; &lt;td&gt;0.900436&lt;/td&gt; &lt;td&gt;0.546299&lt;/td&gt; &lt;td&gt;13.046890&lt;/td&gt; &lt;td&gt;1.146174&lt;/td&gt; &lt;td&gt;0.85632&lt;/td&gt; &lt;td&gt;51.155336&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;min&lt;/th&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;-1.000000&lt;/td&gt; &lt;td&gt;-6.750841&lt;/td&gt; &lt;td&gt;-3.000000&lt;/td&gt; &lt;td&gt;-3.00000&lt;/td&gt; &lt;td&gt;-121.529091&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;25%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;21.187223&lt;/td&gt; &lt;td&gt;0.000000&lt;/td&gt; &lt;td&gt;0.00000&lt;/td&gt; &lt;td&gt;-1.856691&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;50%&lt;/th&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;29.417820&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;0.00000&lt;/td&gt; &lt;td&gt;34.272920&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;75%&lt;/th&gt; &lt;td&gt;3.000000&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;38.370910&lt;/td&gt; &lt;td&gt;1.000000&lt;/td&gt; &lt;td&gt;1.00000&lt;/td&gt; &lt;td&gt;65.550551&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;max&lt;/th&gt; &lt;td&gt;5.000000&lt;/td&gt; &lt;td&gt;2.000000&lt;/td&gt; &lt;td&gt;75.153474&lt;/td&gt; &lt;td&gt;4.000000&lt;/td&gt; &lt;td&gt;2.00000&lt;/td&gt; &lt;td&gt;173.463617&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;from plotnine import *&#10;from plotnine.data import *&#10;from plotutils import * ggplot() + geom_histogram(data=data, mapping=aes(x=&apos;Pclass&apos;), fill=colours[0], bins=3, alpha=0.3) + \\&#10;geom_histogram(data=new_df, mapping=aes(x=&apos;Pclass&apos;), fill=colours[1], bins=3, alpha=0.3) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_1](./images/synthetic-data-with-sdv-and-gaussian-copulas_1.png)&#10;```python&#10;ggplot() + geom_histogram(data=data, mapping=aes(x=&apos;Sex&apos;), fill=colours[0], bins=6, alpha=0.3) + \\&#10;geom_histogram(data=new_df, mapping=aes(x=&apos;Sex&apos;), fill=colours[1], bins=6, alpha=0.3) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_2](./images/synthetic-data-with-sdv-and-gaussian-copulas_2.png)&#10;```python&#10;ggplot(mapping=aes(x=&apos;Age&apos;)) + \\&#10;geom_density(data=data, fill=colours[0], alpha=0.2) + \\&#10;geom_density(data=new_df, fill=colours[1], alpha=0.2) + \\&#10;geom_vline(xintercept=data.Age.mean(), linetype=&apos;dotted&apos;, colour=colours[0]) + \\&#10;geom_vline(xintercept=new_df.Age.mean(), linetype=&apos;dotted&apos;, colour=colours[1]) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_3](./images/synthetic-data-with-sdv-and-gaussian-copulas_3.png)&#10;```python&#10;ggplot(mapping=aes(x=&apos;Fare&apos;)) + \\&#10;geom_density(data=data, fill=colours[0], alpha=0.2) + \\&#10;geom_density(data=new_df, fill=colours[1], alpha=0.2) + \\&#10;geom_vline(xintercept=data.Fare.mean(), linetype=&apos;dotted&apos;, colour=colours[0]) + \\&#10;geom_vline(xintercept=new_df.Fare.mean(), linetype=&apos;dotted&apos;, colour=colours[1]) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_4](./images/synthetic-data-with-sdv-and-gaussian-copulas_4.png)&#10;```python&#10;model = GaussianCopula( field_transformers={ &apos;Pclass&apos;: &apos;categorical&apos;, &apos;Sex&apos;: &apos;categorical&apos;, &apos;Age&apos;: &apos;float&apos;, &apos;SibSp&apos;: &apos;boolean&apos;, &apos;Parch&apos;: &apos;integer&apos;, &apos;Fare&apos;: &apos;float&apos; }&#10;)&#10;``` ```python&#10;model.fit(data)&#10;``` ```python&#10;new_df = model.sample(N_SAMPLES)&#10;``` ```python&#10;new_df.head()&#10;``` &lt;div&gt;&#10;&lt;style scoped&gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }&#10;&lt;/style&gt;&#10;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Pclass&lt;/th&gt; &lt;th&gt;Sex&lt;/th&gt; &lt;th&gt;Age&lt;/th&gt; &lt;th&gt;SibSp&lt;/th&gt; &lt;th&gt;Parch&lt;/th&gt; &lt;th&gt;Fare&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;33.619&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;14.222&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;26.475&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;-1&lt;/td&gt; &lt;td&gt;76.520&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;55.414&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;-16.914&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;11.142&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;139.183&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;33.211&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;-34.416&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&#10;&lt;/table&gt;&#10;&lt;/div&gt;&#10;```python&#10;ggplot() + geom_histogram(data=data, mapping=aes(x=&apos;Pclass&apos;), fill=colours[0], bins=3, alpha=0.3) + \\&#10;geom_histogram(data=new_df, mapping=aes(x=&apos;Pclass&apos;), fill=colours[1], bins=3, alpha=0.3) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_5](./images/synthetic-data-with-sdv-and-gaussian-copulas_5.png)&#10;```python&#10;ggplot() + geom_histogram(data=data, mapping=aes(x=&apos;Sex&apos;), fill=colours[0], bins=6, alpha=0.3) + \\&#10;geom_histogram(data=new_df, mapping=aes(x=&apos;Sex&apos;), fill=colours[1], bins=6, alpha=0.3) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_6](./images/synthetic-data-with-sdv-and-gaussian-copulas_6.png)&#10;```python&#10;ggplot(mapping=aes(x=&apos;Age&apos;)) + \\&#10;geom_density(data=data, fill=colours[0], alpha=0.2) + \\&#10;geom_density(data=new_df, fill=colours[1], alpha=0.2) + \\&#10;geom_vline(xintercept=data.Age.mean(), linetype=&apos;dotted&apos;, colour=colours[0]) + \\&#10;geom_vline(xintercept=new_df.Age.mean(), linetype=&apos;dotted&apos;, colour=colours[1]) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_7](./images/synthetic-data-with-sdv-and-gaussian-copulas_7.png)&#10;```python&#10;ggplot(mapping=aes(x=&apos;Fare&apos;)) + \\&#10;geom_density(data=data, fill=colours[0], alpha=0.2) + \\&#10;geom_density(data=new_df, fill=colours[1], alpha=0.2) + \\&#10;geom_vline(xintercept=data.Fare.mean(), linetype=&apos;dotted&apos;, colour=colours[0]) + \\&#10;geom_vline(xintercept=new_df.Fare.mean(), linetype=&apos;dotted&apos;, colour=colours[1]) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_8](./images/synthetic-data-with-sdv-and-gaussian-copulas_8.png)&#10;```python&#10;data.Fare.describe()&#10;``` ```python&#10;distributions = model.get_distributions()&#10;``` ```python&#10;distributions&#10;``` ```python&#10;model = GaussianCopula( field_transformers={ &apos;Pclass&apos;: &apos;categorical&apos;, &apos;Sex&apos;: &apos;categorical&apos;, &apos;Age&apos;: &apos;float&apos;, &apos;SibSp&apos;: &apos;boolean&apos;, &apos;Parch&apos;: &apos;integer&apos;, &apos;Fare&apos;: &apos;float&apos; }, field_distributions={ &apos;Fare&apos;: &apos;truncated_gaussian&apos; }&#10;)&#10;``` ```python&#10;model.fit(data)&#10;``` ```python&#10;new_df = model.sample(N_SAMPLES)&#10;``` ```python&#10;new_df.Fare.describe()&#10;``` ```python&#10;ggplot(mapping=aes(x=&apos;Fare&apos;)) + \\&#10;geom_density(data=data, fill=colours[0], alpha=0.2) + \\&#10;geom_density(data=new_df, fill=colours[1], alpha=0.2) + \\&#10;geom_vline(xintercept=data.Fare.mean(), linetype=&apos;dotted&apos;, colour=colours[0]) + \\&#10;geom_vline(xintercept=new_df.Fare.mean(), linetype=&apos;dotted&apos;, colour=colours[1]) + \\&#10;theme_classic()&#10;``` ![synthetic-data-with-sdv-and-gaussian-copulas_9](./images/synthetic-data-with-sdv-and-gaussian-copulas_9.png)&#10;```python ```&#10;");
docs.push({"id":68,"title":"Synthetic data with SDV and Gaussian copulas","url":"/synthetic-data-with-sdv-and-gaussian-copulas.html"});
index.add(69, "# Vim keys ## Copy paste&#10;* Press `v` to select characters, or uppercase `V` to select whole lines&#10;* `y` to copy&#10;* Press `P` to paste before the cursor, or `p` to paste after");
docs.push({"id":69,"title":"Vim keys","url":"/vim-keys.html"});
index.add(70, "# Workflow");
docs.push({"id":70,"title":"Workflow","url":"/workflow.html"});
index.add(71, "---&#10;date: 2014-07-31T07:52:00+01:00&#10;draft: false&#10;url: &quot;/gulp.html&quot;&#10;--- # gulp I have been working in a new library called *gulp* which you can find on [https://github.com/ruivieira/gulp](https://github.com/ruivieira/gulp). On the project&apos;s page there are some usage examples but I will try to summarise the main points here. The purpose of this library is to facilitate the parallel development of R and Java code, using [rJava](http://cran.r-project.org/web/packages/rJava/index.html) as the bridge. Creating bindings in `rJava` is quite simple, the tricky part of the process (in my opinion) being the maintenance of the bindings (usually done by hand) when refactoring your code. As an example, let&apos;s assume you have the following Java class: ```java&#10;@ExportClassReference(value=&quot;test&quot;) public class Test { // Java code }&#10;``` That you wish to call from R.");
docs.push({"id":71,"title":"gulp","url":"/gulp.html"});
index.add(72, "---&#10;title: Main page of ruivieira.dev&#10;date: 2019-04-17T19:21:00+01:00&#10;draft: false&#10;url: &quot;/introduction-to-balanced-box-decomposition-trees.html&quot;&#10;---&#10;# ruivieira.dev ```text&#10;-----BEGIN GEEK CODE BLOCK-----&#10;Version: 3.1&#10;GCS/M/MU d-- s:+&gt; !a C++$&gt;+++ ULC P+&gt;++ L+++&gt;++++ E-@ W++ !N&gt;+ !o K--? !w O&gt;+ M+ !V PS++@ PE-@ !Y PGP@&gt;++ t+ !5 X++ R@&gt;+ tv+&gt; b++&gt;+++ DI&gt;+ D++&gt;+++ G++&gt;+++ e++++ h---- r+++ z&#10;------END GEEK CODE BLOCK------&#10;``` ## Welcome Welcome to my site, **ruivieira.dev**. Formerly, this site used to be a blog. Currently, it is adhering to the principles of a [Digital Garden](digital-garden.html) and following the guidelines of [Brutalist Web Design](brutalist-web-design.html). It is intended to be a collection of notes, a learning journal and a reference -- all growing organically. Parts of this site and other additional content (including a `gemlog`) are available at [ruivieira.srht.site](gemini://ruivieira.srht.site/) (using [Gemini](gemini.html)). Technical details about this site are available on the [site details](site-details.html) page. You can still find most of my old &quot;blog posts&quot; here: * [Serving models with Seldon](serving-models-with-seldon.html)&#10;* [(Semi) handcrafted RSS](semi-handcrafted-rss.html)&#10;* [Introduction to Balanced Box-Decomposition Trees](introduction-to-balanced-box-decomposition-trees.html)&#10;* [Monotonic Cubic Spline interpolation (with some Rust)](monotonic-cubic-spline-interpolation-with-some-rust.html)&#10;* [Python monkey patching (for readability)](python-monkey-patching-for-readability.html)&#10;* [Introduction to Isolation Forests](introduction-to-isolation-forests.html)&#10;* [MCMC performance on Substrate VM](mcmc-performance-on-substrate-vm.html)&#10;* [Containerised Streaming Data Generation using State-Space Models](containerised-streaming-data-generation-using-state-space-models.html)&#10;* [A simple Python benchmark exercise](a-simple-python-benchmark-exercise.html)&#10;* [A streaming ALS implementation](a-streaming-als-implementation.html)&#10;* [Bayesian estimation of changepoints](bayesian-estimation-of-changepoints.html)&#10;* [t as mixture of Normals](t-as-mixture-of-normals.html)&#10;* [Langton&apos;s Ant](langtons-ant.html)&#10;* [A Gibbs Sampler in Crystal](a-gibbs-sampler-in-crystal.html)&#10;* [MCMC notifications](mcmc-notifications.html)&#10;* [gulp](gulp.html) Some topics on [Machine Learning](machine-learning.html): * A section on [Explainability](explainability.html), with a focus on * [Counterfactuals](counterfactuals.html)&#10;* [Synthetic data generation](synthetic-data-generation.html) * Using [Gaussian copulas](synthetic-data-with-sdv-and-gaussian-copulas.html), [CTGAN](synthetic-data-with-sdv-and-ctgan.html) and [CopulaGAN](synthetic-data-with-sdv-and-copulagan.html)&#10;* Optimisation methods, including [Hill-climbing optimisation](hill-climbing-optimisation.html) There are some areas about programming languages I frequently use, in no particular order: * [Go](go.html)&#10;* [Python](python.html)&#10;* [Java](java.html)&#10;* [Clojure](clojure.html)&#10;* [Rust](rust.html)&#10;* [Deno](deno.html) And other frameworks/tools: * [Ansible](ansible.html)&#10;* [Emacs](emacs.html) ## About me I&apos;m a software engineer at [Red Hat](https://www.redhat.com/en/about)[^redhat] working on distributed applications, process automation and machine learning. I have a PhD in Bayesian Statistics (specifically in Sequential Monte-Carlo methods for long-running streaming data) from the [School of Mathematics, Statistics and Physics](https://www.ncl.ac.uk/maths-physics/) at Newcastle University, UK. [^redhat]: All opinions on this site are my own, not Red Hat&apos;s. ## Recently changed * [Hill-climbing optimisation](hill-climbing-optimisation.html)&#10;* [Dunn index](dunn-index.html)&#10;* [Optimising random forest hyperparamaters](optimising-random-forest-hyperparamaters.html)&#10;* [SSH](ssh.html)&#10;* [GPG](gpg.html)&#10;* [Python grammar of graphics](python-grammar-of-graphics.html)&#10;* [Counterfactuals with Constraint Solvers](counterfactuals-with-constraint-solvers.html)&#10;* [Counterfactual Fairness](counterfactual-fairness.html)&#10;* [Distance metrics](distance-metrics.html)&#10;* [Portuguese Christmas recipes](portuguese-christmas-recipes.html)&#10;* [Java consumer](java-consumer.html)&#10;");
docs.push({"id":72,"title":"index","url":"/index.html"});
index.add(73, "This is an example of a *notebook* in **jupytext**&#10;```python&#10;stack = &quot;jupytext&quot;&#10;print(f&quot;hello, {stack}!&quot;)&#10;```&#10;");
docs.push({"id":73,"title":"jupytext","url":"/jupytext.html"});
index.add(74, "---&#10;title: &quot;t as mixture of Normals&quot;&#10;date: 2016-11-27T14:00:00+00:00&#10;draft: false&#10;url: &quot;/t-as-mixture-of-normals.html&quot;&#10;Css: - &quot;/assets/katex.min.css&quot;&#10;Js: - /assets/katex.min.js - /assets/auto-render.min.js&#10;--- # t as mixture of Normals (Based on Rasmus B&aring;&aring;th&apos;s [post](http://www.sumsar.net/blog/2013/12/t-as-a-mixture-of-normals/)) A scaled $t$ distribution, with $\\mu$ mean, $s$ scale and $\\nu$ degrees of freedom, can be simulated from a mixture of Normals with $\\\\mu$ mean and precisions following a Gamma distribution: $$&#10;\\begin{aligned}&#10;y &amp;\\sim \\mathcal{N}\\left(\\mu,\\sigma\\right) \\\\\\\\ \\sigma^2 &amp;\\sim \\mathcal{IG}\\left(\\frac{\\nu}{2},s^2\\frac{\\nu}{2}\\right) \\end{aligned}&#10;$$ Since I&apos;ve recently pickep up again the [crystal-gsl](https://github.com/ruivieira/crystal-gsl) in my spare time, I&apos;ve decided to replicate the previously mentioned post using a Crystal one-liner. To simulate 10,000 samples from $t_2\\left(0,3\\right)$ using the mixture, we can then write: ```crystal&#10;samples = (0..10000).map { |x| Normal.sample 0.0, 1.0/Math.sqrt( Gamma.sample 1.0, 9.0 ) }&#10;``` We can see the mixture distribution (histogram) converging nicely to the $(t_2(0,3)$ (red): &lt;video controls loop autoplay width=&quot;100%&quot;&gt;&lt;source src=&apos;./images/t_mixture.mp4&apos; type=&apos;video/mp4&apos;&gt;&lt;/video&gt;");
docs.push({"id":74,"title":"t as mixture of Normals","url":"/t-as-mixture-of-normals.html"});
    const query = new URLSearchParams(document.location.search).get("q");

    const input = document.getElementById('search_terms');
    const results = document.getElementById('results');
    const button = document.getElementById('search_button');

    input.addEventListener("keyup", function (event) {
        // Number 13 is the "Enter" key on the keyboard
        if (event.keyCode === 13) {
            // Cancel the default action, if needed
            event.preventDefault();
            // Trigger the button element with a click
            search_button.click();
        }
    });

    let create_link = function (term, title, url) {
        let a = document.createElement('a');
        let linkText = document.createTextNode(title);
        a.appendChild(linkText);
        a.title = title;
        const query = new URLSearchParams({"h": term});
        a.href = url + "?" + query.toString();
    return a
    }

    let search = function () {

    results.innerHTML = "";
    let matches = index.search(input.value);
    console.log(matches);
    let header = document.createElement("h2");
        header.appendChild(document.createTextNode(`Found ${matches.length} matches:`));
        results.appendChild(header);
        for (let match of matches) {
            let doc = docs[match - 1];
            let div = document.createElement("div");
            let a = create_link(input.value, doc.title, doc.url);
            div.appendChild(a);
            results.appendChild(div);
        }
    }

    if (query != null && query !== "") {
        input.value = query;
        search();
    }

</script>
</body>
</html>