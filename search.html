<!DOCTYPE html>
<head>
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/favicons/favicon.ico">

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@master/dist/flexsearch.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Nunito:400,300i,800&display=swap" rel="stylesheet"/>
    <link href="/assets/style.css" rel="stylesheet"/>
    <link href="/assets/code.css" rel="stylesheet"/>
    <title>ruivieira.dev - Search</title>
    <script type="application/javascript">
        var doNotTrack = false;
        if (!doNotTrack) {
            (function (i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function () {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
            ga('create', 'UA-10507665-2', 'auto');

            ga('send', 'pageview');
        }
    </script>
    <style>
        #search_terms {
            width: 75%;
            font-size: 2rem;
            font-family: Nunito;
            margin-top: 7rem;
        }
        #search_button {
            background-color: #ddd;
            border: none;
            color: black;
            padding: 0.5rem 1rem;
            font-size: 2rem;
            font-family: Nunito;
            text-decoration: none;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 10px;
            width: 20%;
        }
        #results {
            margin-top: 2rem;
        }
    </style>
</head>
<body>

<div id="content">

    <input id="search_terms" type="search"/>
    <button id="search_button" onclick="search()">Search</button>

    <div id="results">

    </div>

    <div class="footer">
        <span class="cc-symbol">&#127341;</span> 2020 CC BY Rui Vieira
    </div>
</div>

<div id="sidebar">
    <p><a href="/">Home</a></p>
    <p><a href="/content.html">All pages</a></p>

    <div class="footer">
        modified .
    </div>

</div>
    <script>
        var index = new FlexSearch({encode: "icase", tokenize: "forward", async: false});
let docs = [];
index.add(1, "Counterfactual fairness Building counterfactually fair models Data To evaluate counterfactual fairness we will be using the \"law school\" dataset1. The Law School Admission Council conducted a survey across 163 law schools in the United States. It contains information on 21,790 law students such as their entrance exam scores (LSAT), their grade-point average (GPA) collected prior to law school, and their first year average grade (FYA). Given this data, a school may wish to predict if an applicant will have a high FYA. The school would also like to make sure these predictions are not biased by an individual\u2019s race and sex. However, the LSAT, GPA, and FYA scores, may be biased due to social factors. We start by importing the data into a Pandas DataFrame. import warnings warnings.filterwarnings(\"ignore\") import pandas as pd df = pd.read_csv(\"data/law_data.csv\", index_col=0) df.head() race sex LSAT UGPA region_first ZFYA sander_index first_pf 0 White 1 39.0 3.1 GL -0.98 0.782738 1.0 1 White 1 36.0 3.0 GL 0.09 0.735714 1.0 2 White 2 30.0 3.1 MS -0.35 0.670238 1.0 5 Hispanic 2 39.0 2.2 NE 0.58 0.697024 1.0 6 White 1 37.0 3.4 GL -1.26 0.786310 1.0 Pre-processing We now pre-process the data. We start by creating categorical \"dummy\" variables according to the race variable. df = pd.get_dummies(df, columns=[\"race\"], prefix=\"\", prefix_sep=\"\") df.iloc[:, : 7].head() sex LSAT UGPA region_first ZFYA sander_index first_pf 0 1 39.0 3.1 GL -0.98 0.782738 1.0 1 1 36.0 3.0 GL 0.09 0.735714 1.0 2 2 30.0 3.1 MS -0.35 0.670238 1.0 5 2 39.0 2.2 NE 0.58 0.697024 1.0 6 1 37.0 3.4 GL -1.26 0.786310 1.0 df.iloc[:, 7 :].head() Amerindian Asian Black Hispanic Mexican Other Puertorican White 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 1 5 0 0 0 1 0 0 0 0 6 0 0 0 0 0 0 0 1 We also want to expand the sex variable into male/female categorical variables and remove the original. df[\"male\"] = df[\"sex\"].map(lambda x: 1 if x == 2 else 0) df[\"female\"] = df[\"sex\"].map(lambda x: 1 if x == 1 else 0) df = df.drop(axis=1, columns=[\"sex\"]) df.iloc[:, 0:7].head() LSAT UGPA region_first ZFYA sander_index first_pf Amerindian 0 39.0 3.1 GL -0.98 0.782738 1.0 0 1 36.0 3.0 GL 0.09 0.735714 1.0 0 2 30.0 3.1 MS -0.35 0.670238 1.0 0 5 39.0 2.2 NE 0.58 0.697024 1.0 0 6 37.0 3.4 GL -1.26 0.786310 1.0 0 df.iloc[:, 7:].head() Asian Black Hispanic Mexican Other Puertorican White male female 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 2 0 0 0 0 0 0 1 1 0 5 0 0 1 0 0 0 0 1 0 6 0 0 0 0 0 0 1 0 1 We will also convert the entrance exam scores (LSAT) to a discrete variable. df[\"LSAT\"] = df[\"LSAT\"].astype(int) df.iloc[:, :6].head() LSAT UGPA region_first ZFYA sander_index first_pf 0 39 3.1 GL -0.98 0.782738 1.0 1 36 3.0 GL 0.09 0.735714 1.0 2 30 3.1 MS -0.35 0.670238 1.0 5 39 2.2 NE 0.58 0.697024 1.0 6 37 3.4 GL -1.26 0.786310 1.0 df.iloc[:, 6:].head() Amerindian Asian Black Hispanic Mexican Other Puertorican White male female 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 2 0 0 0 0 0 0 0 1 1 0 5 0 0 0 1 0 0 0 0 1 0 6 0 0 0 0 0 0 0 1 0 1 Protected attributes Counterfactual fairness enforces that a distribution over possible predictions for an individual should remain unchanged in a world where an individual\u2019s protected attributes \\(A\\) had been different in a causal sense. Let's start by defining the protected attributes. Obvious candidates are the different categorical variables for ethnicity (Asian, White, Black, etc) and gender (male, female). A = [ \"Amerindian\", \"Asian\", \"Black\", \"Hispanic\", \"Mexican\", \"Other\", \"Puertorican\", \"White\", \"male\", \"female\", ] Training and testing subsets We will now divide the dataset into training and testing subsets. We will use the same ratio as in 2, that is 20%. from sklearn.model_selection import train_test_split df_train, df_test = train_test_split(df, random_state=23, test_size=0.2); Models Unfair model As detailed in 2, the concept of counterfactual fairness holds under three levels of assumptions of increasing strength. The first of such levels is Level 1, where \\(\\hat{Y}\\) is built using only the observable non-descendants of \\(A\\). This only requires partial causal ordering and no further causal assumptions, but in many problems there will be few, if any, observables which are not descendants of protected demographic factors. For this dataset, since LSAT, GPA, and FYA are all biased by ethnicity and gender, we cannot use any observed features to construct a Level 1 counterfactually fair predictor as described in Level 1. Instead (and in order to compare the performance with Level 2 and 3 models) we will build two unfair baselines. A Full model, which will be trained with the totality of the variables An Unaware model (FTU), which will be trained will all the variables, except the protected attributes \\(A\\). Let's proceed with calculating the Full model. Full model As mentioned previously, the full model will be a simple linear regression in order to predict ZFYA using all of the variables. from sklearn.linear_model import LinearRegression linreg_unfair = LinearRegression() The inputs will then be the totality of the variabes (protected variables \\(A\\), as well as UGPA and LSAT). import numpy as np X = np.hstack( ( df_train[A], np.array(df_train[\"UGPA\"]).reshape(-1, 1), np.array(df_train[\"LSAT\"]).reshape(-1, 1), ) ) print(X) [[ 0. 0. 0. ... 1. 3.1 39. ] [ 0. 0. 0. ... 1. 3.5 36. ] [ 0. 0. 0. ... 1. 3.9 46. ] ... [ 0. 0. 0. ... 1. 2.9 33. ] [ 0. 0. 0. ... 0. 2.9 31. ] [ 0. 0. 0. ... 0. 3.6 39. ]] As for our target, we are trying to predict ZFYA (first year average grade). y = df_train[\"ZFYA\"] y[:10] 10454 0.56 14108 0.60 20624 -0.14 8316 0.20 14250 0.02 18909 -1.47 8949 1.36 1658 0.39 23340 0.10 26884 0.48 Name: ZFYA, dtype: float64 We fit the model: linreg_unfair = linreg_unfair.fit(X, y) And perform some predictions on the test subset. X_test = np.hstack( ( df_test[A], np.array(df_test[\"UGPA\"]).reshape(-1, 1), np.array(df_test[\"LSAT\"]).reshape(-1, 1), ) ) X_test array([[ 0. , 0. , 0. , ..., 0. , 3.4, 32. ], [ 0. , 0. , 0. , ..., 1. , 3.5, 41. ], [ 0. , 0. , 0. , ..., 1. , 3.9, 42. ], ..., [ 0. , 0. , 0. , ..., 0. , 2.3, 28. ], [ 0. , 0. , 0. , ..., 0. , 3.3, 36. ], [ 0. , 0. , 0. , ..., 0. , 2.9, 37. ]]) predictions_unfair = linreg_unfair.predict(X_test) predictions_unfair array([ 0.08676147, 0.34942627, 0.4609375 , ..., -0.25949097, 0.19308472, 0.14471436]) We will also calculate the unfair model score for future use. score_unfair = linreg_unfair.score(X_test, df_test[\"ZFYA\"]) print(score_unfair) 0.12701634112845117 from sklearn.metrics import mean_squared_error RMSE_unfair = np.sqrt(mean_squared_error(df_test[\"ZFYA\"], predictions_unfair)) print(RMSE_unfair) 0.8666709890234552 Fairness through unawareness (FTU) As also mentioned in 2, the second baseline we will use is an Unaware model (FTU), which will be trained will all the variables, except the protected attributes \\(A\\). linreg_ftu = LinearRegression() We will create the inputs as previously, but without using the protected attributes, \\(A\\). X_ftu = np.hstack( ( np.array(df_train[\"UGPA\"]).reshape(-1, 1), np.array(df_train[\"LSAT\"]).reshape(-1, 1), ) ) X_ftu array([[ 3.1, 39. ], [ 3.5, 36. ], [ 3.9, 46. ], ..., [ 2.9, 33. ], [ 2.9, 31. ], [ 3.6, 39. ]]) And we fit the model: linreg_ftu = linreg_ftu.fit(X_ftu, y) Again, let's perform some predictions on the test subset. X_ftu_test = np.hstack( (np.array(df_test[\"UGPA\"]).reshape(-1, 1), np.array(df_test[\"LSAT\"]).reshape(-1, 1)) ) X_ftu_test array([[ 3.4, 32. ], [ 3.5, 41. ], [ 3.9, 42. ], ..., [ 2.3, 28. ], [ 3.3, 36. ], [ 2.9, 37. ]]) predictions_ftu = linreg_ftu.predict(X_ftu_test) predictions_ftu array([-0.06909331, 0.35516229, 0.50304555, ..., -0.53109868, 0.08204563, 0.0226846 ]) As previously, let's calculate this model's score. ftu_score = linreg_ftu.score(X_ftu_test, df_test[\"ZFYA\"]) print(ftu_score) 0.0917442226187073 RMSE_ftu = np.sqrt(mean_squared_error(df_test[\"ZFYA\"], predictions_ftu)) print(RMSE_ftu) 0.8840061503773576 Latent variable model Still according to 2, a Level 2 approach will model latent \u2018fair\u2019 variables which are parents of observed variables. If we consider a predictor parameterised by \\(\\theta\\), such as: \\[ \\hat{Y} \\equiv g_\\theta (U, X_{\\nsucc A}) \\] with \\(X_{\\nsucc A} \\subseteq X\\) are non-descendants of \\(A\\). Assuming a loss function \\(l(\\cdot,\\cdot)\\) and training data \\(\\mathcal{D}\\equiv\\{(A^{(i), X^{(i)}, Y^{(i)}})\\}\\), for \\(i=1,2\\dots,n\\), the empirical loss is defined as \\[ L(\\theta)\\equiv \\sum_{i=1}^n \\mathbb{E}[l(y^{(i)},g_\\theta(U^{(i)}, x^{(i)}_{\\nsucc A}))]/n \\] which has to be minimised in order to \\(\\theta\\). Each \\(n\\) expectation is with respect to random variable \\(U^{(i)}\\) such that \\[ U^{(i)}\\sim P_{\\mathcal{M}}(U|x^{(i)}, a^{(i)}) \\] where \\(P_{\\mathcal{M}}(U|x,a)\\) is the conditional distribution of the background variables as given by a causal model M that is available by assumption. If this expectation cannot be calculated analytically, Markov chain Monte Carlo (MCMC) can be used to approximate it as in the following algorithm. We will follow the model specified in the original paper, where the latent variable considered is \\(K\\), which represents a student's knowledge. \\(K\\) will affect GPA, LSAT and the outcome, FYA. The model can be defined by: \\[ \\begin{aligned} GPA &\\sim \\mathcal{N}(GPA_0 + w_{GPA}^KK + w_{GPA}^RR + w_{GPA}^SS, \\sigma_{GPA}) \\\\ LSAT &\\sim \\text{Po}(\\exp(LSAT_0 + w_{LSAT}^KK + w_{LSAT}^RR + w_L^SS)) \\\\ FYA &\\sim \\mathcal{N}(w_{FYA}^KK + w_{FYA}^RR + w_{FYA}^SS, 1) \\\\ K &\\sim \\mathcal{N}(0,1) \\end{aligned} \\] The priors used will be: \\[ \\begin{aligned} GPA_0 &\\sim \\mathcal{N}(0, 1) \\\\ LSAT_0 &\\sim \\mathcal{N}(0, 1) \\\\ GPA_0 &\\sim \\mathcal{N}(0, 1) \\end{aligned} \\] import pymc3 as pm K = len(A) def MCMC(data, samples=1000): N = len(data) a = np.array(data[A]) model = pm.Model() with model: # Priors k = pm.Normal(\"k\", mu=0, sigma=1, shape=(1, N)) gpa0 = pm.Normal(\"gpa0\", mu=0, sigma=1) lsat0 = pm.Normal(\"lsat0\", mu=0, sigma=1) w_k_gpa = pm.Normal(\"w_k_gpa\", mu=0, sigma=1) w_k_lsat = pm.Normal(\"w_k_lsat\", mu=0, sigma=1) w_k_zfya = pm.Normal(\"w_k_zfya\", mu=0, sigma=1) w_a_gpa = pm.Normal(\"w_a_gpa\", mu=np.zeros(K), sigma=np.ones(K), shape=K) w_a_lsat = pm.Normal(\"w_a_lsat\", mu=np.zeros(K), sigma=np.ones(K), shape=K) w_a_zfya = pm.Normal(\"w_a_zfya\", mu=np.zeros(K), sigma=np.ones(K), shape=K) sigma_gpa_2 = pm.InverseGamma(\"sigma_gpa_2\", alpha=1, beta=1) mu = gpa0 + (w_k_gpa * k) + pm.math.dot(a, w_a_gpa) # Observed data gpa = pm.Normal( \"gpa\", mu=mu, sigma=pm.math.sqrt(sigma_gpa_2), observed=list(data[\"UGPA\"]), shape=(1, N), ) lsat = pm.Poisson( \"lsat\", pm.math.exp(lsat0 + w_k_lsat * k + pm.math.dot(a, w_a_lsat)), observed=list(data[\"LSAT\"]), shape=(1, N), ) zfya = pm.Normal( \"zfya\", mu=w_k_zfya * k + pm.math.dot(a, w_a_zfya), sigma=1, observed=list(data[\"ZFYA\"]), shape=(1, N), ) step = pm.Metropolis() trace = pm.sample(samples, step) return trace train_estimates = MCMC(df_train) Multiprocess sampling (4 chains in 4 jobs) CompoundStep >Metropolis: [sigma_gpa_2] >Metropolis: [w_a_zfya] >Metropolis: [w_a_lsat] >Metropolis: [w_a_gpa] >Metropolis: [w_k_zfya] >Metropolis: [w_k_lsat] >Metropolis: [w_k_gpa] >Metropolis: [lsat0] >Metropolis: [gpa0] >Metropolis: [k] 100.00% [8000/8000 01:16<00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 87 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Let's plot a single trace for \\(k^{(i)}\\). import matplotlib.pyplot as plt import seaborn as sns from plotutils import * # Thin the samples before plotting k_trace = train_estimates[\"k\"][:, 0].reshape(-1, 1)[0::100] plt.subplot(1, 2, 1) plt.hist(k_trace, color=colours[0], bins=100) plt.subplot(1, 2, 2) plt.scatter(range(len(k_trace)), k_trace, s=1, c=colours[0]) plt.show() train_k = np.mean(train_estimates[\"k\"], axis=0).reshape(-1, 1) train_k array([[ 0.01917085], [-0.00632169], [-0.16469706], ..., [ 0.08588421], [ 0.0144014 ], [ 0.03338959]]) We can now estimate \\(k\\) using the test data: test_map_estimates = MCMC(df_test) Multiprocess sampling (4 chains in 4 jobs) CompoundStep >Metropolis: [sigma_gpa_2] >Metropolis: [w_a_zfya] >Metropolis: [w_a_lsat] >Metropolis: [w_a_gpa] >Metropolis: [w_k_zfya] >Metropolis: [w_k_lsat] >Metropolis: [w_k_gpa] >Metropolis: [lsat0] >Metropolis: [gpa0] >Metropolis: [k] 100.00% [8000/8000 00:23<00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 35 seconds. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. test_k = np.mean(test_map_estimates[\"k\"], axis=0).reshape(-1, 1) test_k array([[ 0.04363701], [-0.09365109], [-0.45671879], ..., [-0.03137148], [ 0.28931439], [ 0.07281012]]) We now build the Level 2 predictor, using \\(k\\) as the input. linreg_latent = LinearRegression() linreg_latent = linreg_latent.fit(train_k, df_train[\"ZFYA\"]) predictions_latent = linreg_latent.predict(test_k) predictions_latent array([ 0.0711182 , 0.14217813, 0.33010093, ..., 0.10994238, -0.05604371, 0.05601828]) latent_score = linreg_latent.score(test_k, df_test[\"ZFYA\"]) print(latent_score) 0.008509520014148064 RMSE_latent = np.sqrt(mean_squared_error(df_test[\"ZFYA\"], predictions_latent)) print(RMSE_latent) 0.9236245677858551 Additive error model Finally, in Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex3. This corresponds to \\[ \\begin{aligned} GPA &= b_G + w^R_{GPA}R + w^S_{GPA}S + \\epsilon_{GPA}, \\epsilon_{GPA} \\sim p(\\epsilon_{GPA}) \\\\ LSAT &= b_L + w^R_{LSAT}R + w^S_{LSAT}S + \\epsilon_{LSAT}, \\epsilon_{LSAT} \\sim p(\\epsilon_{LSAT}) \\\\ FYA &= b_{FYA} + w^R_{FYA}R + w^S_{FYA}S + \\epsilon_{FYA} , \\epsilon_{FYA} \\sim p(\\epsilon_{FYA}) \\end{aligned} \\] We estimate the error terms \\(\\epsilon_{GPA}, \\epsilon_{LSAT}\\) by first fitting two models that each use race and sex to individually predict GPA and LSAT. We then compute the residuals of each model (e.g., \\(\\epsilon_{GPA} =GPA\u2212\\hat{Y}_{GPA}(R, S)\\)). We use these residual estimates of \\(\\epsilon_{GPA}, \\epsilon_{LSAT}\\) to predict \\(FYA\\). In 2 this is called Fair Add. Since the process is similar for the individual predictions for GPA and LSAT, we will write a method to avoid repetion. def calculate_epsilon(data, var_name, protected_attr): X = data[protected_attr] y = data[var_name] linreg = LinearRegression() linreg = linreg.fit(X, y) predictions = linreg.predict(X) return data[var_name] - predictions Let's apply it to each variable, individually. First we calculate \\(\\epsilon_{GPA}\\): epsilons_gpa = calculate_epsilon(df, \"UGPA\", A) epsilons_gpa 0 -0.242 1 -0.342 2 -0.100 5 -0.873 6 0.058 ... 27472 0.800 27473 0.358 27474 0.658 27475 -0.300 27476 -0.100 Name: UGPA, Length: 21791, dtype: float64 Next, we calculate \\(\\epsilon_{LSAT}\\): epsilons_LSAT = calculate_epsilon(df, \"LSAT\", A) epsilons_LSAT 0 1.789 1 -1.211 2 -7.689 5 5.055 6 -0.211 ... 27472 -4.689 27473 0.789 27474 -1.211 27475 -6.689 27476 -9.689 Name: LSAT, Length: 21791, dtype: float64 Let's visualise the \\(\\epsilon\\) distribution quickly: import matplotlib.pyplot as plt import seaborn as sns plt.subplot(1, 2, 1) plt.hist(epsilons_gpa, color=colours[0], bins=100) plt.title(\"$\\epsilon_{GPA}$\") plt.xlabel(\"$\\epsilon_{GPA}$\") plt.subplot(1, 2, 2) plt.hist(epsilons_LSAT, color=colours[1], bins=100) plt.title(\"$\\epsilon_{LSAT}$\") plt.xlabel(\"$\\epsilon_{LSAT}$\") plt.show() We finally use the calculated \\(\\epsilon\\) to train a model in order to predict FYA. We start by getting the subset of the \\(\\epsilon\\) which match the training indices. X = np.hstack( ( np.array(epsilons_gpa[df_train.index]).reshape(-1, 1), np.array(epsilons_LSAT[df_train.index]).reshape(-1, 1), ) ) X array([[-0.24179687, 1.7890625 ], [ 0.15820312, -1.2109375 ], [ 0.55820312, 8.7890625 ], ..., [-0.44179688, -4.2109375 ], [-0.25087891, -4.7265625 ], [ 0.39980469, 1.31054688]]) linreg_fair_add = LinearRegression() linreg_fair_add = linreg_fair_add.fit( X, df_train[\"ZFYA\"], ) We now use this model to calculate the predictions X_test = np.hstack( ( np.array(epsilons_gpa[df_test.index]).reshape(-1, 1), np.array(epsilons_LSAT[df_test.index]).reshape(-1, 1), ) ) predictions_fair_add = linreg_fair_add.predict(X_test) predictions_fair_add array([-0.04394693, 0.24454891, 0.35558793, ..., -0.38844376, 0.06136776, 0.01295201]) And as previously, we calculate the model's score: fair_add_score = linreg_fair_add.score(X_test, df_test[\"ZFYA\"]) print(fair_add_score) 0.04475841449183948 RMSE_fair_add = np.sqrt(mean_squared_error(df_test[\"ZFYA\"], predictions_fair_add)) print(RMSE_fair_add) 0.9065835039365202 Comparison The scores, so far, are: print(f\"Unfair score:\\t{score_unfair}\") print(f\"FTU score:\\t{ftu_score}\") print(f\"L2 score:\\t{latent_score}\") print(f\"Fair add score:\\t{fair_add_score}\") Unfair score: 0.12701634112845117 FTU score: 0.0917442226187073 L2 score: 0.008509520014148064 Fair add score: 0.04475841449183948 print(f\"Unfair RMSE:\\t{RMSE_unfair}\") print(f\"FTU RMSE:\\t{RMSE_ftu}\") print(f\"L2 RMSE:\\t{RMSE_latent}\") print(f\"Fair add RMSE:\\t{RMSE_fair_add}\") Unfair RMSE: 0.8666709890234552 FTU RMSE: 0.8840061503773576 L2 RMSE: 0.9236245677858551 Fair add RMSE: 0.9065835039365202 Measuring counterfactual fairness First, we will measure two quantities, the Statistical Parity Difference (SPD)4 and Disparate impact (DI)5. Statistical Parity Difference / Disparate Impact from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio parities = [] impacts = [] for a in A: parity = demographic_parity_difference(df_train[\"ZFYA\"], df_train[\"ZFYA\"], sensitive_features = df_train[a]) di = demographic_parity_ratio(df_train[\"ZFYA\"], df_train[\"ZFYA\"], sensitive_features = df_train[a]) parities.append(parity) impacts.append(di) df_parities = pd.DataFrame({'protected':A,'parity':parities,'impact':impacts}) import matplotlib.pyplot as plt from plotutils import * fig = plt.figure() ax = fig.add_subplot(111) ax2 = ax.twinx() fig.suptitle('Statistical Parity Difference and Disparate Impact') width = 0.4 df_parities.plot(x ='protected', y = 'parity', kind = 'bar', ax = ax, width = width, position=1, color=colours[0], legend=False) df_parities.plot(x ='protected', y = 'impact', kind = 'bar', ax = ax2, width = width, position = 0, color = colours[1], legend = False) ax.axhline(y = 0.1, linestyle = 'dashed', alpha = 0.7, color = colours[0]) ax2.axhline(y = 0.55, linestyle = 'dashed', alpha = 0.7, color = colours[1]) patches, labels = ax.get_legend_handles_labels() ax.legend(patches, ['Stat Parity Diff'], loc = 'upper left') patches, labels = ax2.get_legend_handles_labels() ax2.legend(patches, ['Disparate Impact'], loc = 'upper right') labels = [item.get_text() for item in ax.get_xticklabels()] for i in range(len(A)): labels[i] = A[i] ax.set_xticklabels(labels) ax.set_xlabel('Protected Features') ax.set_ylabel('Statistical Parity Difference') ax2.set_ylabel('Disparate Impact') plt.show() Finding sensitive features Typically a \\(SPD > 0.1\\) and a \\(DI < 0.9\\) might indicate discrimination on those features. All protected attributes fail the SPD test and, in our dataset, we have two features (Hispanic and Mexican) which clearly fail the DI test. for a in [\"Mexican\", \"Hispanic\"]: spd = demographic_parity_difference(y_true=df_train[\"ZFYA\"], y_pred=df_train[\"ZFYA\"], sensitive_features = df_train[a]) print(f\"SPD({a}) = {spd}\") di = demographic_parity_ratio(y_true=df_train[\"ZFYA\"], y_pred=df_train[\"ZFYA\"], sensitive_features = df_train[a]) print(f\"DI({a}) = {di}\") SPD(Mexican) = 0.0014017257538768636 DI(Mexican) = 0.5556529360210342 SPD(Hispanic) = 0.003272247102713093 DI(Hispanic) = 0.34227833235466826 McIntyre, Frank, and Michael Simkovic. \"Are law degrees as valuable to minorities?.\" International Review of Law and Economics 53 (2018): 23-37. \u21a9 Kusner, Matt J., Joshua Loftus, Chris Russell, and Ricardo Silva. \"Counterfactual fairness.\" In Advances in neural information processing systems, pp. 4066-4076. 2017. \u21a9\u21a9\u21a9\u21a9\u21a9 That may in turn be correlated with one-another. \u21a9 See {ref}fairness:demographic-parity-difference. \u21a9 See {ref}fairness:disparate-impact. \u21a9 ");
docs.push({"id": 1, "title": "Counterfactual Fairness", "url": "/counterfactual-fairness.html"});
index.add(2, "Bayesian estimation of changepoints A common introductory problem in Bayesian changepoint detection is the record of UK coal mining disasters from 1851 to 1962. More information can be found in Carlin, Gelfand and Smith (1992). As we can see from the plot below, the number of yearly disasters ranges from 0 to 6 and we will assume that at some point within this time range a change in the accident rate has occured. The number of yearly disasters can be modelled as a Poisson with a unknown rate depending on the changepoint \\(k\\): \\[ y_t \\sim \\text{Po}\\left(\\rho\\right),\\qquad \\rho = \\begin{cases} \\mu, & \\text{if}\\ t=1,2,\\dots,k \\\\\\\\ \\lambda, & \\text{if}\\ t = k +1, k + 2, \\dots,m \\end{cases} \\] Our objective is to estimate in which year the change occurs (the changepoint \\(k\\)) and the accident rate before (\\(\\mu\\)) and after (\\(\\lambda\\)) the changepoint amounting to the parameter set \\(\\Phi = \\left\\lbrace\\mu,\\lambda,k\\right\\rbrace\\). We will use Crystal (with crystal-gsl) to perform the estimation. We start by placing independent priors on the parameters: \\(k \\sim \\mathcal{U}\\left(0, m\\right)\\) \\(\\mu \\sim \\mathcal{G}\\left(a_1, b_1\\right)\\) \\(\\lambda \\sim \\mathcal{G}\\left(a_2, b_2\\right)\\) For the remainder we'll set \\(a_1=a_2=0.5\\), \\(c_1=c_2=0\\) and \\(d_1=d_2=1\\). The joint posterior of \\(\\Phi\\) is then: \\[ \\pi\\left(\\Phi|Y\\right) \\propto p\\left(Y|\\Phi\\right) \\pi\\left(k\\right) \\pi\\left(\\mu\\right) \\pi\\left(\\lambda\\right), \\] where the likelihood is \\[ \\begin{aligned} p\\left(Y|\\Phi\\right) &= \\prod_{i=1}^{k} p\\left(y_i|\\mu,k\\right) \\prod_{i=k+1}^{m} p\\left(y_i|\\lambda,k\\right) \\\\\\\\ &= \\prod_{i=1}^{k} \\frac{\\mu^{y_i}e^{-\\mu}}{y_i!} \\prod_{i=k+1}^{m} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!}. \\end{aligned} \\] As such, the full joint posterior can be written as: \\[ \\begin{aligned} \\pi\\left(\\Phi|Y\\right) &\\propto \\prod_{i=1}^{k} \\frac{\\mu^{y_i}e^{-\\mu}}{y_i!} \\prod_{i=k+1}^{m} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\left(\\mu^{a_1-1} e^{-\\mu b_1}\\right) \\left(\\lambda^{a_2-1} e^{-\\lambda b_2}\\right) \\frac{1}{m} \\\\\\\\ &= \\mu^{a_1 + \\sum_{1}^{k}y_i - 1}e^{-\\mu\\left(k+b\\_1\\right)} \\lambda^{a_2 + \\sum_{k+1}^{m}y_i - 1}e^{-\\lambda\\left(m-k+b_2\\right)} \\end{aligned}. \\] It follows that the full conditionals are, for \\(\\mu\\): \\[ \\begin{aligned} \\pi\\left(\\mu|\\lambda,k,Y\\right) &\\propto \\mu^{a_1 + \\sum_{i=1}^{k}y_i-1}e^{-\\mu\\left(k+b_1\\right)} \\\\\\\\ &= \\mathcal{G}\\left(a_1+\\sum_{i=1}^{k}y_i, k + b_1\\right) \\end{aligned} \\] We can define the \\(\\mu\\) update as: def mu_update(data : Array(Int), k : Int, b1 : Float64) : Float64 Gamma.sample(0.5 + data[0..k].sum, k + b1) end The full conditional for \\(\\lambda\\) is: \\[ \\begin{aligned} \\pi\\left(\\lambda|\\mu,k,Y\\right) &\\propto \\lambda^{a_2 + \\sum_{i=k+1}^{m}y_i-1}e^{-\\lambda\\left(m-k+b_2\\right)} \\\\\\\\ &= \\mathcal{G}\\left(a_2+\\sum_{i=k+1}^{m}y_i, m - k + b_2\\right), \\end{aligned} \\] which we implement as: def lambda_update(data : Array(Int), k : Int, b2 : Float64) : Float64 Gamma.sample(0.5 + data[(k+1)..M].sum, M - k + b2) end The next step is to take \\[ \\begin{aligned} b_1 &\\sim \\mathcal{G}\\left(a_1 + c_1,\\mu + d_1\\right) \\\\\\\\ b_2 &\\sim \\mathcal{G}\\left(a_2 + c_2,\\lambda + d_2\\right), \\end{aligned} \\] which we will implement as: def b1_update(mu : Float64) : Float64 Gamma.sample(0.5, mu + 1.0) end def b2_update(lambda : Float64) : Float64 Gamma.sample(0.5, lambda + 1.0) end And finally we choose the next year, \\(k\\), according to \\[ p\\left(k|Y,\\Phi\\right)=\\frac{L\\left(Y|\\Phi\\right)}{\\sum_{k^{\\prime}} L\\left(Y|\\Phi^{\\prime}\\right)} \\] where \\[ L\\left(Y|\\Phi\\right) = e^{\\left(\\lambda-\\mu\\right)k}\\left(\\frac{\\mu}{\\lambda}\\right)^{\\sum_i^k y_i} \\] implemented as def l(data : Array(Int), k : Int, lambda : Float64, mu : Float64) : Float64 Math::E**((lambda - mu)*k) * (mu / lambda)**(data[0..k].sum) end So, let's start by writing our initials conditions: iterations = 100000 b1 = 1.0 b2 = 1.0 M = data.size # number of data points # parameter storage mus = Array(Float64).new(iterations, 0.0) lambdas = Array(Float64).new(iterations, 0.0) ks = Array(Int32).new(iterations, 0) We can then cast the priors: mus[0] = Gamma.sample(0.5, b1) lambdas[0] = Gamma.sample(0.5, b2) ks[0] = Random.new.rand(M) And define the main body of our Gibbs sampler: (1...iterations).map { |i| k = ks[i-1] mus[i] = mu_update(data, k, b1) lambdas[i] = lambda_update(data, k, b2) b1 = b1_update(mus[i]) b2 = b2_update(lambdas[i]) ks[i] = Multinomial.sample((0...M).map { |kk| l(data, kk, lambdas[i], mus[i]) }) } Looking at the results, we see that the mean value of \\(k\\) is 38.761, which seems to indicate that the change in accident rates occurred somewhere near \\(1850+38.761\\approx 1889\\). We can visually check this by looking at the graph below. Also plotted are the density for the accident rates before (\\(\\mu\\)) and after (\\(\\lambda\\)) the change. Of course, one the main advantages of implementing the solution in Crystal is not only the boilerplate-free code, but the execution speed. Compared to an equivalent implementation in R the Crystal code executed roughly 17 times faster. Language Time (s) R 58.678 Crystal 3.587 ");
docs.push({"id": 2, "title": "Bayesian estimation of changepoints", "url": "/bayesian-estimation-of-changepoints.html"});
index.add(3, "A simple Python benchmark exercise Recently when discussing the Crystal language and specifically the Gibbs sample blog post with a colleague, he mentioned that the Python benchmark numbers looked a bit off and not consistent with his experience of numerical programming in Python. To recall, the numbers were: Language Time(s) R 364.8 Python 144.0 Scala 9.896 Crystal 5.171 C 5.038 To have a better understanding of what is happening, I've decided to profile and benchmark that code (running on Python 3.6). The code is the following: import random, math def gibbs(N=50000, thin=1000): x = 0 y = 0 print(\"Iter x y\") for i in range(N): for j in range(thin): x = random.gammavariate(3, 1.0 / (y y + 4)) y = random.gauss(1.0 / (x + 1), 1.0 / math.sqrt(2 x + 2)) print(i,x,y) if __name__ == \"main\": gibbs() Profiling this code with cProfile gives the following results: Name Call count Time (ms) Percentage gammavariate 50000000 141267 52.1% gauss 50000000 65689 24.2% <built-in method math.log> 116628436 18825 6.9% <method 'random' of '_random.Random' objects> 170239973 17155 6.3% <built-in method math.sqrt> 125000000 12352 4.6% <built-in method math.exp> 60119980 7276 2.7% <built-in method math.cos> 25000000 3338 1.2% <built-in method math.sin> 25000000 3336 1.2% <built-in method builtins.print> 50001 1030 0.4% gibbs.py 1 271396 100.0% The results look different than the original ones on account of being performed on a different machine. However, we will just look into the relative code performance between different implementations and whether the code itself has room for optimisation. Surprisingly, the console I/O took a much smaller proportion of the execution time than I expected (0.4%). On the other hand, as expected, the bulk of the execution time is spent on the gammavariate and gauss methods. These methods, however, are provided by the Python's standard library random, which underneath makes heavy usage of C code (mainly by usage of the random() function). For the second run of the code, I've decided to use numpy to sample from the Gamma and Normal distributions. The new code, gibbs_np.py, is provided below. import numpy as np import math def gibbs(N=50000, thin=1000): x = 0 y = 0 print(\"Iter x y\") for i in range(N): for j in range(thin): x = np.random.gamma(3, 1.0 / (y y + 4)) y = np.random.normal(1.0 / (x + 1), 1.0 / math.sqrt(2 x + 2)) print(i,x,y) if __name__ == \"main\": gibbs() We can see from the plots below that the results from both modules are identical. The profiling results for the numpy version were: Name Call count Time (ms) Percentage \\<method 'gamma' of 'mtrand.RandomState' objects> 50000000 121211 45.8% \\<method 'normal' of 'mtrand.RandomState' objects> 50000000 83092 31.4% \\<built-in method math.sqrt> 50000000 6127 2.3% \\<built-in method builtins.print> 50001 920 0.3% gibbs_np.py 1 264420 100.0% A few interesting results from this benchmark were the fact that using numpy or random didn't make much difference overall (264.4 and 271.3 seconds, respectively). This is despite the fact that, apparently, the Gamma sampling seems to perform better in numpy but the Normal sampling seems to be faster in the random library. You will notice that we've still used Python's built-in math.sqrt since it is known that for scalar usage it out-performs numpy's equivalent. Unfortunately, in my view, we are just witnessing a fact of life: *Python is not the best language for number crunching*. Since the bulk of the computational time, as we've seen, is due to the sampling of the Normal and Gamma distributions, it is clear that in our code there is little room for optimisation except the sampling methods themselves. A few possible solutions would be to: Convert the code to Cython Use FFI to call a highly optimised native library which provides Gamma and Normal distributions (such as GSL) Nevertheless, personally I still find Python a great language for quick prototyping of algorithms and with an excellent scientific computing libraries ecosystem. Keep on Pythoning.");
docs.push({"id": 3, "title": "A simple Python benchmark exercise", "url": "/a-simple-python-benchmark-exercise.html"});
index.add(4, "The Go language Some notes regarding the Go language. Some topics have graduated to their own page: Go resource bundling Go filesystem operations Language design Go doesn't have sets The Go language, notoriously, does not have1 some common data structures like sets. There are two main reasons for that: Go does not have generics1 Go relies on you writing your own data structures, generally Go lacks generics, which prevent writing a ... well, generic and efficient set implementation. Also, writing your own (non-generic) set with maps is quite straight-forward. The usual structure for a type T is map[T]bool, where the key is the element and the value is just a placeholder. For instance, for a int set: s := map[int]bool{1: true, 3: true} where we can add elements: s[1] = true // already present s[2] = true // adds new element Some other techniques for maps replacing sets: Set union set_union := map[int]bool{} for k, _ := range set_1{ set_union[k] = true } for k, _ := range set_2{ set_union[k] = true } Set intersection set_intersection := map[int]bool{} for k,_ := range set_1 { if set_2[k] { set_intersection[k] = true } } Set to array To convert a (map) set to an array: array := make([]int, 0) for k := range set_1 { array = append(array, k) } CI GitHub A potential workflow for GitHub is to use GitHub Actions for Go. An example workflow file, .github/workflows/test.yml, which runs go test (see Go) and go vet is: on: [push, pull_request\\] name: Test jobs: test: strategy: matrix: go-version: [1.14.x, 1.15.x] os: [ubuntu-latest] runs-on: ${{ matrix.os }} steps: - name: Install Go uses: actions/setup-go@v2 with: go-version: ${{ matrix.go-version }} - name: Checkout code uses: actions/checkout@v2 - name: Test run: go test ./... - name: Vet run: go vet ./... Containers Minimal example A minimal example of a Go container configuration for a web server running on port 8080: # Start from the latest golang base image FROM golang:latest # Add Maintainer Info LABEL maintainer=\"Rui Vieira\" # Set the Current Working Directory inside the container WORKDIR /app # Copy go mod and sum files COPY go.mod go.sum ./ # Download all dependencies. Dependencies will be cached if the go.mod and go.sum files are not changed RUN go mod download # Copy the source from the current directory to the Working Directory inside the container COPY . . # Build the Go app RUN go build -o main . # Expose port 8080 to the outside world EXPOSE 8080 # Command to run the executable CMD [\"./main\"] Reference Conversions How to convert a string to byte array? b := []byte(\"This is a string\") Collections Sort map keys alphabetically If a map contains string keys, i.e. var myMap map[string]T, we must sort the map keys independently. For instance: keys := make([]string, 0) for k, _ := range myMap { keys = append(keys, k) } sort.Strings(keys) for _, k := range keys { fmt.Println(k, myMap[k]) } Check for element If we consider a collection, say, []string collection, the way to check for an element already present is, for instance: func existsIn(needle string, haystack []string) bool { for _, element := range haystack { if element == needle { return true } } return false } Templates Check if variable empty In a Go template you check if a variable is empty by doing: {{if .Items}} <ul> {{range .Items}} <li>{{.Name}}</li> {{end}} </ul> {{end}} Looping over a map Looping over the map var data map[string]bool in a Go template: {{range $index, $element := .}} {{$index}}: {{$element}} {{end}} Processes Executing external processes Executing an external process and directing input and output to Stdout and Stderr. cmd := exec.Command(\"ls\", \"-1ao\") cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr err := cmd.Run() if err != nil { log.Fatalf(\"cmd.Run() failed with %s\\\\n\", err) } Testing in Go Place the tests in your place of choosing, but keep the package declaration. Test functions should be parameterised as (t *testing.T and start with the prefix Test, for instance: package main func TestFoo(t *testing.T) { value := Foo(5, 5) // ... assertions The test files themselves must have the suffix *_test.go. Call the tests with go test. As of the time of writing, that is Go 1.15. \u21a9\u21a9 ");
docs.push({"id": 4, "title": "Go", "url": "/go.html"});
index.add(5, "Python Main page for all things Python. Other pages cover specific topics, such as: Python environments How to setup Pweave Pandas Notes on Python's grammar of graphics Modules Relative import in Python 3 If a relative import is present inside a Python 3 file (e.g. file1) inside a module (e.g. mymod), say from .foo import bar We will encounter the error ImportError: attempted relative import with no known parent package A possible solution is to include the following in your module's __init__.py: import os, sys sys.path.append(os.path.dirname(os.path.realpath(__file__))) Boolean unravelling and unravelling the and boolean operator. The operation can be rewritten as the function u_and: def u_and(a, b): result = a if a: result = b return result For instance: a = True ; b = None print(a and b, u_and(a, b)) a = True ; b = True print(a and b, u_and(a, b)) a = False ; b = True print(a and b, u_and(a, b)) None None True True False False or On the other hand, or cand be unravelled as: def u_or(a, b): result = a if not a: result = b return result As an example: a = True ; b = None print(a or b, u_or(a, b)) a = True ; b = True print(a or b, u_or(a, b)) a = False ; b = True print(a or b, u_or(a, b)) True True True True True True The many faces of print Concatenating arguments var1 = \"Foo\" var2 = \"Bar\" print(\"I am \", var1, \" not \", var2) I am Foo not Bar var1 = \"Foo\" var2 = \"Bar\" print(\"I am \", var1, \" not \", var2) I am Foo not Bar It is also possible to use separators by using the sep argument: var1 = \"Foo\" var2 = \"Bar\" print(\"I am\", var1, \"not\", var2, sep=\"!\") I am!Foo!not!Bar String termination The end argument allows to specify the suffix of the whole string. print(\"This is on radio\", end=\" (over)\") This is on radio (over) Filesystem operations Get home directory For Python +3.5: from pathlib import Path home = str(Path.home()) List files recursively For Python +3.5, use glob: import glob # root_dir with trailing slash (i.e. /root/dir/) root_dir = \"./tmp\" for filename in glob.iglob(root_dir + '**/*.md', recursive=True): print(filename) Collections Sort an object list by attribute Assuming a list a with instances of Foo, such that import random from dataclasses import dataclass @dataclass class Foo: bar: str baz: int a = [Foo(bar=f\"foo-{i}\", baz=random.randint(0, 100)) for i in range(50)] a[1:5] [Foo(bar='foo-1', baz=3), Foo(bar='foo-2', baz=30), Foo(bar='foo-3', baz=56), Foo(bar='foo-4', baz=34)] To return a new collection: a_sorted = sorted(a, key=lambda x: x.baz, reverse=False) a_sorted[1:5] [Foo(bar='foo-1', baz=3), Foo(bar='foo-28', baz=3), Foo(bar='foo-41', baz=5), Foo(bar='foo-15', baz=6)] To sort in place using attribute bar: a.sort(key=lambda x: x.baz, reverse=True) a[1:5] [Foo(bar='foo-40', baz=91), Foo(bar='foo-18', baz=87), Foo(bar='foo-37', baz=86), Foo(bar='foo-43', baz=86)] ");
docs.push({"id": 5, "title": "Python", "url": "/python.html"});
index.add(6, "Cookiecutter data science Main documentation is available here. Setup Requirements For the purpose of these instructions we will assume the following are installed: Python 3.9.0 virtualenv A new venv can be created with virtualenv env1 and activated with source venv/bin/activate. Once the environment is active we can install the cookiecutter package using pip install cookiecutter. The create of the cookiecutter project can be done with cookiecutter https://github.com/drivendata/cookiecutter-data-science For the remainder of this text we will call the of the project you've just created as $PROJ. More details at Python environments. \u21a9 ");
docs.push({"id": 6, "title": "Cookiecutter Data Science", "url": "/cookiecutter-data-science.html"});
index.add(7, "Java Completable Futures Running in parallel import java.util.concurrent.CompletableFuture; CompletableFuture<String> future1 = CompletableFuture.supplyAsync(() -> \"Hello\"); CompletableFuture<String> future2 = CompletableFuture.supplyAsync(() -> \"Beautiful\"); CompletableFuture<String> future3 = CompletableFuture.supplyAsync(() -> \"World\"); CompletableFuture<Void> combinedFuture = CompletableFuture.allOf(future1, future2, future3); CompletableFuture<String> result = combinedFuture.thenApply(v -> future1.join() + future2.join() + future3.join()); System.out.println(result.get()); HelloBeautifulWorld ");
docs.push({"id": 7, "title": "Java Completable Futures", "url": "/java-completable-futures.html"});
index.add(8, "Emacs Notes on Emacs. My current flavour/distribution of Emacs is DOOM Emacs. Some configuration/usage notes: Go support in DOOM Emacs ");
docs.push({"id": 8, "title": "Emacs", "url": "/emacs.html"});
index.add(9, "Linux admin Notes on Linux admin.");
docs.push({"id": 9, "title": "Linux admin", "url": "/linux-admin.html"});
index.add(10, "Introduction to Balanced Box-Decomposition Trees Stardate 96893.29. You are the USS Euler's Science Officer at a moment when the computer graphical displays and voice systems went down. You only have enough deuterium for a short travel and need to find the nearest star system. This is not a simple matter of looking at a chart. You have multiple dimensions in which you can travel. In a bid for galactic peace, the Federation mandated that both Emacs and Vim should be installed in all computers. You open your favourite editor and, fortunately, know exactly how to formulate the solution to your problem: a \\(d\\)-dimensional nearest neighbour algorithm. Given a dataset \\(\\mathcal{D}\\) of \\(n\\) points in a space \\(X\\) we want to be able to tell which are the closest point to a query point \\(q \\in X\\), preferably in a way which is computationally cheaper than brute force methods (e.g. iterating through all of the points) which typically solve this problem in \\(\\mathcal{O}(dn)\\) 3. \\(X\\) could have \\(d\\) dimensions (that is \\(\\mathcal{D} \\subset X : \\mathbb{R}^d\\)) and we define closest using1 Minkowski distance metrics, that is: \\[ L_m = \\left(\\sum_{i=1}^d |p_i - q_i|^m\\right)^{\\frac{1}{m}},\\qquad p,q \\in X : \\mathbb{R}^d. \\] A potential solution for this problem would be to use kd-trees, which for low dimension scenarios provide \\(\\mathcal{O}(\\log n)\\) query times 4. However, as the number of dimensions increase (as quickly as \\(d>2\\)) the query times also increase as \\(2^d\\). The case can be made then for approximate nearest neighbour (NN) algorithms and that's precisely what we will discuss here, namely the Balanced Box-Decomposition Tree (BBD, 3). The definition of approximate NN for a query point \\(q\\) can be given as \\[ \\text{dist}(p, q) \\leq (1+\\epsilon)\\text{dist}(p^{\\star},q),\\qquad \\epsilon > 0, \\] where \\(p\\) is the approximate NN and \\(p^{\\star}\\) is the true NN. Let's consider, for the sake of visualisation, a small two dimensional dataset \\(\\mathcal{D} \\to \\mathbb{R}^2\\) as shown in Figure 1. Figure 1. A small test dataset in \\(\\mathbb{R}^2, n=7\\). Space decomposition BBD trees belong to the category of hierarchical space decomposition trees. In BBD trees, specifically, space is divided in \\(d\\)-dimensional rectangles and cells. Cells can either represent another \\(d\\)-dimensional rectangle or the intersection of two rectangles (one, the outer box fully enclosing the other, the inner box). Another important distinction of BBD trees is that rectangle's size (in this context, the largest length in all of the \\(d\\) dimensions) is bounded by a constant value. The space decomposition must follow an additional rule which is boxes must be sticky. If we consider a inner box \\([x_{inner}, y_{inner}]\\) contained in a outer box \\([x_{outer}, y_{outer}]\\), such that \\[ [x_{inner}, y_{inner}] \\subseteq [x_{outer}, y_{outer}], \\] then, considering \\(w = y_{inner} - x_{inner}\\), the box is considered sticky if either \\[ \\begin{aligned} x_{inner}-x_{outer} = 0 &amp;\\lor x_{inner}-x_{outer} \\nleq w \\\\ y_{outer}-y_{inner} = 0 &amp;\\lor y_{outer}-y_{inner} \\nleq w. \\end{aligned} \\] An illustration of the stickiness concept can viewed in the diagram below. Figure 2. Visualisation of the \"stickiness\" criteria for (\\mathbb{R}^2) rectangles. Stickiness provides some important geometric properties to the space decomposition which will be discussed further on. The actual process of space decomposition will produce a tree of nodes, each with an associated \\(d\\)-dimensional rectangle enclosing a set of points. Each node will be further decomposed into children nodes, containing a region of space with a subset of the parent's data points. If a node has no children it will be called a leaf node. The division process can occur either by means of: a fair split, this is done by partitioning the space with an hyperplane, resulting in a low and high children nodes a shrink, splitting the box into a inner box (the inner child) and a outer box (the outer child). Figure 3. \"Fair split\" and \"shrinking\" division strategies example in \\(\\mathbb{R}^2\\) with respective high/low and outer/inner children. The initial node of the tree, the root node, will include all the dataset points, \\(\\mathcal{D}\\). In the Figure 4 we can see a representation of the root node for the dataset presented above. We can see the node boundaries in dashed red lines as well as the node's center, marked as \\(\\mu_{root}\\). Figure 4. Associated cell for the BBD-tree root node for the example dataset. Node boundaries in red and node centre labelled as \\(\\mu_{root}\\). The actual method to calculate the division can either be based on the midpoint algorithm or the middle interval algorithm. The method used for these examples is the latter, for which more details can be found in 6. The next step is to divide the space according to the previously mentioned rules. As an example, we can see the root node's respective children in Figure 5. Figure 5. BBD-tree root node's lower (left) and upper (right) children. Node boundaries in red and centres labelled with a red cross. This process is repeated until the child nodes are leaves and cannot be divided anymore. To better visualise the construction process it would be helpful to have a larger tree, so we will now consider still the 2-dimensional case, but now with a larger dataset (Figure 6), consisting of 2000 samples in total, each half from a bivariate Gaussian distribution: \\[ \\begin{aligned} \\text{X}_1 &amp;\\sim \\mathcal{N}([0,0], \\mathbf{I}) \\\\ \\text{X}_2 &amp;\\sim \\mathcal{N}([3, 3], \\mathbf{I}). \\\\ \\end{aligned} \\] Figure 6. Larger example dataset in \\(\\mathbb{R}^2\\) consisting of a realisation of \\(n=2000\\) from two bivariate Gaussian distributions centred in \\(\\mu_1=(0,0)\\) and \\(\\mu_2=(3,3)\\) and with \\(\\Sigma=\\mathbf{I}\\). With this larger dataset, we have enough points to illustrate the tree node building. This time, we will start from the root node and always follow either the \"lower\" nodes or the \"upper\" nodes (as show in Figure 7). We can clearly see the cells getting smaller, until finally we have a single point included (i.e. a leaf node). Figure 7. BBD-tree node building process for the bivariate dataset. On the left we traverse the upper tree nodes and on the right the lower tree nodes. This division process illustrates an important property of BBD-trees. Although other space decomposition algorithms (such as kd-trees) display a geometric reduction of number of points enclosed in each cell, methods such as the BBD-tree, which impose constraints on the cell's size aspect ratio as stated before, display not only a geometric reduction in the number of points, but also in the cell's size as well. The construction cost of a BBD-tree is \\(\\mathcal{O}(dn \\log n)\\) and the tree itself will have \\(\\mathcal{O}(n)\\) nodes and \\(\\mathcal{O}(\\log n)\\) height. Tree querying Now that we have successfully constructed a BBD-tree, we want to actually find the (approximate) nearest neighbour of an arbitrary query point \\(q\\) (Figure 8). Figure 8. Query point \\(q\\) (red) for the bivariate dataset. The first step consists in descending the tree in order to locate the smallest cell containing the query point \\(q\\). This process is illustrated for the bivariate data in Figure 9. Figure 9. BBD-tree descent to locate the smallest cell containing \\(q\\) (red). Once the cell has been located, we proceed to enumerate all the leaf nodes contained by it and calculate our distance metric \\(L_2\\) in this case) between the query point \\(q\\) and the leaf nodes, eventually declaring the point with the smallest \\(L_2\\) as the aproximate NN. BBD-trees provide strong guarantees that the ANN will be located within this cell and not in a neighbouring cell. In Figure 10 we zoomed in the smallest cell containing \\(q\\) and show the associated calculated \\(L_2\\) distance for each node. Figure 10. \\(L_2\\) distance between leaf nodes and the query point \\(q\\) inside the smallest cell containing \\(q\\). An important property of BBD-trees is that the tree structure does not need to be recalculated if we change either \\(\\epsilon\\) or if we decide to use another \\(L_m\\) distance metric 3. The query time for a point \\(q\\) in a BBD-tree is \\(\\mathcal{O}(\\log n)\\). For comparison, if you recall, the query time for a brute force method is typically \\(\\mathcal{O}(dn)\\). Filtering and k-NN Great. Now that you solved the USS Euler's problem, you want to make a suggestion to the federation. Where to place several star-bases and how to divide the system's coverage between them. An immediate generalisation of this method is easily applicable to the problem of clustering. Note that, at the moment, we are not concerned with determining the \"best\" clusters for our data2. Given a set of points \\(Z = \\{z_1, z_2, \\dots, z_n\\}\\), we are concerned now in partitioning the data in clusters centred in each of the \\(Z\\) points. A way of looking at this, is that we are building, for each point \\(z_n\\) a Voronoi cell \\(V(z_n)\\). This is achieved by a method called filtering. Filtering, in general terms, works by walking the tree with the list of candidate centres (\\(Z\\)) and pruning points from the candidate list as we move down. We will denote an arbitrary node as \\(n\\), \\(z^{\\star}_w\\) and \\(n_w\\) respectively as the candidate and the node weight, \\(z^{\\star}_n\\) and \\(n_n\\) as the candidate and node count. The algorithm steps, as detailed in 5, are detailed below: Filter(\\(n\\), \\(Z\\)) { \\(\\qquad C \\leftarrow n.cell\\) \\(\\qquad\\) if (\\(n\\) is a leaf) { \\(\\qquad\\qquad z^{\\star} \\leftarrow\\) the closest point in \\(Z\\) to \\(n.point\\) \\(\\qquad\\qquad z^{\\star}_w \\leftarrow z^{\\star}_w + n.point\\) \\(\\qquad\\qquad z^{\\star}_n \\leftarrow z^{\\star}_n + 1\\qquad\\) } else { \\(\\qquad\\qquad z^{\\star} \\leftarrow\\) the closest point in \\(Z\\) to \\(C\\)'s midpoint \\(\\qquad\\qquad\\)for each (\\(z \\in Z \\setminus \\{z^{\\star}\\}\\)) { \\(\\qquad\\qquad\\qquad\\) if (\\(z.isFarther(z^{\\star},C)\\)) { \\(\\qquad\\qquad\\qquad\\qquad Z \\leftarrow Z \\setminus \\{z\\}\\) \\(\\qquad\\qquad\\)} \\(\\qquad\\qquad\\)if (\\(|Z|=1\\)) { \\(\\qquad\\qquad\\qquad z^{\\star}_w \\leftarrow z^{\\star}_w + n_w\\) \\(\\qquad\\qquad\\qquad z^{\\star}_n \\leftarrow z^{\\star}_n + n_n\\) \\(\\qquad\\qquad\\)} else { \\(\\qquad\\qquad\\qquad\\)Filter(\\(n_{left}, Z\\)) \\(\\qquad\\qquad\\qquad\\)Filter(\\(n_{right}, Z\\)) \\(\\qquad\\qquad\\)} } To illustrate the assignment of data points to the centres, we will consider the previous bivariate Gaussian data along with two centres, \\(z_1 = \\{0,0\\}\\) and \\(z_2 = \\{3, 3\\}\\). Figure 11 shows the process of splitting the dataset \\(\\mathcal{D}\\) into two clusters, namely the subsets of data points closer to \\(z_1\\) or \\(z_2\\). Figure 11. Assignment of points in \\(\\mathcal{D}\\) to \\(Z\\). Data points coloured according to the assigned center. Lines represent the distance from the cells midpoint to \\(Z\\). We can see in Figure 12 the final cluster assignment of the data points. With a \\(\\mathbb{R}^2\\) dataset and only two centres the organisation of points follows a simple perpendicular bisection of the segment connecting the centres, as expected. Figure 12. Final \\(\\mathcal{D}\\) point assignment to clusters centred in \\(z_1\\) and \\(z_2\\). In Figure 13 we can see more clearly the dataset clusters changing when center \\(z_1\\) is moving around the plane. BBD-trees can play an important role in improving \\(k\\)-means performance, as described in 5. Figure 13. Dynamic assignment of points to a cluster using a BBD-tree. This concludes a (short) introduction to BBD-trees, I hope you enjoyed it. If you have any comments or suggestions, please let me know at Mastodon. The \\(L_m\\) distance may be pre-computed in this method to avoid recalculation for each query. \u21a9 This would be a k-means problem. I intend to write a blog post on k-means clustering (and the role BBD-trees can play) in the future. \u21a9 Arya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., & Wu, A. Y. (1998). An optimal algorithm for approximate nearest neighbor searching fixed dimensions. Journal of the ACM. https://doi.org/10.1145/293347.293348 \u21a9\u21a9\u21a9 Friedman, J. H., & Bentley, J. L. (1977). RA Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software, 3(3), 209-226. \u21a9 Kanungo, T., Mount, D. M., Netanyahu, N. S., Piatko, C. D., Silverman, R., & Wu, A. Y. (2002). An efficient k-means clustering algorithms: Analysis and implementation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(7), 881\u2013892. https://doi.org/10.1109/TPAMI.2002.1017616 \u21a9\u21a9 Callahan, P. B., & Kosaraju, S. R. (1995). A decomposition of multidimensional point sets with applications to k-nearest-neighbors and n-body potential fields. Journal of the ACM, 42(1), 67-90. \u21a9 ");
docs.push({"id": 10, "title": "Introduction to Balanced Box-Decomposition Trees", "url": "/introduction-to-balanced-box-decomposition-trees.html"});
index.add(11, "K-means clustering Introduction K-means is still one of the fundamental clustering algorithms. It is used in such diverse fields as Natural Language Processing (NLP), social sciences and medical sciences. The core idea behind K-means is that we want to group data in clusters. Data points will be assigned to a specific cluster depending on it's distance to a cluster's center, usually called the centroid. It is important to note that typically, the mean distance to a centroid is used to partition the clusters, however, difference distances can be used and different pivot points. An example is the K-medoids clustering algorithm. We will define the two main steps of a generic K-means clustering algorithm, namely the data assignement and the centroid update step. Data assignement The criteria to determine whether a point is closer to one centroid is typically an Euclidean distance (\\(L^2\\)) . If we consider a set of \\(n\\) centroids \\(C\\), such that \\[ C = \\lbrace c_1, c_2, \\dots, c_n \\rbrace \\] We assign each data point in \\(\\mathcal{D}=\\lbrace x_1, x_2, \\dots, x_n \\rbrace\\) to the nearest centroid according to its distance, such that \\[ \\underset{c_i \\in C}{\\arg\\min} \\; dist(c_i,x)^2 \\] As mentioned previously \\(dist(\\cdot)\\) is typically the standard (\\(L^2\\)) Euclidean distance. We define the subset of points assigned to a centroid \\(i\\) as \\(S_i\\). Centroid update step This step corresponds to updating the centroids using the mean of add points assign to a cluster, \\(S_i\\). That is \\[ c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i} x_i \\] Partitioning Different algorithms can be used for cluster partitioning, for instance: PAM CLARA CLARANS PAM To illustrate the PAM partitioning method, we will use a synthetic dataset created along the guidelines in synthetic data generation. Elbow method In order to use the \"Elbow method\" we calculate the Within-Cluster Sum of Squares (WCSS) for a varying number of clusters, \\(K\\). import numpy as np import matplotlib.pyplot as plt import pandas as pd import sklearn import warnings warnings.filterwarnings(\"ignore\") dataset = pd.read_csv('data/mall-customers.zip') X = dataset.iloc[:, [3, 4]].values from sklearn.cluster import KMeans wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42) kmeans.fit(X) wcss.append(kmeans.inertia_) from plotutils import * plt.plot(range(1, 11), wcss) plt.xlabel('Number of clusters') plt.ylabel('WCSS') plt.show() kmeans = KMeans(n_clusters = 5, init = \"k-means++\", random_state = 42) y_kmeans = kmeans.fit_predict(X) ps = 30 plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = ps, c = colours[0], label = 'Cluster1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = ps, c = colours[1], label = 'Cluster2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = ps, c = colours[2], label = 'Cluster3') plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = ps, c = colours[3], label = 'Cluster4') plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = ps, c = colours[4], label = 'Cluster5') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = 'black', label = 'Centroids') plt.xlabel('Annual Income (k$)') plt.ylabel('Spending Score (1-100)') plt.legend() plt.show() ");
docs.push({"id": 11, "title": "K-means clustering", "url": "/k-means-clustering.html"});
index.add(12, "Monotonic Cubic Spline interpolation (with some Rust) Monotonic Cubic Spline interpolation (MCSI) is a popular and useful method which fits a smooth, continuous function through discrete data. MCSI has several applications in the field of computer vision and trajectory fitting. MCSI further guarantees monotonicity of the smoothed approximation, something which a cubic spline approximation alone cannot. In this post I'll show how to implement the method developed by F. N. Fritsch and R. E. Carlson 1 in the Rust programming language. Rust Why Rust? Definitely this is a type of solution so simple that it can be implemented in practically any programming language we can think of. However, I do find that the best way to get acquainted with a new language and its concepts is precisely to try to implement a simple and well-know solution. Although this post does not intend to be an introduction to the Rust language, some of the fundamentals will be presented as we go along. Idiomatic Rust Object-Oriented Programming (OOP) has several characteristics which differ significantly from \"traditional\" OOP languages. Rust achieves data and behaviour encapsulation by means of defining data structure blueprints (called struct) and then defining their behaviour though a concrete implementation (through impl). As an example, a simple \"class\" Foo would consist of: struct Foo { } impl Foo { fn new() -> Foo { return Foo {}; } fn method(&mut self) {} fn static_method() {} } pub fn main() { let mut f = Foo::new(); f.method(); Foo::static_method(); } The \"constructor\" is defined typically as new(), but any \"static\" method which returns an initialised struct can be a constructor and \"object\" methods include the passing of the self instance not unlike languages such as Python. The &mut self refers to the control or exclusive access to self and it is not directly related to mut mutability control. These concepts touch on Rust's borrowing and ownership model which, unfortunately, are way beyond the scope of this blog post. A nice introduction is provided by the \"Rust programming book\" available here. Our implementation aims at building a MCSI class MonotonicCubicSpline by splitting the algorithm into the slope calculation at construction time, a Hermite interpolation function and a partial application function generator. This will follow the general structure pub struct MonotonicCubicSpline { m_x: Vec<f64>, m_y: Vec<f64>, m_m: Vec<f64> } impl MonotonicCubicSpline { pub fn new(x : &Vec<f64>, y : &Vec<f64>) -> MonotonicCubicSpline { // ... } pub fn hermite(point: f64, x : (f64, f64), y: (f64, f64), m: (f64, f64)) -> f64 { // ... } pub fn interpolate(&mut self, point : f64) -> f64 { // ... } fn partial(x: Vec<f64>, y: Vec<f64>) -> impl Fn(f64) -> f64 { // ... } } Vec is a vector, a typed growable collection available in Rust's standard library with documentation available here. Monotonic Cubic Splines MCSI hinges on the concept of cubic Hermite interpolators. The Hermite interpolation for the unit interval for a generic interval \\((x_k,x_{k+1})\\) is \\[ p(x)=p_k h_{00}(t)+ h_{10}(t)(x_{k+1}-x_k)m_k + \\\\ h_{01}(t)p_{k+1} + h_{11}(t)(x_{k+1}-x_{k})m_{k+1}. \\] The \\(h_{\\star}\\) functions are usually called the Hermite basis functions in the literature and here we will use the factorised forms of: \\[ \\begin{aligned} h_{00}(t) &= (1+2t)(1-t)^2 \\\\ h_{10}(t) &= t(1-t)^2 \\\\ h_{01}(t) &= t^2 (3-2t) \\\\ h_{11}(t) &= t^2 (t-1). \\end{aligned} \\] This can be rewritten as \\[ p(x) = (p_k(1 + 2t) + \\Delta x_k m_k t)(1-t)(1-t) + \\\\ (p_{k+1} (3 -2t) + \\Delta x_k m_{k+1} (t-1))t^2 \\] where \\[ \\begin{aligned} \\Delta x_k &= x_{k+1} - x_k \\\\ t &= \\frac{x-x_k}{h}. \\end{aligned} \\] This associated Rust method is the above mentioned \"static\" MonotonicCubicSpline::hermite(): pub fn hermite(point: f64, x : (f64, f64), y: (f64, f64), m: (f64, f64)) -> f64 { let h = x.1 - x.0; let t = (point - x.0) / h; return (y.0 (1.0 + 2.0 t) + h m.0 t) (1.0 - t) (1.0 - t) + (y.1 (3.0 - 2.0 t) + h m.1 (t - 1.0)) t t; } where the tuples correspond to \\(x \\to (x_k, x_{k+1})\\), \\(t \\to (y_k, y_{k+1})\\) and \\(m \\to (m_k, m_{k+1})\\) For a series of data points \\((x_k, y_k)\\) with \\(k=1,\\dots,n\\) we then calculate the slopes of the secant lines between consecutive points, that is: \\[ \\Delta_k = \\frac{\\Delta y_{k}}{\\Delta x_k},\\qquad \\text{for}\\ k=1,\\dots,n-1 \\] with \\(Delta y_k = y_{k+1}-y_k\\) and \\(\\Delta x_k\\) as defined previously. Since the data is represented by the vectors x : Vec<f64> and y : Vec<f64> we implement this in the \"constructor\": let mut secants = vec![0.0 ; n - 1]; let mut slopes = vec![0.0 ; n]; for i in 0..(n-1) { let dx = x[i + 1] - x[i]; let dy = y[i + 1] - y[i]; secants[i] = dy / dx; } The next step is to average the secants in order to get the tangents, such that \\[ m_k = \\frac{\\Delta_{k-1}+\\Delta_k}{2},\\qquad \\text{for}\\ k=2,\\dots,n-1. \\] This is achieved by the code: slopes[0] = secants[0]; for i in 1..(n-1) { slopes[i] = (secants[i - 1] + secants[i]) * 0.5; } slopes[n - 1] = secants[n - 2]; By definition, we want to ensure monotonicity of the interpolated points, but to guarantee this we must avoid the interpolation spline to go too far from a certain radius of the control points. If we define \\(\\alpha_k\\) and \\(\\beta_k\\) as \\[ \\begin{aligned} \\alpha_k &= \\frac{m_k}{\\Delta_k} \\\\ \\beta_k &= \\frac{m_{k+1}}{\\Delta_k}, \\end{aligned} \\] to ensure the monotonicity of the interpolation we can impose the following constraint on the above quantities: \\[ \\phi(\\alpha, \\beta) = \\alpha - \\frac{(2\\alpha+\\beta-3)^2}{3(\\alpha+\\beta-2)}\\geq 0, \\] that is \\[ \\alpha + 2\\beta - 3 \\leq 0, \\text{or}\\ 2\\alpha+\\beta-3 \\leq 0 \\] Typically the vector \\((\\alpha_k, \\beta_k)\\) is restricted to a circle of radius 3, that is \\[ \\alpha^2_l + \\beta_k^2>9, \\] and then setting \\[ m_{k+1} = t\\beta_k\\Delta_k, \\] where \\[ \\begin{aligned} h &amp;= \\sqrt{\\alpha^2_k + \\beta^2_k} \\\\ t &amp;= \\frac{3}{h}. \\end{aligned} \\] One of the ways in which Rust implements polymorphism is through method dispatch. The f64 primitive provides a shorthand for the quantity \\(\\sqrt{\\alpha^2_k + \\beta^2_k}\\) as \\(\\alpha.\\text{hypot}(\\beta)\\). The relevant Rust code will then be: for i in 0..(n-1) { if secants[i] == 0.0 { slopes[i] = 0.0; slopes[i + 1] = 0.0; } else { let alpha = slopes[i] / secants[i]; let beta = slopes[i + 1] / secants[i]; let h = alpha.hypot(beta); if h > 3.0 { let t = 3.0 / h; slopes[i] = t * alpha * secants[i]; slopes[i + 1] = t * beta * secants[i]; } } } We are now able to define a \"smooth function\" generator using MCSI. We generate a smooth function \\(g(\\cdot)\\) given a set of \\((x_k, y_k)\\) points, such that \\[ f(x_k, y_k, p) \\to g(p). \\] Partial application Before anything, it is important to recall the difference between partial application and currying, since the two are (incorrectly) used interchangeably quite often. Function currying allows to factor functions with multiple arguments into a chain of single-argument functions, that is \\[ f(x, y, z) = h(x)(y)(z) \\] The concept is prevalent in functional programming, since its initial formalisation 2. Partial application, however, generally aims at using an existing function conditioned on some argument as a basis to build functions with a reduced arity. In this case this would be useful since ultimately we want to create a smooth, continuous function based on the control points \\((x_k, y_k)\\). The partial application implementation is done in Rust as pub fn partial(x: Vec<f64>, y: Vec<f64>) -> impl Fn(f64) -> f64 { move |p| { let mut spline = MonotonicCubicSpline::new(&x, &y); spline.interpolate(p) } } An example of how to generate a concrete smoothed continuous function from a set of control points can be: let x = vec![0.0, 2.0, 3.0, 10.0]; let y = vec![1.0, 4.0, 8.0, 10.5]; let g = partial(x, y); // calculate an interpolated point let point = g(0.39); The full code can be found here. Fritsch, F. N., & Carlson, R. E. (2005). Monotone Piecewise Cubic Interpolation. SIAM Journal on Numerical Analysis. https://doi.org/10.1137/0717021 \u21a9 Curry, Haskell; Feys, Robert (1958). Combinatory logic. I (2 ed.). Amsterdam, Netherlands: North-Holland Publishing Company. \u21a9 ");
docs.push({"id": 12, "title": "Monotonic Cubic Spline interpolation (with some Rust)", "url": "/monotonic-cubic-spline-interpolation-with-some-rust.html"});
index.add(13, "Portuguese Christmas recipes This year we spent Christmas in \"lockdown\" and we tried to make ourselves our full traditional Portuguese Christmas recipes from scratch -- while not being in Portugal. Herein lies the first issue: there are many different \"traditions\", but these are the ones that me and my partner are used to. Traditionally, Christmas celebrations in Portugal start on the night of Christmas eve and carry on during Christmas day. The main meals are then dinner on the 24th December and lunch on the 25th December. Christmas eve dinner As mentioned, we will follow two separate traditions. From my family's side, originally from the north of Portugal (Porto), we typically have boiled salted cod with vegetables (cabbage, onion and carrots), seasoned with olive oil, vinegar and garlic. From my partner's side, the meal typically consists of octopus rice, accompanied with salted cod fishcakes (\"bolinhos de bacalhau\") and pan-fried octopus (\"filetes de polvo\"). Salted cod Salted cod is a staple from Portuguese cuisine, with possible origins in the cod salt-curing methods of Basque fishermen that ventured into Newfoundland in the 1500s1. Going through the centuries, even as recently as 1884, Portuguese writer E\u00e7a de Queiroz wrote in a letter to Oliveira Martins about his love of a \"bacalhau de cebolada\"2. The salted cod needs to be soaked in water for at least four days to remove the excess salt (picture above, on the left). It's a really simple dish: just put everything on a pot and let it boil for approximately one hour. Since the salted cod has a really firm fleshy texture, it won't fall apart like fresh fish when boiled for a long time. Usually there's no need to add salt, since the cod will probably still have quite a lot of salt in it, but it never hurts to double check. We will cook around two to three times the amount of cod and vegetables that we need for the actual meal. The reason for this is that the starter for the next day (Portuguese Christmas recipes) is made from the left-overs of the Christmas eve's dinner. So essentially, we have to make sure we have plenty of left-overs! And here it is: ready to tuck in. As you can see, I like my salted cod with a very generous amount of olive oil and vinegar. Octopus rice Octopus is another northern Portuguese tradition, especially in the Minho and Tr\u00e1s-os-Montes, possibly due to the proximity with Galiza (Galicia) where octopus fishing has been historically a very important activity. Next it's the octopus rice. Boil the octopus with just some salt for seasoning. Knowing when the octopus is ready is really an art. Make sure its not undercooked, but don't overcook it either since it will be quite chewy. Brown chopped onions in olive oil and add the water from boiling the octopus along with rice, the octopus and chopped parsley. The rice should have a fair amount of water and not end up dry. Part of the octopus goes into the rice and the rest is pan-fried (\"filetes de polvo\"). They are battered, with eggs and flour, and deep-fried. We then proceed to the cod fishcakes (\"bolinhos de bacalhau\"). These are done by shredding some salted code and mixing it with mashed potato, salt and parsley and then deep-fried. Christmas day lunch Roupa-velha The reason why we cook way more quantities than we need for the salted cod, is to make something called \"roupa-velha\" (literal translation \"old clothes\") as a starter on the 25th. This a left-over dish and we use all the left-overs from the Christmas Eve dinner. Start by shredding the cooked salted cod and removing all the fish bones and skin. We then add a good amount of garlic (two or three cloves at least), and prepare a pan with some olive oil. We put first the garlic and let it brown. When the garlic is brown we add all the left-overs (potato, sliced egg, carrot, cabbage and shredded code). We stir it for at least 15 minutes and add vinegar. Lots of vinegar. And here it is. Must be eaten while pipping hot. Lamb roast Usually on the 25th of December we eat a roast (turkey, lamb, goat or pork). We went for a lamb roast. It was seasoned with lemon, rosemary, garlic, paprika, olive oil and salt for four days. It is accompanied by roast potatoes and carrots and (optionally) some white rice. And here it is! Desserts Aletria and arroz doce Aletria is a typical Christmas dessert which is quite similar to rice pudding in taste, but instead of rice, it is done with vermicelli pasta. The preparation is quite similar to rice pudding, but adding some lemon peels to the milk mix. A cinnamon decoration is a must, here shown with a festive \"Feliz Natal\" (Merry Christmas). \"Arroz doce\" (literal translation Sweet Rice) is very similar to rice pudding, also with the addition of some lemon. Filh\u00f3s Filh\u00f3s are a type of slightly sweet doughy pancake, usually sprinkled with sugar and cinnamon, traditional during Christmas. These are specific type of filh\u00f3 called \"Filh\u00f3 tendida no joelho\", traditional from the Beiras Portuguese region, where the dough is stretched on top of the knee. The dough has to be proven at a certain temperature. Here is the contraption we've used: a heating fan, heater and an Hibernate (!) book. The dough must be proved (in our case at least 10 hours) so it can be stretched and fried lightly in olive oil on both sides. After draining any excess oil, they are sprinkled with a sugar and cinnamon mix. Rabanadas \"Rabanadas\" are in essence very similar to \"French toast\". The way to prepare them is to leave dried bread soaking in milk and drained before deep-frying. After draining, until they are mostly dry and without much excess oil, they are sprinkled (generously) with a mixture of sugar and cinnamon. Silva, Ant\u00f3nio Jos\u00e9 Marques da (2015), \"The fable of the cod and the promised sea: About Portuguese traditions of bacalhau\", in Barata, Filipe Themudo; Rocha, Jo\u00e3o Magalh\u00e3es (eds.), Heritages and Memories from the Sea, \u00c9vora, Portugal: 1st International Conference of the UNESCO Chair in Intangible Heritage and Traditional Know-How: Linking Heritage \u21a9 \"A comida como h\u00e1bito e identidade: o bacalhau e os portugueses\", (ISCTE-IUL, Departamento de Antropologia, Escola de Ci\u00eancias Humanas e Sociais), em 28 de fevereiro de 2013 \u21a9 ");
docs.push({"id": 13, "title": "Portuguese Christmas recipes", "url": "/portuguese-christmas-recipes.html"});
index.add(14, "Clojure Notes on Clojure. Reference Concatenating strings (require '\\[clojure.string :as string\\]) (string/join [\"foo\" \"bar\"]) List files recursively (file-seq \"/etc\") (Compare with the Java version.) Filter by extension: (filter #(.endsWith (.toString %) \".conf\") (file-seq \"/etc\"))) Get home directory def home (System/getProperty \"user.home\")) ");
docs.push({"id": 14, "title": "Clojure", "url": "/clojure.html"});
index.add(15, "MCMC notifications It is said that patience is a virtue but the truth is that no one likes waiting (especially waiting around: this interesting article explores why people prefer walking 8 minutes to the airport\u2019s baggage claim and having the bags ready rather than waiting the same amount of time entirely in the claim area). Anyone performing computationally heavy work, such as Monte Carlo methods, will know that these are usually computationally expensive algorithms which, even in modern hardware, can result in waiting times in the magnitude of hours, days and even weeks. These long running times coupled with the fact that in certain cases it is not easy to accurately predict how long a certain number of iterations will take, usually leads to a tiresome behaviour of constantly checking for good (or bad) news. Although it is perfectly possible to specify that your simulation should stop after a certain amount of time (especially valid for very long simulations), this doesn\u2019t seem to be the standard practice. In this post I\u2019ll detail my current setup for being notified exactly of when simulations are finished. To implement this setup, the following stack is required: A JDK Apache Maven A messaging service Pushbullet A smartphone, tablet, smartwatch (or any other internet enabled device) To start, we can create an account in Pushbullet, which will involve, in the simplest case, signing up using some authentication service such as Google. Next, we will install the client application (available for Android, iOS and most modern browsers after which we can enable notifications (at least in the Android client, I\u2019m not familiar with the iPhone version). Since my current work started as a plain Java project which in time evolved mainly to <a href\\=\"http://scala-lang.org\">Scala</a>, it consists of an unholy mixture of Maven as a build tool for Scala code. This shouldn't be a problem for other setups, but I\u2019ll just go through my specific setup (i.e. using Maven dependencies to a Scala project). To implement communication between the code and the messaging service, we can use a simple library such as jpushbullet. The library works well enough, although at the time of writing it only supports Pushbullet\u2019s v1 API but not the newer v2 API. Since the project, unfortunately, is not in Maven central, you should build it from scratch. Fortunately, in a sensibly configured machine, this is trivial. In the machine where you plan to perform the simulations, clone and build jpushbullet. git clone git@github.com:silk8192/jpushbullet.git mvn clean install Once the build is complete, you can add it as a local dependency in your project\u2019s pom.xml: <dependencies> <dependency> <groupId>com.shakethat.jpushbullet</groupId> <artifactId>jpushbullet</artifactId> </dependency> </dependencies> For the purpose of this example, lets assume that you have the following Object as the entry point of your simulation. The next step is to add a call to the Pushbullet service before the exit point. Please keep in mind that it is very bad practice to include your personal API key in your committed code. I strongly suggest you keep this information in a separate file (e.g. in resources), read it at runtime and add it to .gitignore. That being said, place the messaging code as such: package benchmarks import com.shakethat.jpushbullet.PushbulletClient object MCMC { def main(args:Array[String]):Unit = { // Your MCMC code val client = new PushbulletClient(api\\_key) val devices = client.getDevices val title = \"MCMC simulation finished\" val body = \"Some summary can be included\" // n is the preferred device number client.sendNote(true, devices .getDevices .get(n) .getIden(), title, body) } } Usually, I would call this code via ssh into my main machine from Maven (using Scala Maven as: $ nohup mvn scala:run -DmainClass=benchmarks.MCMC & Finally, when the simulation is completed, you will be notified in the client devices (you can select which ones by issuing separate sendNote calls) and include a result summary, as an example. My current setup generates an R script from a template which is run by Rscript in order to produce a PDF report. However, be careful, since file quotas in Pushbullet are limited, so text notifications should be used without worry of going over the free usage tier.> Keep in mind, that there are other alternatives to jpushbullet, such as send-notification, a general notification library for Java for which the setup is quite similar. Hope this was helpful.");
docs.push({"id": 15, "title": "MCMC notifications", "url": "/mcmc-notifications.html"});
index.add(16, "Error metrics Root Mean Squared error A typical way of measuring the difference between observations and results from a predictor. The formal definition is: \\[ \\begin{aligned} RMSE(\\hat{\\theta}) &= \\sqrt{\\operatorname{MSE}(\\hat{\\theta})} \\\\ &= \\sqrt{\\operatorname{E}((\\hat{\\theta}-\\theta)^2)}. \\end{aligned} \\] For \\(N\\) observations \\(Y=\\{y_1,\\dots,y_N\\}\\) we can express it as: \\[ RMSE=\\sqrt{\\frac{\\sum_{n=1}^{N}({\\hat {y}}_{n}-y_{n})^{2}}{N}}. \\]");
docs.push({"id": 16, "title": "Error metrics", "url": "/error-metrics.html"});
index.add(17, "Rust Install curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source $HOME/.cargo/env Create a new project using $ cargo new hello_world --bin # for a binary $ cargo new hello_world # for a library Reference List folders recursively Using the glob crate: use glob::glob; fn main() { for entry in glob(\"./**/*.md\").expect(\"Failed to read glob pattern\") { match entry { Ok(path) => println!(\"{:?}\", path.display()), Err(e) => println!(\"{:?}\", e), } } } ");
docs.push({"id": 17, "title": "Rust", "url": "/rust.html"});
index.add(18, "DOOM Emacs Go Setup The following modules must be enabled in init.el: (go +lsp) in the lang section lsp in the tools section snippets in the editor section gopls should be installed. Running doom sync will finish the setup.");
docs.push({"id": 18, "title": "DOOM Emacs Go", "url": "/doom-emacs-go.html"});
index.add(19, "Go resource bundling Notes on the installation and usage of pkger. Installation done with go get github.com/markbates/pkger/cmd/pkger pkger works by bundling the resources with a code-generated pkg.go. The configuration of assets to be bundled is done by reflection at compile time and not direct configuration. This is done by replacing standard Go file operations with pkger proxy ones, such as: type Pkger interface { Parse(p string) (Path, error) Current() (here.Info, error) Info(p string) (here.Info, error) Create(name string) (File, error) MkdirAll(p string, perm os.FileMode) error Open(name string) (File, error) Stat(name string) (os.FileInfo, error) Walk(p string, wf filepath.WalkFunc) error Remove(name string) error RemoveAll(path string) error } type File interface { Close() error Info() here.Info Name() string Open(name string) (http.File, error) Path() Path Read(p \\[\\]byte) (int, error) Readdir(count int) (\\[\\]os.FileInfo, error) Seek(offset int64, whence int) (int64, error) Stat() (os.FileInfo, error) Write(b \\[\\]byte) (int, error) } Example Bundling a Go template file. tmplFile, _ := pkger.Open(\"/templates/page.tmpl\") tmplBytes, _ := ioutil.ReadAll(tmplFile) tmplString := string(tmplBytes) tpl, err := template.New(\"page\").Parse(tmplString) _ = tpl.Execute(f, ...) The bundling is simply done by running pkger and building as usual go build ");
docs.push({"id": 19, "title": "Go resource bundling", "url": "/go-resource-bundling.html"});
index.add(20, "Serving models with Seldon Deploying machine learning models in production comes with several requirements. We must manage the model lifecycle. We need reproducibility and typically use containerised workflows. Seldon is a tool which aims at providing a production workflow for machine learning models, allowing to build model serving containers which expose well-defined APIs. In this post, I'll show how to create a simple model and how to deploy it with Seldon. The model is a customer segmentation one. The goal is to classify a customer according to a segment (0, 1 or 2), according to its age, income, whether they engaged with previous campaigns and the campaign type. Once we train the model, we deploy it with Seldon in a container orchestration platform such as Kubernetes and OpenShift. Create data We use the Python's scikit-learn to train our model. However, we must first simulate some data to train it. We start by simulating the users age (\\(a\\)) and income (\\(c\\)). We assume income is correlated with age. \\[ c|a \\sim \\mathcal{N}\\left(a + 20, 100\\right) \\\\\\\\ a|k \\sim \\mathcal{U}\\left(A_k, B_k\\right),\\quad A=\\left\\lbrace16, 25, 50, 61\\right\\rbrace,B=\\left\\lbrace24, 49, 60, 90\\right\\rbrace \\\\\\\\ k \\sim \\mathcal{M}\\left(4, \\left\\lbrace 0.15, 0.4, 0.2, 0.25\\right\\rbrace\\right) \\] Let's assume we have eight distinct events (\\(e=\\left(0, 1, \\dots, 7\\right)\\)). We sample them from a multinomial distribution and also assume that two different age bands have different distributions, just to add some variation. \\[ e = \\begin{cases} \\mathcal{M}\\left(7, \\left\\lbrace 0.026, 0.195, 0.156, 0.208, 0.130, 0.205, 0.078 \\right\\rbrace\\right) & \\text{if}\\ a < 50 \\\\\\\\ \\mathcal{M}\\left(7, \\left\\lbrace 0.052, 0.143, 0.169, 0.182, 0.164, 0.182, 0.104 \\right\\rbrace\\right) & \\text{if}\\ a \\geq 50 \\end{cases} \\] The responses are calculated as 0 or 1, representing \"true\" or \"false\", and sampled from Bernoulli distributions, with different distributions depending on the event, again just to add some variation. \\[ r = \\begin{cases} \\text{Bernoulli}\\left(0.6\\right) & \\text{if}\\ e \\in \\left(2, 3, 4, 6\\right) \\\\\\\\ \\text{Bernoulli}\\left(0.4\\right) & \\text{if}\\ e \\in \\left(1, 5, 7\\right) \\end{cases} \\] To predict the response of a customer, we use a logistic model, with coefficients \\(\\beta_{age}=-0.0004\\) and \\(\\beta_{income}=0.0001\\). For the customer level, we use a negative binomial model with coefficients \\(\\beta_{age}=-0.0233\\) and \\(\\beta_{income}=0.0054\\). This results in the following distribution of customer levels: Finally, we create the response according to negative binomial model with coefficients \\(\\beta_{level}=0.1862\\) and \\(\\beta_{response}=0.2076\\). We get the following segments, stratified by age and income: Train model Now that we have our simulated data, we can train a model. Generally, it is straightforward to train model data when in pandas data frame format. Let's proceed with creating a data frame with the data we've just generated: import pandas as pd data = { 'age': age, 'income': income, 'class': _class, 'response': response, 'segment': segment, 'events': events } df = pd.DataFrame(data) We now create the training and testing datasets. The first thing is to define the classifier's inputs and outputs and then splitting each of them into training and testing. Here I have used a split of 60%/40% for training and testing respectively. from sklearn.model_selection import train_test_split cols = ['age', 'income', 'response', 'events'] inputs = df[cols] outputs = df['segment'] # split dataset X_train, X_test, y_train, y_test = \\ train_test_split(inputs, outputs, test_size=0.4, random_state=23) We use a Random Forest classifier as the underlying algorithm for our model. These are available in sciki-learn with the RandomForestClassifier class. However, scikit-learn does not support categorical variables out of the box 1. To deal with them, we build a Pipeline, which allows to chain multiple transformations to our data, including a categorical variable processor, such as OrdinalEncoder 2. We use DataFrameMapper to apply the encoder to the response and events columns and leave the remaining unchanged. from sklearn.ensemble import RandomForestClassifier from sklearn import preprocessing from sklearn.pipeline import Pipeline def build_RF_pipeline(inputs, outputs, rf=None): if not rf: rf = RandomForestClassifier() pipeline = Pipeline([ (\"mapper\", DataFrameMapper([ (['response', 'events'], preprocessing.OrdinalEncoder()), (['age', 'income'], None) ])), (\"classifier\", rf) ]) pipeline.fit(inputs, outputs) return pipeline The actual training involves a simple hyper-parameter estimation using RandomizedSearchCV. This method performs a type of parameter grid search but restricting the search to only the specified values. For the scope of this post, it is not necessary to perform an exhaustive hyperparameter estimation. The RF_estimation function returns the best-fitted model after searching with the test dataset. def RF_estimation(inputs, outputs, estimator_steps=10, depth_steps=10, min_samples_split=None, min_samples_leaf=None): # hyper-parameter estimation n_estimators = [int(x) for x in np.linspace(start=50, stop=100, num=estimator_steps)] max_depth = [int(x) for x in np.linspace(3, 10, num=depth_steps)] max_depth.append(None) if not min_samples_split: min_samples_split = [1, 2, 4] if not min_samples_leaf: min_samples_leaf = [1, 2, 4] bootstrap = [True, False] random_grid = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap} rf_random = RandomizedSearchCV( estimator=RandomForestClassifier(), param_distributions=random_grid, n_iter=100, scoring='neg_mean_absolute_error', cv=3, verbose=1, random_state=42, n_jobs=-1) rf_random.fit(inputs, outputs) best_random = rf_random.best_estimator_ return best_random After applying the parameter estimation, we take the best scoring model and calculate the MSE. Unsurprisingly (given the simple model and simulated data), we get a very good fit. rf_predictions = random_forest_pipeline.predict(X_test) print(f\"MSE: {random_forest_pipeline.score(X_test, y_test)*100}%\") # MSE: 99.95% The final step is serialising the model. Serialisation is necessary since we only serve the pre-trained model. To do so, we use the joblib library and save the model to a model.pkl file. import joblib #save mode in filesystem joblib.dump(random_forest_pipeline, 'model.pkl') Deploy model It is important to note that we don't need the model training code included in the Seldon server. The purpose of Seldon is not to train models, but to deploy them and manage their lifecycle. This workflow means that a typical Seldon deployment would only include the prediction endpoint implementation and a serialised model. This provision is made by firstly create a wrapper for our model which implements the Seldon endpoints. Simple model We create a Python script called Model.py 3. The primary prediction endpoint uses the following signature: def predict(self, X: np.ndarray, names: Iterable[str], meta: Dict = None) The wrapper is straightforward, in this example. We use the joblib library again, to load the serialised model model.pkl, and then pass through any JSON payload as inputs (X) to the model to get a prediction as well as using Python's default logging to provide some feedback. import joblib import logging class Model(object): def __init__(self): logger.info(\"Initializing.\") logger.info(\"Loading model.\") self.model = joblib.load('model.pkl') def predict(self, X, features_names): return self.model.predict_proba(X) We now build the model using the s2i (source-to-image). As the name implies, s2i's allow to create a container image from source code, taking care of any necessary intermediate steps. Seldon support several types of builds (such as Python, R and Java) 4. Typically s2i's rely on certain conventions (over configuration) on your application structure. A requirement when building a Seldon model using its s2i is to provide some specific environment variables. These are usually stored in a file located in $REPO/.s2i/environment. For instance, for this model we use: MODEL_NAME=Model API_TYPE=REST SERVICE_TYPE=MODEL PERSISTENCE=0 The MODEL_NAME corresponds to the script we've created previously, Model.py and instructs Seldon to use it as the REST endpoint provider. API_TYPE defines the endpoint interface. We use the REST interface, other possibilities include gRPC, for instance. To build the container image using the s2i, assuming you want an image named \\(NAME` and tagged with `\\)TAG, we simply need to run: $ s2i build $REPO \\ seldonio/seldon-core-s2i-python36:0.18 \\ $NAME:$TAG You can provide the location of your source code either by specifying a remote Git repository or by passing a local one. Once the container image builds, you can now run it using, for instance: docker run -i --rm -p 5000:5000 $NAME:$TAG Let's get a prediction from the model: $ curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"data\":{\"ndarray\":[34.0, 100.0, 1, 2](/34.0,-100.0,-1,-2.html)}}' \\ http://localhost:5000/predict This will return a prediction: { \"data\": { \"names\": [\"t:0\",\"t:1\",\"t:2\"], \"ndarray\": [0.0,0.9980208571211083,0.00197914287889168](/0.0,0.9980208571211083,0.00197914287889168.html)}, \"meta\": {} } This response corresponds to the probability of each segment (0, 1 and 2), respectively. We can see that a customer with this profile is classified as a segment 1 with an associated probability of 99.8%. With metrics Seldon provides basic metrics by default, covering service, predictor and model name, version and image. However, you can directly add custom metrics. Going back to our Model wrapper class, we add a new method called metrics which returns custom metrics. The metrics are compatible with Prometheus and, therefore, the metric type should be familiar if you have dealt with Prometheus before. These include, for instance: Counters Gauges Timers Let's add to the wrapper: import joblib import logging class Model(object): def __init__(self): logger.info(\"Initializing.\") logger.info(\"Loading model.\") self.model = joblib.load('model.pkl') def predict(self, X, features_names): return self.model.predict_proba(X) # new custom metrics endpoint def metrics(self): return [ # a counter which will increase by the given value {\"type\": \"COUNTER\", \"key\": \"mycounter\", \"value\": 1}, # a gauge which will be set to given value {\"type\": \"GAUGE\", \"key\": \"mygauge\", \"value\": 10}, # a timer which will add sum and count metrics - assumed millisecs {\"type\": \"TIMER\", \"key\": \"mytimer\", \"value\": 1.1}, ] If we now request a new prediction, as previously, we can see the custom metrics included in the model's response. { \"data\": { \"names\": [\"t:0\",\"t:1\",\"t:2\"], \"ndarray\":[0.0,0.9980208571211083,0.00197914287889168](/0.0,0.9980208571211083,0.00197914287889168.html)}, \"meta\": { \"metrics\": [ {\"key\":\"mycounter\",\"type\":\"COUNTER\",\"value\":1}, {\"key\":\"mygauge\",\"type\":\"GAUGE\",\"value\":10}, {\"key\":\"mytimer\",\"type\":\"TIMER\",\"value\":1.1}] } } These values are available via the Prometheus endpoint. The model can also be easily deployed in a container platform, for instance, OpenShift. Assuming you are logged to a cluster and your image is a registry accessible by OpenShift, you can simply deploy it using: $ oc new-app $NAME:$TAG I hope this was useful to you. Happy coding! As of the time of writing. \u21a9 Other encoders are available in scikit-learn. I recommend you experiment with some of them. \u21a9 You can use any file name, as long as it's consistent with .s2i/environment, which we'll look at soon. \u21a9 More information can be found here. \u21a9 ");
docs.push({"id": 20, "title": "Serving models with Seldon", "url": "/serving-models-with-seldon.html"});
index.add(21, "Brutalist web design The major guidelines of the Brutalist web design1 are: Content is readable on all reasonable screens and devices. Only hyperlinks and buttons respond to clicks. Hyperlinks are underlined and buttons look like buttons. The back button works as expected. View content by scrolling. Decoration when needed and no unrelated content. Performance is a feature. https://brutalist-web.design/ \u21a9 ");
docs.push({"id": 21, "title": "Brutalist Web Design", "url": "/brutalist-web-design.html"});
index.add(22, "Workflow");
docs.push({"id": 22, "title": "Workflow", "url": "/workflow.html"});
index.add(23, "MCMC performance on Substrate VM Recently I've been following (but not very closely, I admit) the development of the GraalVM project. The project has many interesting goals (such as Project Metropolis, increased JIT performance and others). However, having dabbled with projects such as Scala native and Kotlin native, one of the aspects of GraalVM that caught my attention was the SubstrateVM, which allegedly allows for a simple, straight-forward compilation of any Java bytecode into a native binary. I specifically wanted to compare the performance and memory consumption of simple scientific computing tasks when using the JVM and native executables. To do this, I picked two simple numerical simulations in the form of toy Gibbs samplers, in order to keep the cores busy for a while. Binomial-Beta case The first problem chosen was the one of sampling from a Beta-Binomial distribution where we have \\[ X \\sim \\text{Binom}\\left(n,\\theta\\right) \\\\\\\\ \\theta \\sim \\text{B}\\left(a,b\\right). \\] Since we know that \\[ \\pi\\left(\\theta|x\\right) \\propto \\theta^x \\left(1-\\theta\\right)^{n-x}\\theta^{a-1}\\left(1-\\theta\\right)^{b-1}, \\] We calculate the joint density \\[ p(x,\\theta) = \\begin{pmatrix} n \\\\\\\\ x \\end{pmatrix} \\theta^x \\left(1-\\theta\\right)^{n-x}\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}\\left(1-\\theta\\right)^{b-1} \\] The marginal distribution is a Binomial-Beta: \\[ p\\left(x\\right)=\\begin{pmatrix} n \\\\\\\\ x \\end{pmatrix}\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma(a)\\Gamma(b)}\\frac{\\Gamma\\left(a+b\\right)\\Gamma\\left(b+n-x\\right)}{\\Gamma\\left(a+b+n\\right)},\\qquad x=0,1,\\cdots,n. \\] The code for this simulation is available here. The project is setup so that Maven produces an assembly Jar file, since I've found that to be the easier artifact we can offer to the GraalVM's native compiler. To enable assembly Jars we add the maven-assembly-plugin to pom.xml and specify a main class. The assembly can then be produced simply by executing mvn package An assembly Jar should be available in the target folder and named benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar. Both the Jar and the native executable allow to specify how many iterations the Gibbs sampler should run for (as well as the thinning factor). If nothing is specified, the default will be used, which is \\(50000\\) iterations thinned by \\(100\\). This particular Gibbs sampler was implemented in two variants. One variant stores the samples draws of \\(x\\) and \\(\\theta\\) in arrays double[] while the other one simply calculates the Gibbs steps by using the previous value, that is \\(x_i=f(x_{i-1},\\theta_{i-1})\\) and then discarding the previous values. The latter has a constant memory cost in \\(\\mathcal{O}(1)\\) in terms of number of iterations, while the former clearly doesn't. We can them proceed with the first test, first benchmarking it under the JVM by running (for both sample history variants): $ /usr/bin/time -v java -jar target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar store 50000 100 $ /usr/bin/time -v java -jar target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar nostore 50000 100 (It is important to note that the time command is the executable under /usr/bin and not your shell's builtin.) The next step is to build the native image using GraalVM's compiler. This is also quite straight-forward and simply a matter of calling: $GRAALVM_BIN/native-image target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar where $GRAALVM_BIN is simply the location where you installed the GraalVM binaries. If the compilation is successful, you should see some information about the compilation steps, such as parsing, inlining, compiling and writing the image. Finally, if using the default, you should have a native executable available in your current directory. Again, the benchmark command is similar to the JVM step, that is: $ /usr/bin/time -v ./benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies store 50000 100 $ /usr/bin/time -v ./benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies nostore 50000 100 The results from the runs which saved the sampling history on both platforms (JVM and native) were consistent as we can see from the plots below: The (peak) memory consumption and execution time for each version is presented in the table below: Time(s) Peak (Mb) JVM (no sample history) 110.09 320.913 native (no sample history) 130.52 273.747 JVM (sample history) 112.51 324.796 native (sample history) 130.62 274.239 Another bivariate case The second problem chosen is another bivariate model, previously detailed in this blog. The code is included in the same repositoty as the Beta-Binomial case and the setup for the benchmarks is similar. The only step needed to run this example is to change the main class in the assemply plugin section of the pom.xml from BinomialBeta to Bivariate. The benchmark results are in the table below: Time(s) Peak (Mb) JVM 106.92 176.541 native 121.29 273.383 Now, in this case, the results are much more interesting. The JVM version outperforms the native version in both execution time and memory consumption. I don't have an explanation for this, but if you think you have (or have any other questions) please let me know on Mastodon or Twitter. Thanks for reading!");
docs.push({"id": 23, "title": "MCMC performance on Substrate VM", "url": "/mcmc-performance-on-substrate-vm.html"});
index.add(24, "gulp I have been working in a new library called gulp which you can find on https://github.com/ruivieira/gulp. On the project's page there are some usage examples but I will try to summarise the main points here. The purpose of this library is to facilitate the parallel development of R and Java code, using rJava as the bridge. Creating bindings in rJava is quite simple, the tricky part of the process (in my opinion) being the maintenance of the bindings (usually done by hand) when refactoring your code. As an example, let's assume you have the following Java class: @ExportClassReference(value=\"test\") public class Test { // Java code } That you wish to call from R.");
docs.push({"id": 24, "title": "gulp", "url": "/gulp.html"});
index.add(25, "Containerised Streaming Data Generation using State-Space Models To prototype and test almost any application some type of input data is needed. Getting the right data can be difficult for several reasons, including strict licenses, a considerable amount of data engineering to shape the data to our requirements and the setup of dedicated data producers. Additionally, in modern applications, we are often interested in realtime/streaming and distributed processing of data with platforms such as Apache Kafka and Apache Spark and deployment in a cloud environment like OpenShift with tools such as oshinko. Simulating data is not trivial, since we might want to capture complex characteristic to evaluate our algorithms in conditions similar to the real world. In this post I'll introduce a tool, timeseries-mock, which allows for a simple, containerised deployment of a data simulator along with some of the theory behind the data generation. State-space models A common way of modelling these patterns is to use state-space models (SSM). SSMs can be divided into a model and a observation structure. \\[ Y_t|\\theta_t,\\Phi \\sim f\\left(y_t|\\theta_t,\\Phi_t\\right) \\\\ \\theta_t|\\theta_{t-1},\\Phi_t \\sim g\\left(\\theta_t|\\theta_{t-1},\\Phi_t\\right). \\] It is clear from the above that the state possesses a Markovian nature. The state at time \\(t\\), \\(\\theta_t\\) will on depend on the previous value, \\(\\theta_{t-1}\\) and an observation at time \\(t\\), \\(y_t\\) will only depend on the current state, \\(\\theta_t\\), that is: \\[ p\\left(\\theta_{t}|\\theta_{0:t-1},y_{0:t-1}\\right)=p\\left(\\theta_{t}|\\theta_{t-1}\\right) \\\\ p\\left(\\theta_{t-1}|\\theta_{t:T},y_{t:T}\\right)=p\\left(\\theta_{t-1}|\\theta_{t}\\right) \\\\ p\\left(y_{t}|\\theta_{0:t},y_{0:t-1}\\right)=p\\left(y_{t}|\\theta_{t}\\right). \\] In this post we will focus on a specific instance of SSMs, namely Dynamic Generalised Linear Models (DGLMs). If you want a deeper theoretical analysis of DGLMs I strongly recommend Mike West and Jeff Harrison's \"Bayesian Forecasting and Dynamic Models\" (1997). In DGLMs, the observation follows a distribution from the exponential family, \\(E\\left(\\cdot\\right)\\) such, that \\[ Y_t|\\theta_t,\\Phi \\sim E\\left(\\eta_t,\\Phi\\right) \\\\ \\eta_t|\\theta_t = L\\left(\\mathsf{F}^T \\theta_t\\right) \\] where \\(L\\left(\\cdot\\right)\\) is the linear predictor and the state evolves according to a multivariate normal (MVN) distribution: \\[ \\theta_t \\sim \\mathcal{N}\\left(\\theta_t;\\mathsf{G}\\theta_{t-1},\\mathsf{W}\\right) \\] Structure The fundamental way in which timeseries-mock works is by specifying the underlying structure and observational model in a YAML configuration file. In the following sections we will look at the options available in terms of structural and observational components and look at how to represent them. As we've seen from (5), the structure will allows us to define the underlying patterns of the state evolution \\(\\lbrace \\theta_1, \\theta_2, \\cdots, \\theta_t\\rbrace\\). One of the advantages of DGLMs is the ability to compose several simpler components into a single complex structure. We will then look at some of these \"fundamental\" components. Mean An underlying mean component will represent a random-walk scalar state which can be specified in the configuration file by structure: - type: mean start: 0.0 noise: 1.5 In this case start will correspond the mean of the state prior, \\(m_0\\), and noise will correspond to the prior's variance, \\(\\tau^2\\), that is \\[ \\theta_0 \\sim \\mathcal{N}\\left(m_0, \\tau^2\\right). \\] In the figure below we can see the above configuration for, respectively, a higher and lower value of noise. Seasonality Seasonality is represented by Fourier components. A Fourier component can be completely specified by providing the period, start, noise and harmonics. The start and noise parameters are analogous to the mean components we saw previously. The period parameter refers to how long does it take for the cyclical pattern to repeat. This is done relatively to your time-point interval, such that \\[ P = p_{\\text{fourier}}\\cdot p_{\\text{stream}}. \\] That is, if your stream's rate is one observation every 100 milliseconds, \\(p_{\\text{stream}}=0.1\\), and the harmonic's period is 2000, \\(p_{\\text{fourier}}=1000\\), then the seasonal component will repeat every \\(200\\) seconds. The configuration example structure: - type: season period: 200 harmonics: 5 start: 0.0 noise: 0.7 will create a sequence of state vectors \\(\\boldsymbol{\\theta}_{0:T}\\) with five components, such that: \\[ \\boldsymbol{\\theta}_t = \\lbrace\\theta_{1,t},\\cdots,\\theta_{5,t}\\rbrace. \\] In this example, period refers to the number of time-points for each cycle's repetition and harmonics to the number of Fourier harmonics used. \"Simpler\" cyclic patterns usually require less harmonics. In the figure below we show on the lowest and highest frequency harmonics, on the left and right respectively. AR-p An AR(\\(p\\)) (Auto-Regressive) component can be specified using the directives: structure: - type: arma start: 0.0 coefficients: 0.1,0.3,0.15 noise: 0.5 In the above example we would be creating an AR(3) component, with respective coefficients \\(\\phi=\\lbrace 0.1,0.3,0.15 \\rbrace\\). These coefficients will take part of the state model as \\[ \\mathsf{G} = \\begin{bmatrix} \\phi_1 & \\phi_2 & \\cdots & \\phi_{p-1} & \\phi_p \\\\ 1 & 0 & \\cdots & 0 & 0 \\\\ \\vdots & & \\ddots & \\vdots & \\vdots \\\\ 0 & \\cdots & 0 & 1 & 0 \\end{bmatrix} \\] In the following plots we show respectively the first and second component of the AR(3) state vector. Composing Structural composition of DGLM structures amounts to the individual composition of the state covariance matrix and state/observational evolution matrices such that: \\[ \\mathsf{F}^T = \\begin{bmatrix}\\mathsf{F}_1 &amp; \\mathsf{F}_2 &amp; \\dots \\mathsf{F}_i\\end{bmatrix}^T \\\\ \\mathsf{G} = \\text{blockdiag}\\left(\\mathsf{G}_1, \\mathsf{G}_2, \\dots, \\mathsf{G}_i\\right) \\\\ \\mathsf{W} = \\text{blockdiag}\\left(\\mathsf{W}_1, \\mathsf{W}_2, \\dots, \\mathsf{W}_i\\right) \\] To express the composition of structures in the YAML configuration, we simply enumerate the separate components under the structure key. As as example, to compose the previous mean and seasonal components, we would simply write: structure: - type: mean start: 0.0 noise: 1.5 - type: season period: 200 harmonics: 5 start: 0.0 noise: 0.7 This would create a structure containing both an underlying mean and a seasonal component. Observations As we have seen from (3) that an observational model can be coupled with a structure to complete the DGLM specification. In the following sections we will look at some example observational models and in which situations they might be useful. Continuous Continuous observations are useful to model real valued data such as stock prices, temperature readings, etc. This can be achieved by specifying the observational component as a Gaussian distribution such that: \\[ Y_t|\\Phi \\sim \\mathcal{N}\\left(y_t|\\eta_t, \\mathsf{W}\\right). \\] observations: - type: continuous noise: 1.5 The following plot shows the coupling of the structure used in the mean section with the continuous (example above) observational model. Discrete Discrete observations, sometimes referred as \"count data\", can be used to model integer quantities. This can be achieved by using a Poisson distribution in the observational model, such that: \\[ Y_t|\\Phi \\sim \\text{Po}\\left(y_t|\\eta_t\\right) \\] An example configuration would be: observations: - type: discrete In this case we will use the previous ARMA(3) structure example and couple it with a discrete observational model. The result is shown in the plot below: Categorical In the categorical case, we model the observations according to a binomial distribution, such that \\[ Y_t \\sim \\text{Bin}\\left(y_t|\\eta_t,r\\right), \\] where \\(r\\) represents the number of categories. A typical example would be the case where \\(r=1\\) which would represent a binary outcome (0 or 1). The following configuration implements this very case: observations: - type: categorical categories: 1 We can see in the plot below (states on the left, observations on the right) a realisation of this stream, when using the previous seasonal example structure. Often, when simulating a data stream, we might be interested in the category labels themselves, rather than a numerical value. The generator allows to pass directly a list of labels and output the labelled observations. Let's assume we wanted to generate a stream of random DNA nucleotides (C, T, A and G). The generator allows to pass the labels directly and output the direct mapping between observations and label, that is: \\[ y_t: \\lbrace 0, 1, 2, 3 \\rbrace \\mapsto \\lbrace\\text{C, T, A, G}\\rbrace \\] observations: - type: categorical values: C,T,A,G Using the same seasonal structure and observation model as above, the output would then be: Composite model In a real-world scenario we are interested in simulating multivariate data and that comprises of different observational models. For instance, combining observation components from categorical, continuous, etc. The approach taken for multivariate composite models, is that the structures are composed as seen previously into a single one and the resulting state vector is then \"collapsed\" into a vector on natural parameters, \\(\\eta_t\\) which are then used to sample the individual observation components. \\[ \\theta_t = \\lbrace\\underbrace{\\theta_{1}, \\theta_{2}, \\theta_{3}}_{\\eta_1}, \\underbrace{\\theta_{4}, \\theta_{5}, \\theta_{6}}_{\\eta_2}\\rbrace \\\\ y = f(\\eta_t) \\\\ = \\lbrace f_1(\\eta_1), f_2(\\eta_2)\\rbrace \\] The model composition can be expressed by grouping the different structures and observations under a compose key: compose: - structure: # component 1 - type: mean start: 0.0 noise: 0.5 - observations: - type: continuous noise: 0.5 - structure: # component 2 - type: mean start: 5.0 noise: 3.7 - observations: - type: continuous noise: 1.5 Examples We will look at two separate examples, one that creates a stream of simulated stock prices and one that generates a fake HTTP log. We assume we want to simulate a stream of per-day stock prices for 3 different companies, each with different characteristics. In this case, we will model the following: Company A's stocks start at quite a high value ($700) and are quite stable throughout time Company B's stocks start slightly lower than A ($500) and are quite stable in the long run, but show heavy fluctuation from day to day Company C's stocks start at $600 are very unpredictable Since we will be using per-day data we won't be streaming this in realtime! We can map each daily observation to a second in our stream so we will specify a period=1. All stocks will exhibit a small monthly effect (period=30), which will be indicated by a noise=0.01 and a yearly effect (period=365) with a noise=2.5. The resulting configuration will be: compose: - structure: # company A - type: mean start: 700 noise: 0.01 # low structural variance - type: season period: 30 # monthly seasonality noise: 0.1 - type: season period: 365. # yearly seasonality noise: 1.7 - observations: - type: continuous noise: 0.05 # low observational variance - structure: # company B - type: mean start: 500 noise: 0.01 # low structural variance - type: season period: 30 # monthly seasonality noise: 0.7 - type: season period: 365. # yearly seasonality noise: 3.7 - observations: - type: continuous noise: 3.00 # higher observational variance - structure: # company C - type: mean start: 600 noise: 3.0 # higher structural variance - type: season period: 30 # monthly seasonality noise: 0.1 - type: season period: 365. # yearly seasonality noise: 0.25 - observations: - type: continuous noise: 4.0 # higher observational variance A realisation of this stream looks like the figure below. To generate the fake HTTP log we will make the following assumptions: We will have a request type (GET, POST, PUT) which will vary following a random walk A set of visited pages which, for illustration purposes, will be limited to (/site/page.htm, /site/index.htm and /internal/example.htm). We also want that the URLs visited follow a seasonal pattern. An IP address in the IPv4 format (i.e. 0-255.0-255.0-255.0-255) It is clear that for all variables the appropriate observational model is the categorical one. For the request type and the visited page we can pass directly the category name in the configuration file and for the IP we simply need four categorical observations with \\(r=255\\). If the underlying structure is the same, a useful shortcut to specify several observation component is the replicate key. In this particular example to generate four 0-255 numbers with an underlying mean as the structure, we simple use: - replicate: 4 structure: - type: mean start: 0.0 noise: 2.1 observations: type: categorical categories: 255 The full configuration for the HTTP log simulation could then be something like this: compose: - structure: - type: mean start: 0.0 noise: 0.01 observations: type: categorical values: GET,POST,PUT - structure: - type: mean start: 0.0 noise: 0.01 - type: season start: 1.0 period: 15 noise: 0.2 observations: type: categorical values: /site/page.htm,/site/index.htm,/internal/example.htm - replicate: 4 structure: - type: mean start: 0.0 noise: 2.1 observations: type: categorical categories: 255 [\"PUT\", \"/internal/example.htm\", 171, 158, 59, 89] [\"GET\", \"/internal/example.htm\", 171, 253, 71, 146] [\"PUT\", \"/internal/example.htm\", 224, 252, 9, 156] [\"POST\", \"/site/index.htm\", 143, 253, 6, 126] [\"POST\", \"/site/page.htm\", 238, 254, 2, 48] [\"GET\", \"/site/page.htm\", 228, 252, 52, 126] [\"POST\", \"/internal/example.htm\", 229, 234, 103, 233] [\"GET\", \"/internal/example.htm\", 185, 221, 109, 195] ... Setting up the generator As I have mentioned in the beginning of this post, we want to fit the data simulation solution into a cloud computing workflow. To illustrate this we will use the OpenShift platform which allows for the deployment of containerised applications. A typical setup for a streaming data processing application would be as illustrated in the figure below. We have several sources connected to a message broker, such as Apache Kafka in this case. Data might be partitioned into \"topics\" which are then consumed by different applications, each performing data processing, either independently or in a distributed manner. An advantage of timeseries-mock would then be to replace the \"real\" data sources with a highly configurable simulator either for the prototyping or initial testing phase. If we consider our previous example of the \"fake\" HTTP log generation, an application for Web log analytics could be prototyped and tested with simulated log data very quickly, without being blocked by the lack of suitable real data. Since the data is consumed by proxy via the message broker's topics, we could later on replace the simulator with real data sources seamlessly without an impact on any of the applications. To setup the generator (and assuming Kafka and your consumer application are already running on OpenShift) we only need to perform two steps: Write the data specifications in a YAML configuration Use the s2i to deploy the simulator The s2i functionality of OpenShift allows to create deployment ready images by simply pointing to a source code location. In this case we could simply write: oc new-app centos/python-36-centos7~https://github.com/ruivieira/timeseries-mock \\ -e KAFKA_BROKERS=kafka:9092 \\ -e KAFKA_TOPIC=example \\ -e CONF=examples/mean_continuous.yml \\ --name=emitter In this case, we would deploy a simulator generating data according to the specifications in mean_continuous.yml. This data will be sent to the topic example of a Kafka broker running on port 9092. The stream will be ready to consume and message payload will a stream of serialised JSON strings. In the case of the simulated HTTP log this would be: { name: \"HTTP log\" values: [\"GET\", \"/internal/example.htm\", 185, 221, 109, 195] } name - the name given to this stream in the configuration file values - a single observation for this stream After consuming the data it is straight-forward to do any post-processing if needed. For instance, the values above could be easily transformed into a standard Apache Web server log line. I hope you found this tool useful, simple to use and configure. Some future work includes adding more observations distributions beyond the exponential family and the ability to directly add transformation rules to the generated observations. If you have any suggestions, use cases (or found an issue!), please let me know in the repository. If you have any comments please let me know on Mastodon (or Twitter). Happy coding!");
docs.push({"id": 25, "title": "Containerised Streaming Data Generation using State-Space Models", "url": "/containerised-streaming-data-generation-using-state-space-models.html"});
index.add(26, "Topics Counterfactuals Counterfactuals with Constraint Solvers Fairness Counterfactual Fairness Resources A nice presentation on AI/ML explainability: https://explainml-tutorial.github.io/neurips20 ");
docs.push({"id": 26, "title": "Explainability", "url": "/explainability.html"});
index.add(27, "A Gibbs Sampler in Crystal Recently, I've been following with interest the development of the Crystal language. Crystal is a statically type language with a syntax resembling Ruby's. The main features which drawn me to it were its simple boilerplate-free syntax (which is ideal for quick prototyping), tied with the ability to compile directly to native code along with a dead simple way of creating bindings to existing C code. These features make it quite attractive, in my opinion, for scientific computing. To test it against more popular languages, I've decided to run the Gibbs sampling examples created in Darren Wilkinson's blog. I recommend reading this post, and in fact, if you are interested in Mathematics and scientific computing in general, I strongly recommend you follow the blog. As explained in the linked post, I will make a Gibbs sampler for \\[ f\\left(x,y\\right)=kx^2\\exp\\left\\lbrace-xy^2-y^2+2y-4x\\right\\rbrace \\] with \\[ \\begin{aligned} x|y &\\sim Ga\\left(3,y^2+4\\right) \\\\\\\\ y|x &\\sim N\\left(\\frac{1}{1+x},\\frac{1}{2\\left(1+x\\right)}\\right) \\end{aligned} \\] The original examples were ran again, without any code alterations. I've just added the Crystal version. This implementation uses a very simple wrapper I wrote to the famous GNU Scientific Language (GSL). require \"../libs/gsl/statistics.cr\" require \"math\" def gibbs(n : Int = 50000, thin : Int = 1000) x = 0.0 y = 0.0 puts \"Iter x y\" (0..n).each do |i| (0..thin).each do |j| x = Statistics::Gamma.sample(3.0, y\\*y+4.0) y = Statistics::Normal.sample(1.0/(x+1.0), 1.0/Math.sqrt(2.0\\*x+2.0)) end puts \"#{i} #{x} #{y}\" end end gibbs (As you can see, the Crystal code is quite similar to the Python one). To make sure it's a fair comparison, I ran it in compiled (and optimised) mode build using $ crystal build gibbs.cr --release $ time ./gibbs > gibbs_crystal.csv Looking at the results, you can see that they are consistent with the other implementations: The timings for each of the different versions (ran in a 1.7 GHz Intel Core i7 Macbook Air) were Language Time (s) R 364.8 Python 144.0 Scala 9.896 Crystal 5.171 C 5.038 So there you have it. A Ruby-like language which can easily compete with C performance-wise. I sincerely hope that Crystal gets some traction in the scientific community. That of course won't depend solely on its merits but rather on an active community along with a strong library ecosystem. This is lacking at the moment, simply because it is relatively new language with the specs and standard library still being finalised.");
docs.push({"id": 27, "title": "A Gibbs Sampler in Crystal", "url": "/a-gibbs-sampler-in-crystal.html"});
index.add(28, "Counterfactuals with constraint solvers Scoring An implementation on how to calculate counterfactuals with Constraint Solvers (namely OptaPlanner) is available here. This implementation satisfies several criteria of the counterfactuals desiderata. The penalisation score is represented with a BendableBigDecimalScore1, having three \"hard\" levels and one \"soft\" level. The first hard level component, 1, penalises the score according to the distance between the prediction, \\(y^{\\prime}\\) for the currently proposed solution, \\(x^{\\prime}\\) and the original prediction \\(y\\), that is this our \\((\\hat{f}(x^{\\prime})-y^{\\prime})^2\\). The actionability is score with 2. This component penalises the score according to number of immutable features which were changed in the counterfactual. A confidence score component, 3 is use to, optionally, impose a minimum confidence threshold to the counterfactual's associated prediction, \\(x^{\\prime}\\). Finally, the feature distance, 4, penalises the score according to the feature distance. This is the representation of \\[ d(x, x^{\\prime}). \\] In the concrete implementation linked above, the distance, \\(d\\), chosen is a Manhattan (or \\(L^1\\)) distance calculated feature-wise. Implementation Entities are defined by classes such as Integer, Categorical, Boolean or Float, as shown in 5. Each of the features, shown in 6, is created as an instance of one of these entities. For instance, feature1 would be of type Integer and feature2 would be of type Categorical, etc. The original data point, \\(x\\) is represented by this set of features (6). A planning solution (PlanningSolution), illustrated in 7 will produce candidate solutions (shown in 8) For each solution, we propose a new set of features (\\(x^{\\prime}\\)) as a counterfactual candidate. For instance, solution A in 8. In the following section we will look at how each component is calculated. We will refer to each \"hard\" level component as \\(H_1, H_2\\) and \\(H_3\\) and the \"soft\" component as \\(S_1\\). The overal score consists, then, of \\(S=\\{H_1, H_2, H_3, S_1 \\}\\) Prediction distance The first component of the score, 1 is established by sending the proposed counterfactual \\(x^{\\prime}\\), 8 to a predictive model, 9 and calculating the distance between the desired outcome, \\(y^{\\prime}\\) and the model's prediction. This is done component wise, for each feature of the output. That is, for a prediction with \\(N\\) features, we calculate \\[ H_1=\\left(\\sum_i^Nf(x^{\\prime}_i) - y^{\\prime}_i\\right)^2 \\] Actionability score For the second component, the actionability score, 2. We calculate the number of features for the protected set \\(\\mathcal{A}\\), which have a different value from the original. That is, assuming we have a certain number of protectd features \\(M\\), such that \\(\\mathcal{A}=\\{A_1,A_2,\\dots,A_M\\}\\), we calculate: \\[ H_2 = \\sum_{a \\in \\mathcal{A}} \\mathbb{1}(x_a \\neq x^{\\prime}_a), \\] Confidence score For each feature \\(i\\), if we have a prediction confidence, \\(p_i(f(x^{\\prime}))\\), we calculate the number of features which have a confidence below a certain predefined threshold, \\(P_i\\). If the threshold is not defined, this component will always be zero and not influence the counterfactual selection. Assuming we have defined a threshold for all \\(N\\) features, \\(P = \\{P_1, P_2, \\dots, P_N\\}\\) we calculate this score as \\[ H_3 = \\sum_i^N \\mathbb{1} \\left( p_i \\left( f(x^{\\prime}) < P_i \\right) \\right) \\] Feature distance c.f. https://docs.optaplanner.org/8.0.0.Final/optaplanner-javadoc/org/optaplanner/core/api/score/buildin/bendablebigdecimal/BendableBigDecimalScore.html \u21a9 ");
docs.push({"id": 28, "title": "Counterfactuals with Constraint Solvers", "url": "/counterfactuals-with-constraint-solvers.html"});
index.add(29, "Java consumer Introduction Introduced in Java 8, the Consumer interface aims at providing additional functional programming capabilities for Java. Consumer defined functions do not return any value. Let's look at an example: import java.util.ArrayList; import java.util.LinkedList; import java.util.List; import java.util.function.Consumer; Consumer<String> say = a -> System.out.println(\"Hello, \" + a + \"!\"); say.accept(\"World\"); Hello, World! Consumer functions can also modify reference objects. For instance: List<Double> numbers = new ArrayList<Double>(); numbers.add(1d); numbers.add(2d); numbers.add(3d); Consumer<List<Double>> square = list -> { for (int i = 0; i < list.size(); i++) { double x = list.get(i); list.set(i, x*x); }; }; System.out.println(numbers); square.accept(numbers); System.out.println(numbers); [1.0, 2.0, 3.0] [1.0, 4.0, 9.0] ");
docs.push({"id": 29, "title": "Java consumer", "url": "/java-consumer.html"});
index.add(30, "Vim keys Copy paste Press v to select characters, or uppercase V to select whole lines y to copy Press P to paste before the cursor, or p to paste after ");
docs.push({"id": 30, "title": "Vim keys", "url": "/vim-keys.html"});
index.add(31, "Introduction to Isolation Forests Isolation Forests (IFs), presented in Liu1 et. al (2012), are a popular algorithm used for outlier classification. In a very simplified way, the method consists of building an ensemble of Isolation Trees (ITs) for a given data set and observations are deemed anomalies if they have short adjusted average path lengths on the ITs. ITs, which will be covered shortly, have several properties in common with a fundamental data structure: the Binary Search Tree (BSTs). In a very simplified way, BSTs are a special instance of tree structures where keys are kept in such an order that a node search is performed by iteratively (or recursively) choosing a left or right branch based on a quantitative comparison (e.g. lesser or greater). Node insertion is performed by doing a tree search, using the method described previously, until reaching an external node, where the new node will be inserted. This allows for efficient node searches since, on average, half the tree will not be visited. To illustrate this assume the values \\(x=[1, 10, 2, 4, 3, 5, 26, 9, 7, 54]\\) and the respective insertion on a BST. The intermediate steps would then be as shown below. One of the properties of BSTs is that, with randomly generated data, the path between the root node and the outliers will typically be shorter. We can see from the illustration below that, with our example data, the path length for (say) 7 is twice the length than for the suspicious value of 54. This property will play an important role in the IF algorithm, as we will see further on. Isolation Trees Since ITs are the fundamental component of IFs, we will start by describing their building process. We start by defining \\(t\\) as the number of trees in the IF, \\(\\mathcal{D}\\) as the training data (contained in an \\(n\\)-dimensional feature space, \\(\\mathcal{D} \\subset \\mathbb{R}^n\\)) and \\(\\psi\\) as the subsampling size. The building of a IT consists then in recursively partitioning the data \\(\\mathcal{D}\\) by sampling (without replacement) a subsample \\(\\mathcal{D}^{\\prime}\\) of size \\(\\psi\\). We then build an isolation tree \\(\\mathcal{T}^{\\prime}\\) with this subsample (in order to later add it to the isolation forest \\(\\mathcal{F}\\)) and the process is repeated \\(t\\) times. To build an isolation tree \\(\\mathcal{T}^{\\prime}\\) from the subsample we proceed as follows: if the data subsample \\(\\mathcal{D}^{\\prime}\\) is indivisible, a tree is returned containing a single external node corresponding to the feature dimensions, \\(n\\). If it can be divided, a series of steps must be performed. Namely, if we consider \\(Q = \\lbrace q_1,\\dots,q_n\\rbrace\\) as the list of features in \\(\\mathcal{D}^{\\prime}\\), we select a random feature \\(q \\in Q\\) and a random split point \\(p\\) such that \\[ \\min(q) < p < \\max(q), \\qquad q \\in Q. \\] Based on the cut-off point \\(p\\), we filter the features into a BST\u2019s left and right nodes according to \\[ \\mathcal{D}_l := \\lbrace \\mathcal{D}^{\\prime} : q \\in Q, q<p\\rbrace \\\\ \\mathcal{D}_r := \\lbrace \\mathcal{D}^{\\prime} : q \\in Q, q \\geq p\\rbrace, \\] and return an internal node having an isolation tree with left and right nodes as respectively \\(\\mathcal{D}_l\\) and \\(\\mathcal{D}_r\\). To illustrate this (and the general method of identifying anomalies in a two dimensional feature space, \\(x\\in\\mathbb{R}^2\\)) we will look at some simulated data and its processing. We start by simulating two clusters of data from a multivariate normal distribution, one centred in \\(x_a=[-10, 10]\\) and another centred in \\(x_b=[10, 10]\\), with a variance of \\(\\Sigma=\\text{diag}(2, 2)\\), that is \\[ X_a \\sim \\mathcal{N}\\left([-10, -10], \\text{diag}(2, 2)\\right) \\\\ X_b \\sim \\mathcal{N}\\left([10, 10], \\text{diag}(2, 2)\\right). \\] The particular realisation of this simulation looks like this: Below we illustrate the building of a single IT (given the data), illustrating the feature split point \\(p\\) and respective division of the feature list into left or right IT nodes. The process is conducted recursively until the feature list is no longer divisible. As mentioned previously, this process, the creation of an IT, is repeated \\(t\\) times in order to create the IF. In order to perform anomaly detection (e.g. observation scoring) we will then use the IT equivalent of the BST unsuccessful search heuristics. An external node termination in an IT is equivalent to a BST unsuccessful search. Given an observation \\(x\\), our goal is then to calculate the score for this observation, given our defined subsampling size, that is, \\(s(x,\\psi)\\). This technique amounts to partitioning the feature space randomly until feature points are \u201cisolated\u201d. Intuitively, points in high density regions will need more partitioning steps, whereas anomalies (by definition away from high density regions) will need fewer splits. Since the building of the ITs is performed in a randomised fashion and using a subsample of the data, this density predictor can be average over a number of ITs, the Isolation Forest. Intuitively, this could be done by calculating the average path length for our \\(\\mathcal{T}n, n=1,\\dots,t\\) ITs, \\(\\overline{h}(x)\\). However, as pointed in Liu1 et. al (2012), a problem with calculating this is that maximum possible height of each \\(\\mathcal{T}_n\\) grows as \\(\\mathcal{O}(\\log(\\psi))\\). To compare \\(h(x)\\) given different subsampling sizes, a normalisation factor, \\(c(\\psi)\\) must be established. This can be calculated by \\[ c(\\psi) = \\begin{cases} 2H(\\psi-1)-2\\frac{\\psi-1}{n},\\text{if}\\ \\psi >2,\\\\ 1, \\text{if}\\ \\psi=2,\\\\ 0, \\text{otherwise}, \\end{cases} \\] where \\(H(i)\\) is the harmonic number estimated by \\(H(i)\\approx\\log(i) + e\\). Denoting \\(h_{max}\\) as the tree height limit and e as the current path length, initialised as \\(e=0\\) we can then calculate \\(h(x)\\) recursively as: \\[ h(x,\\mathcal{T},h_{max},e) = \\begin{cases} h(x,\\mathcal{T}_{n,left},h_{max},e+1) \\text{if}\\ x_a < q_{\\mathcal{T}} \\\\ h(x,\\mathcal{T}_{n,right},h_{max},e+1) \\text{if}\\ x_a \\geq q_{\\mathcal{T}} \\\\ e+c(\\mathcal{T_{n,s}}) \\text{if}\\ \\mathcal{T} \\text{is a terminal node or}\\ e \\geq h_{max}. \\end{cases} \\] Given these quantities we can then, finally, calculate the anomaly score, \\(s\\) as \\[ s(x,\\psi) = 2^{-\\frac{\\text{E}[h(x)]}{c(\\psi)}} \\] with \\(\\text{E}[h(x)]\\) being the average \\(h(x)\\) for a collection of ITs. Parameters As mentioned in Liu1 et. al (2012), the empirical subsampling size \\(\\psi=2^8\\) is typically enough to perform anomaly detection in a wide range of data. Regarding the number of trees, \\(t\\) no considerable accuracy gain is usually observed with \\(t>100\\). In the plots below, we can see the score calculation for two point in our data, namely an outlier (\\(x_o=[3.10, -12.69])\\) and a normal observation (\\(x_n=[8.65, 9.71]\\)) with a varying number of trees and \\(\\psi=2^8\\) (left) and a varying subsample size and \\(t=100\\) (right). We can see that the score value stabilised quite early on when using \\(\\psi=2^8\\) and that very low subsampling sizes can lead to problems when classifying anomalies. Now that we know how to implement an IF algorithm and calculate an anomaly score, we will try to visualise the anomaly score distribution in the vicinity of the simulated data. To do so, we simply create a two dimensional lattice enclosing our data an iteratively calculate \\(s(\\cdot, \\psi)\\). The result is show below: The above steps fully define a naive isolation forest algorithm, which when applied to the previously simulated data, result in 88% of the anomalies being correctly identified. Thanks for reading! If you have any questions or comments, please let me know on Mastodon or Twitter. Liu, F. T., Ting, K. M., & Zhou, Z. H. (2012). Isolation-Based Anomaly Detection. ACM Transactions on Knowledge Discovery from Data, 6(1), 1\u201339. https://doi.org/10.1145/2133360.2133363 \u21a9\u21a9\u21a9 ");
docs.push({"id": 31, "title": "Introduction to Isolation Forests", "url": "/introduction-to-isolation-forests.html"});
index.add(32, "Python Pweave Installing Installing pweave is a matter of simply running1: pip3 install pweave Editor support At the moment of writing, the editor which, IMO, has the best support for pweave is Atom (using Hydrogen). To install the necessary packages run: apm install language-weave Hydrogen apm install language-markdown atom-html-preview pdf-view Chunk options I recommend using a separate pyenv for this. \u21a9 ");
docs.push({"id": 32, "title": "Python Pweave", "url": "/python-pweave.html"});
index.add(33, "Generating synthetic data Synthetic data will be used mainly for these scenarios: Regression Classification Regression data What does a regression consist of? For this section we will mainly use scikit-learn's make_regression method. For reproducibility, we will set a random_state. import warnings warnings.filterwarnings('ignore') warnings.simplefilter('ignore') random_state = 23 We will create a dataset using make_regression's random linear regression model with input features \\(x=(f_1,f_2,f_3,f_4)\\) and an output \\(y\\). import matplotlib.pyplot as plt from plotnine import * from plotnine.data import * import numpy as np import pandas as pd from sklearn.datasets import make_regression from scipy.stats import linregress N_FEATURES = 4 N_TARGETS = 1 N_SAMPLES = 100 dataset = make_regression( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=2, n_targets=N_TARGETS, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=random_state, ) print(dataset[0][:10]) print(dataset[1][:10]) [[ 0.87305874 -1.63096187 0.52538404 -0.19035824] [ 1.00698671 0.79834941 -0.04057655 -0.31358605] [-0.61464273 1.65110321 0.75791487 -0.0039844 ] [-1.08536678 1.82337823 0.4612592 -1.72325306] [-1.67774847 -0.54401341 0.86347869 -0.30250463] [-0.02427254 0.75537599 -0.04644972 -0.85153564] [-0.48085576 0.82100952 -0.9390196 -0.25870492] [-0.66772841 -2.46244005 -0.19855095 -1.85756579] [-0.29810663 -0.02239635 0.25363492 -1.22688366] [ 1.48146924 0.38269965 -1.18208819 -1.31062148]] [ 20.00449025 -30.41054677 52.65371365 -119.26376184 33.78805456 -78.12189078 -88.41673748 -177.21674804 -90.13920313 -197.90799195] Let's turn this dataset into a Pandas DataFrame: df = pd.DataFrame(data=dataset[0], columns=[f\"f{i+1}\" for i in range(N_FEATURES)]) df[\"y\"] = dataset[1] df.head() f1 f2 f3 f4 y 0 0.873 -1.631 0.525 -0.190 20.004 1 1.007 0.798 -0.041 -0.314 -30.411 2 -0.615 1.651 0.758 -0.004 52.654 3 -1.085 1.823 0.461 -1.723 -119.264 4 -1.678 -0.544 0.863 -0.303 33.788 Let's plot the data: from plotutils import * def plot_regression(df, size): for i in range(size): fit = np.polyfit(df[df.columns[i]], df[\"y\"], 1) fit_fn = np.poly1d(fit) plt.subplot(2, 2, i + 1) plt.xlabel(\"y\") plt.ylabel(f\"f{i+1}\") plt.scatter(df[df.columns[i]], df[\"y\"], s=30, c=colours[1], edgecolor=edges[1]) plt.plot( df[df.columns[i]], fit_fn(df[df.columns[i]]), ls=\"--\", c=colours[0], lw=1 ) plot_regression(df, N_FEATURES) Changing the Gaussian noise level The noise parameter in make_regression allows to adjust the scale of the data's gaussian centered noise. dataset = make_regression( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=2, n_targets=N_TARGETS, bias=0.0, effective_rank=None, tail_strength=0.5, noise=2.0, shuffle=True, coef=False, random_state=random_state, ) df = pd.DataFrame(data=dataset[0], columns=[f\"f{i+1}\" for i in range(N_FEATURES)]) df[\"y\"] = dataset[1] plot_regression(df, N_FEATURES) Visualising increasing noise Let's increase the noise by \\(10^i\\), for \\(i=1, 2, 3\\) and see what the data looks like. df = pd.DataFrame(data=np.zeros((N_SAMPLES, 1))) def create_noisy_data(noise): return make_regression( n_samples=N_SAMPLES, n_features=1, n_informative=1, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=noise, shuffle=True, coef=False, random_state=random_state, ) for i in range(3): data = create_noisy_data(10 ** i) df[f\"f{i+1}\"] = data[0] df[f\"y{i+1}\"] = data[1] for i in range(3): fit = np.polyfit(df[f\"f{i+1}\"], df[f\"y{i+1}\"], 1) fit_fn = np.poly1d(fit) plt.subplot(1, 3, i + 1) plt.scatter(df[f\"f{i+1}\"], df[f\"y{i+1}\"], s=30, c=colours[1], edgecolor=edges[1]) plt.plot( df[f\"f{i+1}\"], fit_fn(df[f\"f{i+1}\"]), ls=\"--\", color=colours[0], lw=1, ) plt.xlabel(f\"f{i+1}\") plt.ylabel(f\"y{i+1}\") (data:classification)= Classification data To generate data for classification we will use the make_classification method. from sklearn.datasets import make_classification N = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(N)]) df[\"y\"] = data[1] df.head() f1 f2 f3 f4 y 0 -3.216 -0.416 -1.295 -1.882 0 1 -1.426 -1.257 -1.734 -1.804 0 2 2.798 -3.010 -1.085 -3.134 1 3 0.633 2.502 -1.553 1.625 1 4 1.494 0.912 -1.887 -1.457 1 from itertools import combinations from math import ceil lst_var = list(combinations(df.columns[:-1], 2)) len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[\"y\"].apply(lambda y: colours[y]), edgecolor=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2) Cluster separation According to the docs1, class_sep is the factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier. N_FEATURES = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=3.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(N_FEATURES)]) df[\"y\"] = data[1] from itertools import combinations from math import ceil lst_var = list(combinations(df.columns[:-1], 2)) len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[\"y\"].apply(lambda y: colours[y]), edgecolor=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2) We can make the cluster separability more difficult, by decreasing the value of class_sep. N_FEATURES = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(N_FEATURES)]) df[\"y\"] = data[1] from itertools import combinations from math import ceil lst_var = list(combinations(df.columns[:-1], 2)) len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[\"y\"].apply(lambda y: colours[y]), edgecolor=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2) Noise level According to the documentation1, flip_y is the fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder. N_FEATURES = 4 for i in range(6): data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.1 * i, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(N_FEATURES)]) df[\"y\"] = data[1] plt.subplot(2, 3, i + 1) plt.title(f\"flip_y={round(0.1*i,2)}\") plt.scatter( df[\"f1\"], df[\"f2\"], s=50, c=df[\"y\"].apply(lambda y: colours[y]), edgecolor=df[\"y\"].apply(lambda y: edges[y]), ) plt.tight_layout(pad=3.0) df = pd.DataFrame(data=np.zeros((N_SAMPLES, 1))) for i in range(3): data = make_classification( n_samples=N_SAMPLES, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0, class_sep=i + 0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=random_state, ) df[f\"f{i+1}1\"] = data[0][:, 0] df[f\"f{i+1}2\"] = data[0][:, 1] df[f\"t{i+1}\"] = data[1] for i in range(3): plt.subplot(1, 3, i + 1) plt.scatter( df[f\"f{i+1}1\"], df[f\"f{i+1}2\"], s=50, c=df[f\"t{i+1}\"].apply(lambda y: colours[y]), edgecolor=df[f\"t{i+1}\"].apply(lambda y: edges[y]), ) Separability from sklearn.datasets import make_blobs N_FEATURE = 4 data = make_blobs( n_samples=60, n_features=N_FEATURE, centers=3, cluster_std=1.0, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(N_FEATURE)]) df[\"y\"] = data[1] from itertools import combinations from math import ceil lst_var = list(combinations(df.columns[:-1], 2)) len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[\"y\"].apply(lambda y: colours[y]), edgecolor=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2) To make a cluster more separable we can change cluster_std. data = make_blobs( n_samples=60, n_features=N_FEATURES, centers=3, cluster_std=0.3, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(N_FEATURES)]) df[\"y\"] = data[1] from itertools import combinations from math import ceil lst_var = list(combinations(df.columns[:-1], 2)) len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[\"y\"].apply(lambda y: colours[y]), edgecolor=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2) By decreasing cluster_std we make them less separable. data = make_blobs( n_samples=60, n_features=N_FEATURES, centers=3, cluster_std=2.5, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(N_FEATURES)]) df[\"y\"] = data[1] from itertools import combinations from math import ceil lst_var = list(combinations(df.columns[:-1], 2)) len_var = len(lst_var) for i in range(1, len_var + 1): plt.subplot(2, ceil(len_var / 2), i) var1 = lst_var[i - 1][0] var2 = lst_var[i - 1][1] plt.scatter( df[var1], df[var2], s=50, c=df[\"y\"].apply(lambda y: colours[y]), edgecolor=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(var1) plt.ylabel(var2) Anisotropic data data = make_blobs(n_samples=50, n_features=2, centers=3, cluster_std=1.5) transformation = [[0.5, -0.5], [-0.4, 0.8]] data_0 = np.dot(data[0], transformation) df = pd.DataFrame(data_0, columns=[f\"f{i}\" for i in range(1, 3)]) df[\"y\"] = data[1] plt.scatter( df[\"f1\"], df[\"f2\"], c=df[\"y\"].apply(lambda y: colours[y]), s=50, edgecolors=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(\"f1\") plt.ylabel(\"f2\") plt.show() Concentric clusters Sometimes we might be interested in creating a non-separable cluster. The simples way is to create concentric clusters with the make_circles method. from sklearn.datasets import make_circles data = make_circles( n_samples=N_SAMPLES, shuffle=True, noise=None, random_state=random_state, factor=0.6 ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(2)]) df[\"y\"] = data[1] plt.scatter( df[\"f1\"], df[\"f2\"], c=df[\"y\"].apply(lambda y: colours[y]), s=50, edgecolors=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(\"f1\") plt.ylabel(\"f2\") plt.show() Adding noise The noise parameter allows to create a concentric noisy dataset. data = make_circles( n_samples=N_SAMPLES, shuffle=True, noise=0.15, random_state=random_state, factor=0.6 ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(2)]) df[\"y\"] = data[1] plt.scatter( df[\"f1\"], df[\"f2\"], c=df[\"y\"].apply(lambda y: colours[y]), s=50, edgecolors=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(\"f1\") plt.ylabel(\"f2\") plt.show() Moon clusters A shape that can be useful to other methos (such as counterfactuals, for instance) is the one generated by the make_moons method. from sklearn.datasets import make_moons data = make_moons( n_samples=N_SAMPLES, shuffle=True, noise=None, random_state=random_state ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(2)]) df[\"y\"] = data[1] plt.scatter( df[\"f1\"], df[\"f2\"], c=df[\"y\"].apply(lambda y: colours[y]), s=50, edgecolors=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(\"f1\") plt.ylabel(\"f2\") plt.show() Adding noise As usual, the noise parameter allows to control the noise. data = make_moons( n_samples=N_SAMPLES, shuffle=True, noise=0.1, random_state=random_state ) df = pd.DataFrame(data[0], columns=[f\"f{i+1}\" for i in range(2)]) df[\"y\"] = data[1] plt.scatter( df[\"f1\"], df[\"f2\"], c=df[\"y\"].apply(lambda y: colours[y]), s=50, edgecolors=df[\"y\"].apply(lambda y: edges[y]), ) plt.xlabel(\"f1\") plt.ylabel(\"f2\") plt.show() https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html \u21a9\u21a9 ");
docs.push({"id": 33, "title": "Synthetic Data Generation", "url": "/synthetic-data-generation.html"});
index.add(34, "Go filesystem operations Notes on Go filesystem operations. Copying files Go does not have an utility method to copy files. We have to rely on writing our own implementation using the reading and writing functionality in other packages. As an example: package main import ( \"io\" \"log\" \"os\" ) func main() { from, err := os.Open(\"./foo.txt\") if err != nil { log.Fatal(err) } defer from.Close() to, err := os.OpenFile(\"./bar.txt\", os.O_RDWR|os.O_CREATE, 0666) if err != nil { log.Fatal(err) } defer to.Close() _, err = io.Copy(to, from) if err != nil { log.Fatal(err) } } Path operations Basepath To get the basepath of a path string we use the Dir method: filepath.Dir(\"/etc/foo/file.txt\") // \"/etc/foo\" Check if directory exists if _, err := os.Stat(\"/etc/foo/\"); os.IsNotExist(err) { // do something because it does not exist } Create nested directories Use MkdirAll: os.MkdirAll(\"/etc/long/nested/path/to/create\", os.ModePerm) ");
docs.push({"id": 34, "title": "Go filesystem operations", "url": "/go-filesystem-operations.html"});
index.add(35, "Python environments Interpreters To install different Python interpreters I strongly recommend asdf1. Let's look at to install Python in two different OSes, macOS and Fedora. macOS To install asdf on a macOS, first install the general dependencies with $ brew install coreutils curl git then install asdf itself with $ brew install asdf Add to the shell, in our case zsh with: $ echo -e \"\\n. $(brew --prefix asdf)/asdf.sh\" >> ~/.zshrc Add a plugin, in our case Python, with $ asdf plugin add Python You can list all available versions with $ asdf list all Python Install a specific version, say, $ asdf install Python 3.9.0 Fedora To install asdf on a Fedora, first install the general dependencies $ sudo dnf install curl git Clone the repository $ git clone https://github.com/asdf-vm/asdf.git \\ ~/.asdf --branch v0.8.0 Add to zsh with $ . $HOME/.asdf/asdf.sh` pyenv Compiling on macOS pyenv can be notoriously problematic on macOS. For instance, running pyenv doctor on my laptop2 will result in: Cloning ~/.pyenv/plugins/pyenv-doctor/bin/..... Installing python-pyenv-doctor... python-build: use readline from homebrew python-build: use zlib from xcode sdk BUILD FAILED (OS X 10.15.7 using python-build 20180424) Inspect or clean up the working tree at /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/python-build.20210128094523.17091 Results logged to /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/python-build.20210128094523.17091.log Last 10 log lines: checking readline/readline.h, presence... no checking for readline/readline.h,... no checking readline/rlconf.h usability... yes checking readline/rlconf.h presence... yes checking for readline/rlconf.h... yes checking for SSL_library_init in -lssl... no configure: WARNING: OpenSSL <1.1 not installed. Checking v1.1 or beyond... checking for OPENSSL_init_ssl in -lssl... no configure: error: OpenSSL is not installed. make: *** No targets specified and no makefile found. Stop. Problem(s) detected while checking system. See https://github.com/pyenv/pyenv/wiki/Common-build-problems for known solutions. The problem in this case is that pyenv can't find the relevant C headers for compilation of new versions. This can be fixed by using: $ CFLAGS=\"-I$(brew --prefix openssl)/include \\ -I$(brew --prefix readline)/include \\ -I$(xcrun --show-sdk-path)/usr/include\" \\ LDFLAGS=\"-L$(brew --prefix openssl)/lib \\ -L$(brew --prefix readline)/lib \\ -L$(xcrun --show-sdk-path)/usr/lib\" \\ pyenv doctor and the output will be: Cloning ~/.pyenv/plugins/pyenv-doctor/bin/..... Installing python-pyenv-doctor... python-build: use readline from homebrew python-build: use zlib from xcode sdk Installed python-pyenv-doctor to /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/pyenv-doctor.20210128095003.18889/prefix Congratulations! You are ready to build pythons! Poetry Poetry as Jupyter kernel To register a poetry environment (named foo) as a Jupyter kernel, run: poetry run python -m ipykernel install --user --name foo venv Create a new venv with the command: $ virtualenv venv and activate it using (under Bash or zsh) with: $ source venv/bin/activate https://asdf-vm.com/ \u21a9 I'm running Big Sur at the moment of writing \u21a9 ");
docs.push({"id": 35, "title": "Python environments", "url": "/python-environments.html"});
index.add(36, "Pandas Basics ");
docs.push({"id": 36, "title": "Pandas", "url": "/pandas.html"});
index.add(37, "(Semi) handcrafted RSS I've been using a minimalist blog setup for some time now. I was having something of a framework fatigue after switching between a few static site generators. Each new generator I decided to try implied usually either learning a new programming language (Python, Ruby, Go) to perform basic setup and a new template engine syntax. Typically I wasn't using the vast majority of the features available for each generator. And finally, most of the generators I tried over the years rely on heavy configuration if I want to maintain the site organisation and look. I've discussed with some friends and colleagues why it's my opinion that a plain HTML blog is still superior to other solutions (such as Markdown coupled with some generator framework). I'll leave my arguments to a future post. However, I am still using some form of a generator. The blog writing process at the moment is the following: I write the content of the post to an HTML fragment (no HEAD, for instance). All files are HTML and in the same folder. I have a shell script to walk through the files in the input folder and add a common header, footer and process all code blocks with syntax highlighting. Save the \"processed\" files to an output folder Upload (currently to Github to be served via Github pages). The HTML fragments are minimal, for instance: <h1>A post-modern title</h1> <p>Yes, this could be an entire blog post.</p> The point is, where do we draw the line on what a static generator is? For this post, I won't consider a loose collection of specialised scripts to be a static generator. There is no configuration, no convention, no theming ability 1. You can argue that this is what many generators do, but I think that's beyond the scope of this short post. Since my static blog is straightforward, with minimal markup, why not create something equally simple for RSS generation? To do so, I've decided to go the way of \"handcrafted\" HTML. However, I was accustomed to a static site generator to generate some goodies, such as syndication feeds automatically. I've decided to add an RSS feed to the site, using minimal dependencies (only shell scripting and a couple of universal user-land tools such as grep and cat). This approach has the added benefit that it is applicable to expose other types of data as an RSS feed, such as server and periodic job logs. We start by adding the feed header to the index.xml: <?xml version=\"1.0\" encoding=\"utf-8\"?> <rss version=\"2.0\"> <channel> <title>Rui Vieira's blog</title> <description>Rui Vieira's personal blog</description> <link>https://ruivieira.dev/</link> <lastBuildDate>$(date)</lastBuildDate> EOF The RSS 2.0 specification is quite simple in terms of the minimum requirements for a valid feed. The mandatory \"header\" fields are: title, the name of the channel. link, the URL to the HTML website corresponding to the channel. description, phrase or sentence describing the channel. In terms of feed items, according to the specification, at least one of title or description must be present, and all remaining elements are optional. We use the following in this feed: title, the title of the item. link, the URL of the item. pubDate indicates when the item was published. pubDate needs to conform with RFC 822. Just as interesting tidbit, RFC 822 (which defines Internet Text Message formats) is one of the core email RFCs. It predates ISO 8601 by six years (1982) and it's itself based on 1977's RFC 733. We then loop over all the input files to build the RSS entries. FILES=input/*.html for FILE in $FILES do FILENAME=\"${FILE##*/}\" FILENAME=\"${FILENAME%.*}\" # extract title ... # write entry to index.xml done Using Bash First, extract the title. The actual title is not inside the <title> tag, but on the first header <h1>. cat output/nb-estimation.html | grep -E \"<h1.*>(.*?)</h1>\" | sed 's/.*<h1.*>\\(.*\\)<\\/h1>.*/\\1/' The first produces: <div id=\"main\"> <h1 id=\"negative-binomial-estimation\">Negative Binomial estimation</h1> </div> While the second produces: Negative Binomial estimations Now, what happens if we have more than one <h1> header? UNIX pipelines to the rescue. We simple retrieve the first line of the matching grep, by inserting a head -1. To get the modified date of $FILE we can use: date -r $FILE.html The final RSS feed build is: cat >output/index.xml <<EOF <?xml version=\"1.0\" encoding=\"utf-8\"?> <rss version=\"2.0\"> <channel> <title>Rui Vieira's blog</title> <description>Rui Vieira's personal blog</description> <link>https://ruivieira.dev/</link> <lastBuildDate>$(date)</lastBuildDate> EOF FILES=input/*.html for FILE in $FILES do FILENAME=\"${FILE##*/}\" FILENAME=\"${FILENAME%.*}\" TITLE=$(cat output/$FILENAME.html | grep -E \"<h1.*>(.*?)</h1>\" | head -1 | sed 's/.*<h1.*>\\(.*\\)<\\/h1>.*/\\1/') cat >>output/index.xml <<EOF <item> <title>$TITLE</title> <link>https://ruivieira.dev/$FILENAME.html</link> <pubDate>$(date -r output/$FILENAME.html)</pubDate> </item> EOF done cat >>output/index.xml <<EOF </channel> </rss> EOF Using Python Another possibility is to use a specialised tool to extract an RSS item from an HTML file. To do so, we need to parse the necessary data and replace the extraction part of the loop. This is, after all, along the lines of the Unix philosophy: create specialised tools with a focus on modularity and reusability. To do, we create a simple script called post_title.py. It uses the Beautiful Soup library, which you can install using: $ pip install beautifulsoup4 The script reads an HTML file, extract the title and return: from bs4 import BeautifulSoup import sys with open(sys.argv[1], 'r') as file: data = file.read() soup = BeautifulSoup(data, features=\"html.parser\") print(soup.h1.string) This script can now be used to replace the title extraction: cat >output/index.xml <<EOF <?xml version=\"1.0\" encoding=\"utf-8\"?> <rss version=\"2.0\"> <channel> <title>Rui Vieira's blog</title> <description>Rui Vieira's personal blog</description> <link>https://ruivieira.dev/</link> <lastBuildDate>$(date)</lastBuildDate> EOF FILES=input/*.html for FILE in $FILES do FILENAME=\"${FILE##*/}\" FILENAME=\"${FILENAME%.*}\" cat >>output/index.xml <<EOF <item> <title>$(post_title.py $FILENAME.html)</title> <link>https://ruivieira.dev/$FILENAME.html</link> <pubDate>$(date -r output/$FILENAME.html)</pubDate> </item> EOF done cat >>output/index.xml <<EOF </channel> </rss> EOF The reason why the whole RSS feed is not generated in Python is to have the title extraction as a \"function\" which can map to whichever logic the shell script is using. Hope this could be useful to you. Happy coding! Apart from plain CSS theming, that is. \u21a9 ");
docs.push({"id": 37, "title": "(Semi) handcrafted RSS", "url": "/semi-handcrafted-rss.html"});
index.add(38, "Python monkey patching (for readability) When preparing a Jupyter notebook for a workshop on recommendation engines which I've presented with a colleague, I was faced with the following problem: \"How to break a large class definition into several cells so it can be presented step-by-step.\" Having the ability to declare a rather complex (and large) Python class in separate cells has several advantages, the obvious one being the ability to fully document each method's functionality with Markdown, rather than comments. Python does allow for functionality to be added to classes after their declaration via the assignment of methods through attributes. This is commonly known as \"monkey patching\" and hinges on the concepts of bound and unbound methods. I will show a quick and general overview of the methods that Python puts at our disposal for dynamic runtime object manipulation, but for a more in-depth please consult the official Python documentation. Bound and unbound methods Let's first look at bound methods. If we assume a class called Class and an instance instance, with an instance method bound and class method unbound such that class Class: def bound(self): pass @staticmethod def unbound(): pass instance = Class() Then foo is a bound method and bar is an unbound method. This definition, in practice, can be exemplified by the standard way of calling .foo(), which is instance.bound() which in turn is equivalent to Class.bound(instance) The standard way of calling unbound is , similarly instance.unbound() This, however, is equivalent to Class.unbound() In the unbound case, we can see there's no need to pass the class instance. unbound is not bound to the class instance. As mentioned before, Python allow us to change the class attributes at runtime. If we consider a method such as def newBound(self): pass we can then add it to the class, even after declaring it. For instance: Class.newBound = newBound instance = Class() instance.newBound() # Class.newBound(instance) It is interesting to note that any type of function definition will work, since functions are first class objects in Python. As such, if the method can be written as a single statement, a lambda could also be used, i.e. Class.newBound = lambda self: print(\"I'm a lambda\") A limitation of the \"monkey patching\" method, is that attributes can only be changed at the class definition level. As an example, although possible, it is not trivial to add the .newBound() method to instance. A solution is to either call the descriptor methods (which allow for instance attribute manipulation), or declare the instance attribute as a MethodType. To illustrate this in our case: import types instance.newBound = types.MethodType(newBound, instance) instance.newBound() # Prints \"I'm a lambda\" This method is precisely, as mentioned, to change attributes for a specific instance, so in this case, if we try to access the bound method from another instance anotherInstance, it would fail anotherInstance = Class() anotherInstance.newBound() # fails with AttributeError Abstract classes Python supports abstract classes, i.e. the definition of \"blueprint\" classes for which we delegate the concrete implementation of abstract methods to subclasses. In Python 3.x this is done via the @abstractmethod annotation. If we declare a class such as from abc import ABC, abstractmethod class AbstractClass(ABC): @abstractmethod def abstractMethod(self): pass we can then implement abstractMethod in all of AbstractClass's subclasses: class ConcreteClass(AbstractClass): def abstractMethod(self): print(\"Concrete class abstract method\") We could, obviously, do this in Python without abstract classes, but this mechanism allows for a greater safety, since implementation of abstract methods is mandatory in this case. With regular classes, not implementing abstractMethod would simply assume we were using the parent's definition. Unfortunately, monkey patching of abstract methods is not supported in Python. We could monkey patch the concrete class: ConcreteClass.newBound = lambda self: print(\"New 'child' bound\") c = ConcreteClass() c.newBound() # prints \"New 'child' bound\" And we could even add a new bound method to the superclass, which will be available to all subclasses: AbstractClass.newBound = lambda self: print(\"New 'parent' bound\") c = ConcreteClass() c.newBound() # prints \"New 'parent' bound\" However, we can't add abstract methods with monkey patching. This is a documented exception of this functionality with the specific warning that Dynamically adding abstract methods to a class, or attempting to modify the abstraction status of a method or class once it is created, are not supported. The abstractmethod() only affects subclasses derived using regular inheritance; \"virtual subclasses\" registered with the ABC's register() method are not affected. Private methods We can dynamically add and replace inner methods, such as: class Class: def _inner(self): print(\"Inner bound\") def __private(self): print(\"Private bound\") def callNewPrivate(self): self.__newPrivate() Class._newInner = lambda self: print(\"New inner bound\") c = Class() c._inner() # prints \"Inner bound\" c._newInner() # prints \"New inner bound\" However, private methods behave differently. Python enforces name mangling for private methods. As specified in the documentation: Since there is a valid use-case for class-private members (namely to avoid name clashes of names with names defined by subclasses), there is limited support for such a mechanism, called name mangling. Any identifier of the form __spam (at least two leading underscores, at most one trailing underscore) is textually replaced with _classname__spam, where classname is the current class name with leading underscore(s) stripped. This mangling is done without regard to the syntactic position of the identifier, as long as it occurs within the definition of a class. We can then still access the private methods (although we probably shouldn't), but monkey patching won't work as before due to the above. c._Class__private() # Private bound Class.__newPrivate = lambda self: print(\"New private bound\") c = Class() c._Class__newPrivate() # fails with AttributeError We have defined a new method called __newPrivate() but interestingly, this method is not private. We can see this by calling it directly (which is allowed) and by calling the new \"private\" method from inside the class as self.__newPrivate(): c.__newPrivate() # prints \"New private bound\" c.callNewPrivate() # fails with AttributeError (can't find _Class_NewPrivate) It is possible to perform some OOP abuse and declare the private method by mangling the name ourselves. In this case we could then do: Class._Class__newPrivate = lambda self: print(\"New private bound\") c = Class() c._Class__newPrivate() # prints \"New private bound\" c.callNewPrivate() # prints \"New private bound\" Builtins Is it possible to monkey patch builtin classes in Python, e.g. int or float? In short, yes, it is. Although the usefulness is arguable and I strongly urge not to do this in any production scenario, we'll look at how to achieve this, for the sake of completeness. A very interesting and educational read is available from the Forbidden Fruit Python module. Primitive (or builtin) classes in Python are typically written in C and as such some of these meta-programming facilities require jumping through extra hoops (as well as being a Very Bad Idea\u2122). Let's first look at the integer class representation, int. A int doesn't allow bound methods to be added dynamically as previously. For instance: p = 5 type(p) # int We can try to add a method to int to square the value of the instance: int.square = lambda self: self ** 2 This fails with the error TypeError: can't set attributes of built-in/extension type 'int'. The solution (as presented in Forbidden Fruit) is to first create classes to hold the ctype information of a builtin (C) class. We subclass ctypes Python representation of a C struct in native byte order and hold the signed int size and pointer to PyObject. import ctypes class PyObject(ctypes.Structure): pass PyObject.fields = [ ('ob_refcnt', ctypes.c_int), ('ob_type', ctypes.POINTER(PyObject)), ] Next we create a holder for Python objects slots, containing a reference to the ctype structure: class SlotsProxy(PyObject): _fields_ = [('dict', ctypes.POINTER(PyObject))] The final step is extract the PyProxyDict from the object referenced by the pointer. Ideally, we should get the builtin's namespace so we can freely set attributes as we did previously. A helper function to retrieve the builtins (mutable) namespace can then be: def patch(klass): name = klass.__name__ target = klass.__dict__ proxy_dict = SlotsProxy.from_address(id(target)) namespace = {} ctypes.pythonapi.PyDict_SetItem( ctypes.py_object(namespace), ctypes.py_object(name), proxy_dict.dict, ) return namespace[name] We can now easily patch builtin classes. Let's try to add the square method again by first retrieving the namespace (stored below in d) and setting it directly d = patch(int) d[\"square\"] = lambda self: self ** 2 p.square() # 25 All future instance of int will also contain the square method now: (2 + p).square() # 49 Conclusion \"Monkey patching\" is usually, and rightly so, considered a code smell, due to the increased indirection and potential source of unwanted surprises. However, having the ability to \"monkey patch\" classes in Python allows us to write Jupyter notebooks in a more literate, fluid way rather than presenting the user with a \"wall of code\". Thank you for reading. If you have any comments or suggestions please drop me a message on Mastodon.");
docs.push({"id": 38, "title": "Python monkey patching (for readability)", "url": "/python-monkey-patching-for-readability.html"});
index.add(39, "ruivieira.dev -----BEGIN GEEK CODE BLOCK----- Version: 3.1 GCS/M/MU d-- s:+> !a C++$>+++ ULC P+>++ L+++>++++ E-@ W++ !N>+ !o K--? !w O>+ M+ !V PS++@ PE-@ !Y PGP@>++ t+ !5 X++ R@>+ tv+> b++>+++ DI>+ D++>+++ G++>+++ e++++ h---- r+++ z ------END GEEK CODE BLOCK------ Welcome Welcome to ruivieira.dev. Formerly, this site used to be a blog. Currently, it is adhering to the principles of a Digital Garden and following the guidelines of Brutalist Web Design. It is intended to be a collection of notes, a learning journal and a reference -- all growing organically. This site is also available at hollowed.space (using Gemini). Technical details about this site are available at the site details page. You can still find most of my old \"blog posts\" here: Serving models with Seldon (Semi) handcrafted RSS Introduction to Balanced Box-Decomposition Trees Monotonic Cubic Spline interpolation (with some Rust) Python monkey patching (for readability) Introduction to Isolation Forests MCMC performance on Substrate VM Containerised Streaming Data Generation using State-Space Models A simple Python benchmark exercise A streaming ALS implementation Bayesian estimation of changepoints t as mixture of Normals Langton's Ant A Gibbs Sampler in Crystal MCMC notifications gulp Some topics on Machine Learning: A section on Explainability, with focus on Counterfactuals There are some areas about programming languages too: Go Python Java Clojure Rust And other frameworks/tools: Ansible Emacs Recent changes Python grammar of graphics Counterfactuals with Constraint Solvers Counterfactual Fairness Distance metrics Portuguese Christmas recipes Java consumer ");
docs.push({"id": 39, "title": "index", "url": "/index.html"});
index.add(40, "Java Notes on Java. Java Completable Futures Java consumer Reference Get user home directory System.getProperty(\"user.home\"); List files recursively try (Stream<Path> walk = Files.walk(Paths.get(input))) { List<String> result = walk.filter(Files::isRegularFile) .map(x ->x.toString()) .collect(Collectors.toList()); result.forEach(System.out::println); } catch (IOException e) { e.printStackTrace(); } In case we want the file subset with a specific extension, txt we can filter the stream with List<String> result = walk.filter(Files::isRegularFile) .filter(x -> x.toString().endsWith(\".txt\")) .map(x -> x.toString()) .collect(Collectors.toList()); ");
docs.push({"id": 40, "title": "Java", "url": "/java.html"});
index.add(41, "Python grammar of graphics import pandas as pd import warnings warnings.filterwarnings('ignore') warnings.simplefilter('ignore') mpg = pd.read_csv(\"data/mpg.csv\") mpg.head() mpg cylinders displacement horsepower weight acceleration model_year origin name 0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140 3449 10.5 70 1 ford torino from plotnine import * from plotnine.data import * ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\"displ\", y=\"hwy\")) + theme_classic() <ggplot: (366319782)> ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\"displ\", y=\"hwy\", color=\"class\")) + theme_classic() <ggplot: (366405940)> ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\"displ\", y=\"hwy\", size=\"class\")) + theme_classic() <ggplot: (366362040)> # Left ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\"displ\", y=\"hwy\", alpha=\"manufacturer\")) + theme_classic() <ggplot: (366298329)> # Right ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\"displ\", y=\"hwy\", shape=\"manufacturer\")) + theme_classic() <ggplot: (381115233)> ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\"displ\", y=\"hwy\"), color=\"blue\") + theme_classic() <ggplot: (381154256)> ggplot(data=mpg) +\\ geom_smooth(mapping=aes(x=\"displ\", y=\"hwy\")) + theme_classic() <ggplot: (381119507)> ggplot(data=mpg) +\\ geom_smooth(mapping=aes(x=\"displ\", y=\"hwy\", linetype=\"drv\")) + theme_classic() <ggplot: (381071879)> ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\"displ\", y=\"hwy\")) +\\ geom_smooth(mapping=aes(x=\"displ\", y=\"hwy\")) + theme_classic() <ggplot: (366461585)> ggplot(data=mpg, mapping=aes(x=\"displ\", y=\"hwy\")) +\\ geom_point(mapping=aes(color=\"class\")) +\\ geom_smooth() + theme_classic() <ggplot: (366351376)> ");
docs.push({"id": 41, "title": "Python grammar of graphics", "url": "/python-grammar-of-graphics.html"});
index.add(42, "Ansible Ansible notes. Reference");
docs.push({"id": 42, "title": "Ansible", "url": "/ansible.html"});
index.add(43, "A streaming ALS implementation In this blog post I would like to talk a little bit about recommendation engines in general and how to build a streaming recommendation engine on top of Apache Spark. I will start by introducing the concept of collaborative filtering, and focus in two variants: batch and streaming Alternating Least Squares (ALS). I will look at the principles of a streaming distributed recommendation engine on Spark and finally, I'll talk about practical issues when using these methods. Recommendation engines So what are \"recommendation engines\"? Recommendation engines are a popular method to match users, products and historical data on user behaviour. Collaborative filtering In the majority of cases, we assume there's a unique mapping between a user \\(x\\), a product \\(y\\) and rating \\(\\mathsf{R}_{x,y}\\). \\[ \\left(x,y\\right) \\mapsto \\mathsf{R}_{x,y} \\] The \"collaborative\" aspect refers to the fact that we are using collective information from a group of users and \"filtering\" is simply a synonym for \"prediction\". So, we use collaborative filtering quite frequently in our daily life and it really seems like common sense. The main principle is that if a group of people tend to collectively have similar tastes, it is more likely that they agree on an unknown product. Let's imagine that you have a number of friends with whom you share a very similar musical taste, let's call it A and another group, B, compared to which you have very different musical tastes. If group A and group B both recommend you a new album which they regard highly, which one would you pick? You will probably pick the album from group A, right? So that's collaborative filtering in a nutshell. Bonus question: what if an album is considered really bad by group B? Does it mean you'll like it? It's difficult to tell. Because group A has relevance to you, it's easy to match. Because B is too dissimilar, a low rating is not very informative. Alternating Least Squares (ALS) One of the most popular collaborative filtering methods is Alternating Least Squares (ALS). In ALS we assume that the available rating data can be represented in a sparse matrix form, that is, we will assume a sequential ordering of both users and products. Each entry of the matrix will then represent the rating for a unique pair of user and products. If we then consider ratings data as a matrix, let's call it \\(\\mathsf{R}\\), the user and product ids will represent coordinates in a ratings matrix and the actual rating will be the value for that particular entry. To keep the notation consistent with the above we simply call the entry \\((x,y)\\) as \\(\\mathsf{R}_{x,y}\\). This will look something like the matrix represented in the figure below. The idea behind ALS is to factorise the ratings matrix \\(\\mathsf{R}_{x,y}\\) into two matrices \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\), which in turn, when multiplied back, will return an approximation of the original ratings matrix, that is: \\[ \\mathsf{R} \\approx \\hat{\\mathsf{R}} = \\mathsf{U}^T \\mathsf{P} \\] To \"predict\" a missing rating for a user \\(x\\) and product \\(y\\), we can simply multiply two vectors, namely the \\(x\\) row from the user latent factors and the \\(y\\) column from the product latent factors, \\(\\hat{\\mathsf{R}}_{x,y}\\), that is: \\[ \\hat{\\mathsf{R}}_{x,y} = \\mathsf{U}_x^T \\mathsf{P}_y \\] There are several ways to tackle this factorisation problem and we will cover two of them in here. We will first look at a batch method, which aims at factorising using the whole of the ratings matrix and a stochastic gradient descent method, which uses a single observation at a time. Batch ALS This factorisation is performed by first defining an (objective) loss function (here called \\(\\ell\\)). A general form is represented below where, as before, \\(\\mathsf{R}_{x,y}\\) is the true rating and \\(\\hat{\\mathsf{R}}_{x,y}\\) is the predicted rating, calculated as seen previously. The remaining terms are simply regularisation terms to help prevent overfitting. \\[ \\ell = \\sum c_{x,y} \\left(\\mathsf{R}_{x,y} - \\underbrace{\\mathsf{U}_x^T \\mathsf{P}_y}_{\\hat{\\mathsf{R}}_{x,t}}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right) \\] The value of \\((c_{x,y})\\) constitutes a penalisation function and will depend on whether we are considering explicit or implicit feedback. If we consider the known ratings as our training dataset \\(\\mathcal{T}\\), then, in the case of explicit feedback we have \\[ c_{x,y} = \\begin{cases} 0,\\qquad\\text{if}\\ \\left(x,y\\right) \\notin \\mathcal{T} \\\\\\\\ 1,\\qquad\\text{if}\\ \\left(x,y\\right) \\in \\mathcal{T} \\end{cases} \\] Constraining our loss function to only include known ratings. The implicit feedback case is different (and a possible future topic) and for the remainder of this post we will only consider the explicit feedback case. Given the above, we can then simplify our loss function, in the explicit feedback case, to \\[ \\ell = \\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}_{x,y} -\\hat{\\mathsf{R}}_{x,y}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right) \\] Minimizing \\(\\ell\\) is however an NP-hard problem, due to its non-convexity. However, if we treat \\(\\mathsf{U}\\) as constant, then \\(\\ell\\) is a convex in relation to \\(\\mathsf{P}\\) and if we treat \\(\\mathsf{P}\\) as constant, \\(\\ell\\) is convex in relation to \\(\\mathsf{U}\\). We can then alternate between fixing \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\), changing the values such that the loss function \\(\\ell\\) (above) is minimized. This procedure is then repeated until we reach convergence. The way that ALS works is, in simplified terms, to find the factors \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\), which when multiplied together provide an approximation of our ratings matrix \\(\\mathsf{R}\\), as we've seen previously. Once we have the factors \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\), we can then predict the missing values in \\(\\mathsf{R}\\) by using the approximation \\(\\hat{\\mathsf{R}}\\). It is clear that in a real world scenario we would have many missing ratings, simply due to the assumption that no user rates all products (if they did, the case for a recommendation engine will be significantly weaker). ALS is designed to deal with sparse matrices and to fill the blanks using predicted values. After factorization, our approximated ratings matrix will look something like this: As mentioned previously, the first step is then to minimise the loss function. In this case we take the partial derivatives and set them to zero and fortunately this has a closed form solution. We get a system of linear equations which we can easily implement. The system will correspond to the solution of \\[ \\frac{\\partial \\ell}{\\partial \\mathsf{U}_x}=0, \\qquad \\frac{\\partial \\ell}{\\partial \\mathsf{P}_y}=0. \\] We start by solving the user latent factor minimisation using: \\[ \\frac{1}{2}\\frac{\\partial \\ell}{\\partial \\mathsf{U}_x}=0 \\\\\\\\ \\frac{1}{2}\\frac{\\partial}{\\partial \\mathsf{U}_x} \\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}_{x,y} - \\mathsf{U}_x^T \\mathsf{P}_y\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right)=0 \\\\\\\\ -\\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}_{x,y} - \\mathsf{U}_x^T \\mathsf{P}_y\\right)\\mathsf{P}_y^T + \\lambda \\mathsf{U}\\_x^T=0\\\\\\\\ -\\left(\\mathsf{R}_x -\\mathsf{U}_x^T \\mathsf{P}^T\\right)\\mathsf{P} + \\lambda \\mathsf{U}_x^T=0\\\\\\\\ \\mathsf{U}_x^T\\left(\\mathsf{P}^T \\mathsf{P} + \\lambda \\boldsymbol{\\mathsf{I}}\\right) = \\mathsf{R}_x \\mathsf{P} \\\\\\\\ \\mathsf{U}_x^T = \\mathsf{R}_x \\mathsf{P} \\left(\\mathsf{P}^T \\mathsf{P} + \\lambda \\boldsymbol{\\mathsf{I}}\\right)^{-1}. \\] Similarly, we can solve for the product latent factor by using: \\[ \\frac{1}{2}\\frac{\\partial \\ell}{\\partial \\mathsf{P}_y}=0 \\\\\\\\ -\\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}\\_{x,y} - \\mathsf{P}\\_y^T \\mathsf{U}_x\\right)\\mathsf{U}_x^T + \\lambda \\mathsf{P}_y^T=0\\\\\\\\ -\\left(\\mathsf{R}_y - \\mathsf{P}_y^T \\mathsf{U}^T\\right)\\mathsf{U} + \\lambda \\mathsf{P}_y^T=0\\\\\\\\ \\mathsf{P}_y^T\\left(\\mathsf{U}^T \\mathsf{U} + \\lambda \\boldsymbol{\\mathsf{I}}\\right) = \\mathsf{R}_y \\mathsf{U} \\\\\\\\ \\mathsf{P}_y^T = \\mathsf{R}_y \\mathsf{U} \\left(\\mathsf{U}^T \\mathsf{U} + \\lambda \\boldsymbol{\\mathsf{I}}\\right)^{-1}. \\] We can then calculate each factor iteratively, by fixing the other one and solving the estimator. While this process is alternated, an error measure (usually the Root Mean Squared Error RMSE), or \\(RMSE\\) is calculated (as below) between the rating matrix approximation given by the latent factors and the ratings which we have, \\(\\mathcal{T}\\). This method is guaranteed to converge and when we consider out approximation to be good enough, or after a set number of iterations we can then stop the refinement. \\[ RMSE = \\sqrt{\\frac{1}{n}\\sum_{x,y \\in \\mathcal{T}}\\lvert \\hat{\\mathsf{R}}_{x,y} - \\mathsf{R}_{x,y}\\rvert} \\] After the latent factors are estimated, we can then use them to try to recreate the original ratings matrix with the approximation as we've seen. The missing ratings in the original matrix will now be filled by values which minimize the least squares recursion and these are taken as the ratings \"predictions\". To illustrate the working of ALS, let's assume we have a very quirky shop that only ever sells 300 products and has exactly 300 customers. On top of that, users are allowed to use 8 bit to rate the products. We will also assume in this unusual shop that every user has rated every product. Now we're humans, and we visualise patterns in colour more easily than in numbers. We will assign a palette to the ratings, so that each rating corresponds to a colour. I think you know where this is going ... we make up this final ratings matrix so now we can visualise the ALS progress. So how do we perform this factorisation? The initial step is to fill the latent factors \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\)) with random values. Since at this point, we assume we don't have any ratings, having random factors will lead to an initial random guess of the ratings matrix. We then proceed to calculate each factor matrix, as we've seen, by calculating one using the estimator while keeping the other one constant and then alternating. We can see by the movie below that at each iteration the approximation to the original ratings gets better, stabilising after a few steps. This is to be expected, in this case, since this would be the simplest implementation of ALS: a batch ALS on a single machine where we know all the ratings. So a fair question that arises is: why can't we update this model and perform recommendations in a streaming fashion using this method? After all, if users add product ratings, we can simply update the predictions by recalculating the factors! The problem is that when a new rating is added, or when new users and new products are added, we need to recalculate the entirety of the \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\) matrices, and to do so, we need to have access to all of the data, \\(\\mathsf{R}\\). Streaming ALS Ideally, we want a method that would allow us to update \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\) using one observation, \\(\\mathsf{R}_{x,y}\\) at a time It turns out that the Stochastic Gradient Descent (or SGD) method allows us to do precisely that. We'll look at the specific variant of SGD we've used which is called Bias-Stochastic Gradient Descent (B-SGD). It is important to keep in mind, under a certain point of view, both methods aim at the same thing. They both try to factorise the ratings matrix as latent factors, which would then be used to perform predictions. The main differences are of course, how the data is used (batch or one observation at the time) and how the factorisation is calculated. In the SGD case we use the concept of biases in both users and items. The bias is a measure of how consistently a product is rated by different users. The bias of rating \\((x,y)\\), that is the rating given by user \\(x\\) to product \\(y\\), can be calculated as the sum of \\(\\mu\\), an overall average rating and the observed deviations of user \\(x\\), which we call \\(b_x\\), and the observed deviations of product \\(y\\), called \\(b_y\\), that is: \\[ b_{x,y} = \\mu + b_x + b_y \\] This bias information is now incorporated in the rating prediction. We can see that the SGD prediction is simply the batch prediction plus the corresponding bias term \\[ \\hat{\\mathsf{R}}_{x,y} = b_{x,y} + \\underbrace{\\mathsf{U}^T_x \\cdot \\mathsf{P}_y}_{batch} \\] If we take the loss function definition for the batch method (and still considering the explicit feedback case), we can then replace the predicted rating formulation with our new one. We have, as before, some regularisation terms, but now also include a new regularisation term for the bias components, but we don't need to go into that. \\[ \\ell_{SGD} = \\sum_{x,y \\in \\mathcal{T}} \\left(\\mathsf{R}_{x,y} - b_{x,y} - \\hat{\\mathsf{R}}_{x,y}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2 + b_x^2 + b_y^2\\right) \\] Since calculating the full gradient is computationally very expensive, we calculate it for a single observation. As we can see, the SGD method allows us to update the user and product specific bias as well as a single user and product latent factor row given a single rating. Provided we have a single rating, the rating of user \\(x\\) for product \\(y\\), we can update the biases as well as the latent vectors for user \\(x\\) and for product \\(y\\), that is, we no longer need to update the entire matrices \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\), while still maintaining a convergence property. Provided with a learning rate \\(\\gamma\\) and defining our prediction error as \\[ \\epsilon_{x,y}=\\mathsf{R}_{x,y}-\\hat{\\mathsf{R}}_{x,y}, \\] the biases and latent factors can now be updated in the opposite direction of the calculated gradient, proportionally to the learning rate, such that \\[ \\begin{aligned} b_x &\\leftarrow b_x + \\gamma \\left(\\epsilon_{x,y}-\\lambda_x b_x\\right) \\\\\\\\ b_y &\\leftarrow b_y + \\gamma \\left(\\epsilon_{x,y}-\\lambda_y b_y\\right) \\\\\\\\ \\mathsf{U}_x &\\leftarrow \\mathsf{U}_x + \\gamma \\left(\\epsilon_{x,y}\\mathsf{P}_y - \\lambda^\\prime_x \\mathsf{U}_x\\right) \\\\\\\\ \\mathsf{P}_y &\\leftarrow \\mathsf{P}_y + \\gamma \\left(\\epsilon_{x,y}\\mathsf{U}_x - \\lambda^\\prime_y \\mathsf{P}_y\\right) \\end{aligned} \\] So the practical difference, in terms of streaming data is evident now. Given that, in both methods, the objective is to estimate the latent factors, given the ratings: with batch ALS, whenever we get a new rating, we need to fully recalculate the factors iteratively until we reach convergence. Conversely, with an SGD based factorisation, whenever we have a new rating, we can simply estimate the relevant row and column in the latent factors, by calculating the gradients and adjusting its values. Next we show the previous manufactured ratings matrix being factorised using B-SGD. We now simply recalculate the biases and a single latent factor vector, one observation at the time. We can see that, as expected, the convergence is slower (we are using a single observation at each step) but in the end, it produces a similar result. Now, this works fine for a single machine implementing streaming ALS. But we are interested in scaling this to something larger than this example so we will use a distributed implementation of ALS. And this is were it can start to get tricky. As it is the case with distributed algorithms, there are some pitfalls which we need to avoid in order to have a performant implementation. We will look at a few of these by looking at the Apache Spark's and its default ALS implementation. Apache Spark As probably most of you are familiar with, Spark is an Apache community project which aims at providing a modern platform for distributed computations. Spark provides several core data structures, such as RDDs (Resilient Distributed Datasets), Dataframes and Datasets. The RDD is an immutable, distributed typed collection of objects. The RDD is partitioned across the cluster. This allows the spark operations, such as function mapping, to be applied to each subset of the RDD in parallel at each partition. For the streaming ALS application we will use RDDs to implement the algorithm. Spark's MLlib provides a collaborative filtering implementation based on the distributed batch ALS which we've covered previously. The API is quite simple and to train a model we need: val model = ALS.train(ratings, rank, iterations, lambda) case class Rating(int user, int product, double rating) val ratings: RDD[Rating] val rank: int val iterations: int val lambda: Double An RDD containing the ratings. The RDD has elements of the class Rating, which is basically a wrapper around a tuple of user id, product id and rating. This RDD corresponds to all the entries in our ratings matrix used previously. The rank which corresponds to the number of elements in our latent factor vectors (this would be the number of columns or rows in our \\(\\mathsf{U}\\) and \\(\\mathsf{P}\\) matrices). A stopping criteria in terms of iterations for the ALS. And finally, we set the lambda parameter, a regularisation parameter, which we've shown to be a part of the loss function's regularisation. Since we have the data, the question is then how to choose the parameters. A typical method is to split the original ratings data into two random sets, one for training and one for validation. We then proceed to train the model to several different parameters, usually according to a grid search, and calculate some error measure between the predicted and validation ratings, choosing the parameters which minimise the error. Once the model is trained, we get a MatrixFactorizationModel instance, which is basically a wrapper for the latent factors as RDDs. val model = ALS.train(ratings, rank, iterations, lambda) model: MatrixFactorizationModel class MatrixFactorizationModel { val userFeatures: RDD[(Int, Array[Double])] val productFeatures: RDD[(Int, Array[Double])] } One we have the trained model, we can now perform predictions. Streaming data We now want to build a streaming recommender system. For this scenario, we will assume that the observations take the form of a Spark's Discretised Stream or DStream. With DStreams we consume the stream as mini-batches of RDDs over a certain interval window. We can for instance, use the first mini-batch to initialise the model and the following batches to continuously train the model. One immediate advantage of using observations as a stream is that we no longer need to keep the entirety of the data in memory or read it from storage. If we consider the batch implementation with a very large dataset if we had a single new observation and wanted to retrain the model, we would have to, for instance, read several million ratings from a database. With a streaming variant we can use that single observation to update the latent factors. We will try to recreate Spark's batch ALS API by allowing model training using a ratings RDD, however this time, we consume each RDD from the stream mini-batch and will incrementally train the model as observations trickle in. First, we start by establishing the quantities and data structures needed to implement streaming ALS. We've seen in the previous slides that the recursions for the gradient calculation take the following form, here presented in pseudo-code: userBias += gamma * (error - lambda \\* userBias) userFeature(i) += gamma * (error prodFeature(j) - lambda * userFeature(i)) We create a Factor class to encapsulate the features and the corresponding bias. We recall that in the Spark ALS implementation, the features were stored in RDDs typed as a tuple of (id, Array), where now we have an equivalent form of (id, Factor) which allows us to capture the bias. I'll now provide a quick overview of the steps required to go from the initial ratings stream to the trained model, in terms of Spark's RDD operations. Similarly to Spark's ALS, we can assume that the model data will be in the form of Rating's RDDs. These, as we've seen, will correspond to a mini-batch of ratings from our data stream. We first need to create the initial user and product latent factors for the observed data and we would start by creating two separate RDDs from the data, one keyed by user id, the other by product id. For each entry of those new RDDs (the user and product indexed ones) we will now generate a random vector of features. This can be done by simply filling a vector of size rank with random uniform values, but as you will recall, we now also have a bias associated with each entry, which will initially also be set to a random value. We now join the incoming ratings, with the generated user factors (using the user id as the key) getting a resulting RDD consisting of product ids, user ids, ratings and user factors (and the same thing for products and product factors). Finally, we have these two joint RDDs which have all the necessary quantities needed to calculate the partial gradient in each element. Recalling how to calculate a predicted rating in streaming ALS, we need the global bias, the user and product bias and the corresponding user and product latent vectors. This is straightforward to calculate for each element as we can see from the pseudo-code. Here the dot function is simply a function to calculate the dot product treating the two factor arrays as vectors. \\[ \\hat{\\mathsf{R}}_{x,y}=\\mu + b_x + b_y + \\mathsf{U}_x^T \\cdot \\mathsf{P}_y \\] prediction = dot(userFactors.features, itemFactors.features) + userFactors.bias + itemFactors.bias + bias Given the prediction, the error is also straightforward to calculate, since the real rating is also included in this RDD. \\[ \\epsilon_{x,y} = \\mathsf{R}_{x,y} - \\hat{\\mathsf{R}}_{x,y} \\] eps = rating - prediction And now, since we have the error, we can also easily calculate the update term for the user and product features. As mentioned previously, gamma and lambda are known model parameters which we pick ourselves when instantiating the streaming ALS algorithm. \\[ \\gamma \\left(\\epsilon_{x,y}\\mathsf{U}_x-\\lambda^{\\prime}\\mathsf{P}_y\\right) \\] (0 until rank).map { i => gamma * (eps * userFactors.features(i) - lambda * itemFactors.features(i)) } Finally, we update the user and product biases given the model parameters and the previous bias. \\[ \\gamma \\left(\\epsilon_{x,y}-\\lambda_y b_y\\right) \\] gamma * (eps - lambda * itemFactors.bias) These calculated gradients can now be mapped to a new RDD which we will use to update the final biases and latent factors. The last step is to split the gradients according to user and product, and finally, when in possession of all the individual gradients, we reduce them into latent factors by performing an aggregated sum for each user and product. So these steps define the entirety of the streaming ALS operation. For each observation window, we calculate the latent factors RDD, and on the following window we update these factors, given the current observations. We've covered the initialisation case, that is, we assumed the case where the model is not initialised and we received the first mini-batch of ratings. If, on the following window, we receive ratings for previously unseen user or products, the procedure is exactly the same, that is, we generate random factors and update as described. Now I'll just quickly cover the case where we get some ratings from users or for some product we've already seen. For this new set of observations, we proceed exactly as previously, that is, we split the data into separate RDDs, each one containing the ratings but keyed by user and product id. I'll assume that we get a mixture of completely new data, that is, unseen user and products and some ratings for previously seen users and products shown in red. The difference now is that, instead of assigning random factors and biases to each entry of these RDDs, we perform a full outer join between them and the current latent factors. The strategy is then to keep the matching existing latent factor and create random features and biases just for the user and product entries we haven't seen before. Now that we are in possession of this joint RDD, we can apply exactly the same steps as previously to update the factors and repeat these steps for all future incoming observations, allowing us to continuously update the model. It is now easy to see that, in the limit situation where we only have one new rating, we would now only have to update a single entry of the latent factors RDD, in contrast with the batch method, where the entirety of the factors would be used in the ALS step. Let's look at some results comparing the streaming implementation with the Spark's batch implementation. The dataset we have chosen to use in these tests is one of the MovieLens' datasets. These dataset are a widely used data in recommendation engine research. They are managed by the Lens corporation and are freely available for non-commercial applications. These datasets come in several variants, namely a small variant, useful for a quick algorithm prototyping and testing, and a full variant, with approximately 26 million ratings (from 45,000 movies and 270,000 users), useful for a more comprehensive testing and benchmark. The data is available as a set of Comma Separated Value files, each containing different variables, but we are mainly interested in the ratings file which contains four variables: a unique user and movie id, represented as integers, a rating represented by a value from 0 to 5 with steps of 0.5 and a timestamp for when the movie was rated by this user. First, we'll start by training a batch ALS model using the MovieLens data. We assume that we already have the observations as an RDD of ratings and simply split the data into 80% for training and 20% for validation. val split: Array[RDD[Rating]] = ratings.randomSplit(0.8, 0.2) val model = ALS.train(split(0), rank, iter, lambda) Here, we won't show the steps to determine the best parameters for this dataset, were we performed a simple parameter grid search over a number of possible candidates. The Spark ALS API is quite simple and to train the model we simply pass the training RDD and the parameters. We can now use the remaining 20% of the observations to calculate the RMSE between the model predictions and the actual ratings. We now can persist the validation RDD, so we can use the exact same one for the streaming ALS run. val predictions: RDD[Rating] = model .predict(split(1).map { x =>(x.user, x.product)) } val pairs = predictions .map(x => ((x.user, x.product), x.rating)) .join( split(1) .map(x => ((x.user, x.product), x.rating)) .values val RMSE = math.sqrt( pairs.map(x => math.pow(x._1 - x._2, 2)).mean()) In order to test the streaming version, we first need to define a data source. We start with the original MovieLens data and remove all the ratings from the validation observations. We then create a simulated stream of observations using Kafka, with an interval of 5 seconds and with 1000 observations in each mini-batch. These are arbitrary numbers, chosen just for practical reasons. We could have, for instance, a single observation in each mini-batch. It is not guaranteed that the best parameters (namely rank and lambda) chosen for the batch version are the best for the streaming implementation, however we've decided to use the same ones. For each mini-batch we then incrementally train the model and calculate the RMSE up to that point. Given the actual ratings in the validation set and the model's prediction, the RMSE calculation is the same as in the batch version. And looking at the results, we can see that with each mini-batch (of 1000 observations), the RMSE from the streaming version (in blue) is edging towards the batch value (plotted as the horizontal dashed line). Caveats However, streaming ALS has pitfalls which we have to take into account. Cold start An issue, which is shared with batch ALS, is usually called the cold start problem. This refers to initial point in a recommender engine where we have too few observations to make meaningful predictions. As we now know, when having a small number of ratings, since our latent factors are initialised to random numbers, most of our predicted ratings will also be random. Although this is not an exclusive problem to the streaming implementation, we might be tempted, since the system is suited for realtime recommendations, to immediately start serving predictions. It might be wise to exercise caution and train the model offline with a larger dataset or at least perform some model diagnostics to check how sensible our predictions are. Hyperparameter estimation Another challenge we encounter is hyperparameter estimation. In the batch ALS case, we can perform a grid search for instance and estimate the hyperparameters. If, after some time, we find ourselves with a new ratings or even new product and users, we can simply repeat this procedure using the totality of the data. As an example, if in batch ALS at any point we wish to estimate the model with a different rank, this would be perfectly acceptable. In the streaming case, we can't do that. When we have a new batch of observations, we assume that previous ones were discarded since they are already incorporated in the latent factors. If they weren't and we keep all the observations in the stream, we might as well use batch ALS. A solution is to perform a grid search in parallel from the start and prune the least performant models as time progresses. This has the disadvantage of being expensive in terms of resources, since we have to keep several models simultaneously and again, we have the cold start problem surfacing. This means that we have no guarantee that the best parameters for a initial batch with few observations will still be the best further on. Performance Also, there are some performance considerations. As we've seen, we implement some operations which can be costly in a Spark setting. We have several join operations which can lead to a considerable amount of data shuffling between partitions. Care must be taken into choosing an appropriate partitioning strategy to minimise data shuffling. Spark's implementation of batch ALS uses a specific method called blocked ALS, which computes outgoing and ingoing links between user and products vectors and then partitioning them in blocks in order to minimise data transfer between nodes. Also, to make predictions we might have to try and perform random access to the latent factors RDDs. This also can be quite inefficient since we are using lookup methods. If you want to get straight setting up your own distributed recommendation engine, I highly suggest you start with Spark's builtin solution. I would highly recommended looking at the jiminy project (part of the radanalytics.io community), a micro-service oriented complete recommendation engine, ready to deploy on OpenShift. The engine is split into services such as a predictor and a modeler, along with a front-end and tools to simplify tasks (such as using the MovieLens data) and it's a great way to look at how to put a modern recommender engine together and also a great code read.");
docs.push({"id": 43, "title": "A streaming ALS implementation", "url": "/a-streaming-als-implementation.html"});
index.add(44, "Gemini This site is also available on Gemini: gemini://hollowed.space. A recommended graphical Gemini browser is Lagrange or Amfora for a text-based/terminal browser. Syntax Gemini syntax is quite similar to Markdown Setup Using the agate server ");
docs.push({"id": 44, "title": "Gemini", "url": "/gemini.html"});
index.add(45, "Pandas basics Column operations Renaming columns import pandas as pd import numpy as np import warnings warnings.filterwarnings('ignore') df = pd.DataFrame({ 'a':np.random.randn(6), 'b':np.random.choice( [5,7,np.nan], 6), 'c':np.random.choice( ['foo','bar','baz'], 6), }) df.head() a b c 0 0.549838 5.0 baz 1 0.658684 NaN foo 2 -0.784545 NaN foo 3 0.204787 5.0 foo 4 1.206179 5.0 foo df.rename(columns={\"a\": \"new_name\"}, inplace=True) df.columns Index(['new_name', 'b', 'c'], dtype='object') Using a mapping function. In this case str.upper(): df.rename(columns=str.upper, inplace=True) df.columns Index(['NEW_NAME', 'B', 'C'], dtype='object') We can also use a lambda. For instance, using lambda x: x.capitalize() would result: df.rename(columns=lambda x: x.capitalize(), inplace=True) df.columns Index(['New_name', 'B', 'C'], dtype='object') A list of column names can be passed directly to columns. df.columns = [\"first\", \"second\", \"third\"] df.columns Index(['first', 'second', 'third'], dtype='object') Dropping columns A column can be dropped using the .drop() method along with the column keyword. For instance in the dataframe df: df first second third 0 0.549838 5.0 baz 1 0.658684 NaN foo 2 -0.784545 NaN foo 3 0.204787 5.0 foo 4 1.206179 5.0 foo 5 -0.898500 5.0 baz We can drop the second column using: df.drop(columns='second') first third 0 0.549838 baz 1 0.658684 foo 2 -0.784545 foo 3 0.204787 foo 4 1.206179 foo 5 -0.898500 baz The del keyword is also a possibility. However, del changes the dataframe in-place, therefore we will make a copy of the dataframe first. df_copy = df.copy() df_copy first second third 0 0.549838 5.0 baz 1 0.658684 NaN foo 2 -0.784545 NaN foo 3 0.204787 5.0 foo 4 1.206179 5.0 foo 5 -0.898500 5.0 baz del df_copy['second'] df_copy first third 0 0.549838 baz 1 0.658684 foo 2 -0.784545 foo 3 0.204787 foo 4 1.206179 foo 5 -0.898500 baz Yet another possibility is to drop the column by index. For instance: df.drop(columns=df.columns[1]) first third 0 0.549838 baz 1 0.658684 foo 2 -0.784545 foo 3 0.204787 foo 4 1.206179 foo 5 -0.898500 baz Or we could use ranges, for instance: df.drop(columns=df.columns[0:2]) third 0 baz 1 foo 2 foo 3 foo 4 foo 5 baz ");
docs.push({"id": 45, "title": "Pandas basics", "url": "/pandas-basics.html"});
index.add(46, "Python Pandas Subsetting and indexing Indexing performance Let's assume the case where you a column BOOL with values Y or N that you want to replace with an integer 1 or 0 value. The inital1 instinct would be to do something like: df[\"BOOL\"] = df[\"BOOL\"].eq(\"Y\").mul(1) This will result in the warning SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead Pandas documentation recommends the usage of the following idiom, since it can be considerably faster: df.loc[:, (\"BOOL\")] = df.loc[:, (\"BOOL\")].eq(\"Y\").mul(1) and Pythonic? \u21a9 ");
docs.push({"id": 46, "title": "Python Pandas", "url": "/python-pandas.html"});
index.add(47, "Distance metrics L-p metrics Manhattan distance (L1) Given two vectors \\(p\\) and \\(q\\), such that \\[ \\begin{aligned} p &= \\left(p_1, p_2, \\dots,p_n\\right) \\\\ q &= \\left(q_1, q_2, \\dots,q_n\\right) \\end{aligned} \\] we define the Manhattan distance as: \\[ d_1(p, q) = \\|p - q\\|_1 = \\sum_{i=1}^n |p_i-q_i| \\] Euclidean distance (L2) In general, for points given by Cartesian coordinates in \\(n\\)-dimensional Euclidean space, the distance is \\[ d(p,q)=\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\cdots +(p_{i}-q_{i})^{2}+\\cdots +(p_{n}-q_{n})^{2}} \\] Cluster distances Within-cluster sum of squares (WCSS) Given a set of observations (\\(x_1, x_2,\\dots,x_n\\)), where each observation is a \\(d\\)-dimensional real vector, \\(k\\)-means clustering aims to partition the \\(n\\) observations into \\(k\\) (\\(\\leq n\\)) sets \\(S=\\lbrace S_1, S_2, \\dots, S_k\\rbrace\\) so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find: \\[ {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}|S_{i}|\\operatorname {Var} S_{i} \\] where \\(\\mu_i\\) is the mean of points in \\(S_i\\). This is equivalent to minimizing the pairwise squared deviations of points in the same cluster: \\[ {\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\,{\\frac {1}{2|S_{i}|}}\\,\\sum _{\\mathbf {x} ,\\mathbf {y} \\in S_{i}}\\left\\|\\mathbf {x} -\\mathbf {y} \\right\\|^{2}} \\] The equivalence can be deduced from identity \\[ \\] Because the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS) which follows from the law of total variance.");
docs.push({"id": 47, "title": "Distance metrics", "url": "/distance-metrics.html"});
index.add(48, "Site details Assets This site's CSS takes 2796 bytes. This, however, does not include the following dependencies: LaTeX processor (KaTeX) custom monospace font (Julia Mono). The minimal CSS is heavily inspired by 58 bytes of css to look great nearly everywhere. The site is generated from a set of Markdown files using a Go pre-processor, which mainly converts the Mardown to HTML and gathers some metadata (such as gathering the backlinks for each page). The generator also automatically detects whether a page includes mathematical notation, and if so, the proper dependencies are added. Search The site is searchable from here. The search page also allows for query string searches using the q keyword, for instance: /search.html?q=statistics This allows you to add a custom search engine to most modern browsers, such as Firefox or Chrome.");
docs.push({"id": 48, "title": "Site details", "url": "/site-details.html"});
index.add(49, "Shell configurations According to Bash's man: /bin/bash The bash executable /etc/profile The system-wide initialization file, executed for login shells ~/.bash_profile The personal initialization file, executed for login shells ~/.bashrc The individual per-interactive-shell startup file ~/.bash_logout The individual login shell cleanup file, executed when a login shell exits ~/.inputrc Individual readline initialization file With zsh, .zshrc is always read for an interactive shell, whether it's a login one or not.");
docs.push({"id": 49, "title": "Shell configurations", "url": "/shell-configurations.html"});
index.add(50, "Digital garden Digital gardens are places where information grows. Challenges Chronological is the wrong metaphor, but how to capture time and sequence? Example, have a Git commit hash as well as a recent changes history. ");
docs.push({"id": 50, "title": "Digital Garden", "url": "/digital-garden.html"});
index.add(51, "Containers Notes on containers.");
docs.push({"id": 51, "title": "Containers", "url": "/containers.html"});
index.add(52, "Counterfactuals A special type of Explainability. Desiderata According to Verma et al 1 the counterfactual desiderata is: Validity Actionability Sparsity Data manifold closeness Causality Amortised inference Validity We assume that a counterfactual is valid if it solves the optimisation as states in Wachter et al2. If we defined the loss function as \\[ L(x,x^{\\prime},y^{\\prime},\\lambda)=\\lambda\\cdot(\\hat{f}(x^{\\prime})\u2212y^{\\prime})^2+d(x,x^{\\prime}), \\] we can define the counterfactual as \\[ \\arg \\underset{x^{\\prime}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\cdot(\\hat{f}(x^{\\prime})\u2212y^{\\prime})^2+d(x,x^{\\prime}) \\] where: * \\(x \\in \\mathcal{X}\\) is the original data point * \\(x^{\\prime} \\in \\mathcal{X}\\) is the counterfactual * \\(y^{\\prime} \\in \\mathcal{Y}\\) is the desired label * \\(d\\) is a distance metric to measure the distance between \\(x\\) and \\(x^{\\prime}\\). this could be a L1 or L2 distance, a quadratic distance, etc. Actionability Still according to 2, actionability refers to the ability of a counterfactual method to separate between mutable and immutable features. Immutable, and additionally legally protected features, shouldn't be changed by a counterfactual implementation. Formally, if we defined our set of mutable (or actionable) features as \\(\\mathcal{A}\\), we have \\[ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\cdot(\\hat{f}(x^{\\prime})\u2212y^{\\prime})^2+d(x,x^{\\prime}) \\] Sparsity According to 2 Shorter counterfactuals are easier to understand and an effective counterfactual implementation should change the least amount of features as possible. If a sparsity penalty term is added to our definition \\[ g(x^{\\prime}-x) \\] which increases the more features are changed and could be a L0 or L1 metric, for instance. We can then define the counterfactual as \\[ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\cdot(\\hat{f}(x^{\\prime})\u2212y^{\\prime})^2+d(x,x^{\\prime})+g(x^{\\prime}-x) \\] Data manifold closeness Still according to 2, data manifold closeness is the property which guarantees that the counterfactual will be as close to the training data as possible. This can translate into a more \"realistic\" counterfactual, since it is possible that the counterfactual would take extreme or never seen before values in order to satisfy the previous conditions. Formally, we can write a penalty term for the adherence to the training data manifold, \\(\\mathcal{X}\\) as \\(l(x^{\\prime};\\mathcal{X})\\) and the define the counterfactual as \\[ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\cdot(\\hat{f}(x^{\\prime})\u2212y^{\\prime})^2+d(x,x^{\\prime})+g(x^{\\prime}-x)+l(x^{\\prime};\\mathcal{X}) \\] Causality Causality refers to the property where feature changes will impact dependent features. That is, we no longer assume that all features are independent. This implies that the counterfactual method needs to mantain the causal relations between features. Amortised inference Amortised inference refers to the property of a counterfactual search to provide multiple counterfactuals for a single data point. Alternative methods Constraint solvers An alternative method to find counterfactuals is to use constraint solvers. This is explored more in-depth in Counterfactuals with Constraint Solvers. Verma, Sahil, John Dickerson, and Keegan Hines. \"Counterfactual Explanations for Machine Learning: A Review.\" arXiv preprint arXiv:2010.10596 (2020). \u21a9 Wachter, Sandra, Brent Mittelstadt, and Chris Russell. \"Counterfactual explanations without opening the black box: Automated decisions and the GDPR.\" Harv. JL & Tech. 31 (2017): 841. \u21a9\u21a9\u21a9\u21a9 ");
docs.push({"id": 52, "title": "Counterfactuals", "url": "/counterfactuals.html"});
index.add(53, "Langton's Ant Last week, at the North East Functional Programming meet up, we were given a code Kata consisting of the Langton's ant algorithm. I've had a go at Scala but decided later on to put a live version in this blog. I considered several implementation options, such as scala.js and Elm, but in the end decided to implement it in plain Javascript. Add ant");
docs.push({"id": 53, "title": "Langton's Ant", "url": "/langtons-ant.html"});
index.add(54, " \"/assets/katex.min.css\" Js: /assets/katex.min.js /assets/auto-render.min.js t as mixture of Normals (Based on Rasmus B\u00e5\u00e5th's post) A scaled \\(t\\) distribution, with \\(\\mu\\) mean, \\(s\\) scale and \\(\\nu\\) degrees of freedom, can be simulated from a mixture of Normals with \\(\\\\mu\\) mean and precisions following a Gamma distribution: \\[ \\begin{aligned} y &\\sim \\mathcal{N}\\left(\\mu,\\sigma\\right) \\\\\\\\ \\sigma^2 &\\sim \\mathcal{IG}\\left(\\frac{\\nu}{2},s^2\\frac{\\nu}{2}\\right) \\end{aligned} \\] Since I've recently pickep up again the crystal-gsl in my spare time, I've decided to replicate the previously mentioned post using a Crystal one-liner. To simulate 10,000 samples from \\(t_2\\left(0,3\\right)\\) using the mixture, we can then write: samples = (0..10000).map { |x| Normal.sample 0.0, 1.0/Math.sqrt( Gamma.sample 1.0, 9.0 ) } We can see the mixture distribution (histogram) converging nicely to the \\((t_2(0,3)\\) (red): ");
docs.push({"id": 54, "title": "t as mixture of Normals", "url": "/t-as-mixture-of-normals.html"});
index.add(55, "Machine Learning Notes on machine learning. Topics Synthetic data generation Explainability K-means clustering Metrics Error metrics Distance metrics Frameworks Cookiecutter Data Science ");
docs.push({"id": 55, "title": "Machine Learning", "url": "/machine-learning.html"});

        const query = new URLSearchParams(document.location.search).get("q");

        const input = document.getElementById('search_terms');
        const results = document.getElementById('results');
        const button = document.getElementById('search_button');

        input.addEventListener("keyup", function(event) {
            // Number 13 is the "Enter" key on the keyboard
            if (event.keyCode === 13) {
                // Cancel the default action, if needed
                event.preventDefault();
                // Trigger the button element with a click
                search_button.click();
            }
        });

        let create_link = function(title, url) {
            let a = document.createElement('a');
            let linkText = document.createTextNode(title);
            a.appendChild(linkText);
            a.title = title;
            a.href = url;
            return a
        }

        let search = function() {

            results.innerHTML = "";
            let matches = index.search(input.value);
            console.log(matches);
            let header = document.createElement("h2");
            header.appendChild(document.createTextNode(`Found ${matches.length} matches:`));
            results.appendChild(header);
            for (let match of matches) {
                let doc = docs[match-1];
                let div = document.createElement("div");
                let a = create_link(doc.title, doc.url);
                div.appendChild(a);
                results.appendChild(div);
            }
        }

        if (query!=null && query!=="") {
            input.value = query;
            search();
        }

    </script>
</body>