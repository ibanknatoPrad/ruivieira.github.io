<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/favicons/favicon.ico">

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Notes on Serving models with Seldon">
    <meta name="robots" content="index">
    <link rel="canonical" href="https://ruivieira.dev/serving-models-with-seldon.html">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
          integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
            integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
            crossorigin="anonymous"></script>

        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
            integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>

    <script src="/assets/mark.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Nunito:400,300i,800&display=swap" rel="stylesheet"/>
    <link href="/assets/style.css" rel="stylesheet">
    <style>
        /*Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>*/

        .hljs {
            display: block;
            overflow-x: auto;
            padding: 0.5em;
            background: white;
            color: black;
            -webkit-text-size-adjust: none;
        }

        .hljs-string,
        .hljs-tag .hljs-value,
        .hljs-filter .hljs-argument,
        .hljs-addition,
        .hljs-change,
        .hljs-name,
        .apache .hljs-tag,
        .apache .hljs-cbracket,
        .nginx .hljs-built_in,
        .tex .hljs-formula {
            color: #888;
        }

        .hljs-comment,
        .hljs-shebang,
        .hljs-doctype,
        .hljs-pi,
        .hljs-javadoc,
        .hljs-deletion,
        .apache .hljs-sqbracket {
            color: #ccc;
        }

        .hljs-keyword,
        .hljs-tag .hljs-title,
        .ini .hljs-title,
        .lisp .hljs-title,
        .http .hljs-title,
        .nginx .hljs-title,
        .css .hljs-tag,
        .hljs-winutils,
        .hljs-flow,
        .apache .hljs-tag,
        .tex .hljs-command,
        .hljs-request,
        .hljs-status {
            font-weight: bold;
        }
    </style>
    <title>ruivieira.dev - Serving models with Seldon</title>
    <script data-goatcounter="https://ruivieira-dev.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
    <style>
        #search_terms {
            font-size: 1rem;
            font-family: Nunito;
            width: 40%;
        }
        #search_terms::placeholder {
            color: #bbb;
        }
        #search_button {
            background-color: #eee;
            border: none;
            color: black;
            padding: 0.25rem 0.25rem;
            font-size: 1rem;
            font-family: Nunito;
            text-decoration: none;
            cursor: pointer;
            border-radius: 5px;
            width: 3rem;
        }
    </style>
</head>
<body>
<div id="grid">

    <div id="content">
        <h1 id="serving-models-with-seldon">Serving models with Seldon</h1>
<p>Deploying machine learning models in production comes with several requirements.
We must manage the model lifecycle. We need reproducibility and typically use containerised workflows.</p>
<p><a href="https://github.com/SeldonIO/seldon-core">Seldon</a> is a tool which aims at providing a production workflow for machine learning models, allowing to build model serving containers which expose well-defined APIs.</p>
<p>In this post, I'll show how to create a simple model and how to deploy it with Seldon. The model is a customer segmentation one. The goal is to classify a customer according to a segment (0, 1 or 2), according to its age, income, whether they engaged with previous campaigns and the campaign type.</p>
<p>Once we train the model, we deploy it with Seldon in a container orchestration
platform such as <a href="https://kubernetes.io/">Kubernetes</a> and <a href="https://www.openshift.com/">OpenShift</a>.</p>
<h2 id="create-data">Create data</h2>
<p>We use the Python's <a href="https://scikit-learn.org/stable/"><code>scikit-learn</code></a> to train our model.
However, we must first simulate some data to train it.
We start by simulating the users age ($a$) and income ($c$). We assume income is correlated with age.</p>
<p>$$
c|a \sim \mathcal{N}\left(a + 20, 100\right)  \\\\
a|k \sim \mathcal{U}\left(A_k, B_k\right),\quad A=\left\lbrace16, 25, 50, 61\right\rbrace,B=\left\lbrace24, 49, 60, 90\right\rbrace  \\\\
k \sim \mathcal{M}\left(4, \left\lbrace 0.15, 0.4, 0.2, 0.25\right\rbrace\right)
$$</p>
<p><img src="./images/seldon_segments.png" alt=""></p>
<p>Let's assume we have eight distinct events ($e=\left(0, 1, \dots, 7\right)$). We sample them from a multinomial
distribution and also assume that two different age bands have different distributions, just to add some variation.</p>
<p>$$
e = \begin{cases} \mathcal{M}\left(7, \left\lbrace 0.026, 0.195, 0.156, 0.208, 0.130, 0.205, 0.078 \right\rbrace\right) &amp; \text{if}\ a &lt; 50 \\\\
\mathcal{M}\left(7, \left\lbrace 0.052, 0.143, 0.169, 0.182, 0.164, 0.182, 0.104 \right\rbrace\right) &amp; \text{if}\ a \geq 50
\end{cases}
$$</p>
<p><img src="./images/seldon_hist_event_income.png" alt=""></p>
<p>The responses are calculated as <code>0</code> or <code>1</code>, representing &quot;true&quot; or &quot;false&quot;, and sampled from Bernoulli
distributions, with different distributions depending on the event, again just to add some variation.</p>
<p>$$
r = \begin{cases}
\text{Bernoulli}\left(0.6\right) &amp; \text{if}\ e \in \left(2, 3, 4, 6\right) \\\\
\text{Bernoulli}\left(0.4\right) &amp; \text{if}\ e \in \left(1, 5, 7\right)
\end{cases}
$$</p>
<p>To predict the response of a customer, we use a logistic model, with coefficients $\beta_{age}=-0.0004$ and $\beta_{income}=0.0001$. For the customer level, we use a negative binomial model with coefficients $\beta_{age}=-0.0233$ and $\beta_{income}=0.0054$.
This results in the following distribution of customer levels:</p>
<p><img src="./images/seldon_level.png" alt=""></p>
<p>Finally, we create the response according to negative binomial model with coefficients $\beta_{level}=0.1862$ and $\beta_{response}=0.2076$. We get the following segments, stratified by age and income:</p>
<p><img src="./images/seldon_segments.png" alt=""></p>
<h2 id="train-model">Train model</h2>
<p>Now that we have our simulated data, we can train a model.
Generally, it is straightforward to train model data when in <code>pandas</code> data frame format.
Let's proceed with creating a data frame with the data we've just generated:</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

data = { <span class="hljs-string">&#x27;age&#x27;</span>: age, <span class="hljs-string">&#x27;income&#x27;</span>: income,
         <span class="hljs-string">&#x27;class&#x27;</span>: _class, <span class="hljs-string">&#x27;response&#x27;</span>: response,
         <span class="hljs-string">&#x27;segment&#x27;</span>: segment, <span class="hljs-string">&#x27;events&#x27;</span>: events }

df = pd.DataFrame(data)
</code></pre>
<p>We now create the training and testing datasets. The first thing is to define the classifier's <code>inputs</code> and <code>outputs</code> and then splitting each of them into training and testing. Here I have used a split of 60%/40% for training and testing respectively.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split

cols = [<span class="hljs-string">&#x27;age&#x27;</span>, <span class="hljs-string">&#x27;income&#x27;</span>, <span class="hljs-string">&#x27;response&#x27;</span>, <span class="hljs-string">&#x27;events&#x27;</span>]
inputs = df[cols]
outputs = df[<span class="hljs-string">&#x27;segment&#x27;</span>]

<span class="hljs-comment"># split dataset</span>
X_train, X_test, y_train, y_test = \
    train_test_split(inputs,
                     outputs,
                     test_size=<span class="hljs-number">0.4</span>,
                     random_state=<span class="hljs-number">23</span>)
</code></pre>
<p>We use a Random Forest classifier as the underlying algorithm for our model.
These are available in <code>sciki-learn</code> with the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"><code>RandomForestClassifier</code></a> class.
However, <code>scikit-learn</code> does not support categorical variables out of the box <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.
To deal with them, we build a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"><code>Pipeline</code></a>, which allows to chain multiple transformations to our data, including a categorical variable processor, such as <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html"><code>OrdinalEncoder</code></a> <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>.
We use <a href="https://github.com/scikit-learn-contrib/sklearn-pandas"><code>DataFrameMapper</code></a> to apply the encoder to the <code>response</code> and <code>events</code> columns and leave the remaining unchanged.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing
<span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_RF_pipeline</span>(<span class="hljs-params">inputs, outputs, rf=<span class="hljs-literal">None</span></span>):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> rf:
        rf = RandomForestClassifier()
    pipeline = Pipeline([
        (<span class="hljs-string">&quot;mapper&quot;</span>, DataFrameMapper([
            ([<span class="hljs-string">&#x27;response&#x27;</span>, <span class="hljs-string">&#x27;events&#x27;</span>], 
                preprocessing.OrdinalEncoder()),
            ([<span class="hljs-string">&#x27;age&#x27;</span>, <span class="hljs-string">&#x27;income&#x27;</span>], <span class="hljs-literal">None</span>)
        ])),
        (<span class="hljs-string">&quot;classifier&quot;</span>, rf)
    ])
    pipeline.fit(inputs, outputs)
    <span class="hljs-keyword">return</span> pipeline
</code></pre>
<p>The actual training involves a simple hyper-parameter estimation using
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"><code>RandomizedSearchCV</code></a>. This method performs a type of parameter grid search but restricting the search to only the specified values. For the scope of this post, it is not
necessary to perform an exhaustive hyperparameter estimation.
The <code>RF_estimation</code> function returns the best-fitted model after searching with the test dataset.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">RF_estimation</span>(<span class="hljs-params">inputs, outputs,
                  estimator_steps=<span class="hljs-number">10</span>,
                  depth_steps=<span class="hljs-number">10</span>,
                  min_samples_split=<span class="hljs-literal">None</span>,
                  min_samples_leaf=<span class="hljs-literal">None</span></span>):</span>
    <span class="hljs-comment"># hyper-parameter estimation</span>
    n_estimators = [<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start=<span class="hljs-number">50</span>,
                                                stop=<span class="hljs-number">100</span>,
                                                num=estimator_steps)]
    max_depth = [<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(<span class="hljs-number">3</span>, <span class="hljs-number">10</span>, num=depth_steps)]
    max_depth.append(<span class="hljs-literal">None</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> min_samples_split:
        min_samples_split = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> min_samples_leaf:
        min_samples_leaf = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]
    bootstrap = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]
    random_grid = {<span class="hljs-string">&#x27;n_estimators&#x27;</span>: n_estimators,
                   <span class="hljs-string">&#x27;max_depth&#x27;</span>: max_depth,
                   <span class="hljs-string">&#x27;min_samples_split&#x27;</span>: min_samples_split,
                   <span class="hljs-string">&#x27;min_samples_leaf&#x27;</span>: min_samples_leaf,
                   <span class="hljs-string">&#x27;bootstrap&#x27;</span>: bootstrap}

    rf_random = RandomizedSearchCV(
                    estimator=RandomForestClassifier(),
                    param_distributions=random_grid,
                    n_iter=<span class="hljs-number">100</span>,
                    scoring=<span class="hljs-string">&#x27;neg_mean_absolute_error&#x27;</span>,
                    cv=<span class="hljs-number">3</span>, verbose=<span class="hljs-number">1</span>, random_state=<span class="hljs-number">42</span>, n_jobs=-<span class="hljs-number">1</span>)
    rf_random.fit(inputs, outputs)
    best_random = rf_random.best_estimator_

    <span class="hljs-keyword">return</span> best_random
</code></pre>
<p>After applying the parameter estimation, we take the best scoring model and calculate the MSE. Unsurprisingly (given the simple model and simulated data), we get a very good fit.</p>
<pre><code class="language-python">rf_predictions = random_forest_pipeline.predict(X_test)
print(<span class="hljs-string">f&quot;MSE: <span class="hljs-subst">{random_forest_pipeline.score(X_test, y_test)*<span class="hljs-number">100</span>}</span>%&quot;</span>)
<span class="hljs-comment"># MSE: 99.95%</span>
</code></pre>
<p>The final step is serialising the model. Serialisation is necessary since we only serve the pre-trained model.
To do so, we use the <a href="https://github.com/joblib/joblib"><code>joblib</code></a> library and save the model to a <code>model.pkl</code> file.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> joblib

<span class="hljs-comment">#save mode in filesystem</span>
joblib.dump(random_forest_pipeline, <span class="hljs-string">&#x27;model.pkl&#x27;</span>)
</code></pre>
<h2 id="deploy-model">Deploy model</h2>
<p>It is important to note that we don't need the model training code included in the Seldon server. The purpose of Seldon is not to <em>train</em> models, but to <em>deploy</em> them and manage their lifecycle.
This workflow means that a typical Seldon deployment would only include the prediction endpoint implementation and
a serialised model. This provision is made by firstly create a <em>wrapper</em> for our model which implements the Seldon endpoints.</p>
<h3 id="simple-model">Simple model</h3>
<p>We create a Python script called <code>Model.py</code> <sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.
The primary prediction endpoint uses the following signature:</p>
<pre><code class="language-python">def predict(self, X: np.ndarray, names: Iterable[str], meta: Dict = None)
</code></pre>
<p>The wrapper is straightforward, in this example.
We use the <code>joblib</code> library again, to load the serialised model <code>model.pkl</code>, and then pass through any JSON payload as inputs (<code>X</code>) to the model to get a prediction as well as using Python's default <a href="https://docs.python.org/3/library/logging.html">logging</a>
to provide some feedback.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> joblib
<span class="hljs-keyword">import</span> logging

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>
        logger.info(<span class="hljs-string">&quot;Initializing.&quot;</span>)
        logger.info(<span class="hljs-string">&quot;Loading model.&quot;</span>)
        self.model = joblib.load(<span class="hljs-string">&#x27;model.pkl&#x27;</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self, X, features_names</span>):</span>
        <span class="hljs-keyword">return</span> self.model.predict_proba(X)
</code></pre>
<p>We now build the model using the <code>s2i</code> (<a href="https://github.com/openshift/source-to-image">source-to-image</a>).
As the name implies, <code>s2i</code>'s allow to create a container image from source code, taking care of any necessary intermediate steps.
Seldon support several types of builds (such as Python, R and Java) <sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>.</p>
<p>Typically <code>s2i</code>'s rely on certain conventions (over configuration) on your application structure. A requirement when building a Seldon model using its <code>s2i</code> is to provide some specific environment variables. These are usually stored in a file located in <code>$REPO/.s2i/environment</code>. For instance, for this model we use:</p>
<pre><code class="language-ini"><span class="hljs-attr">MODEL_NAME</span>=Model
<span class="hljs-attr">API_TYPE</span>=REST
<span class="hljs-attr">SERVICE_TYPE</span>=MODEL
<span class="hljs-attr">PERSISTENCE</span>=<span class="hljs-number">0</span>
</code></pre>
<p>The <code>MODEL_NAME</code> corresponds to the script we've created previously, <code>Model.py</code> and instructs Seldon to use it as the REST endpoint provider. <code>API_TYPE</code> defines the endpoint interface. We use the REST interface, other possibilities include gRPC, for instance.</p>
<p><img src="./images/diagram.png" alt=""></p>
<p>To build the container image using the <code>s2i</code>, assuming you want an image named <code>$NAME</code> and tagged with <code>$TAG</code>, we simply need to run:</p>
<pre><code class="language-bash">$ s2i build <span class="hljs-variable">$REPO</span> \
  seldonio/seldon-core-s2i-python36:0.18 \
  <span class="hljs-variable">$NAME</span>:<span class="hljs-variable">$TAG</span>
</code></pre>
<p>You can provide the location of your source code either by specifying a remote Git repository or by passing a local one.
Once the container image builds, you can now run it using, for instance:</p>
<pre><code class="language-shell">docker run -i --rm -p 5000:5000 $NAME:$TAG
</code></pre>
<p>Let's get a prediction from the model:</p>
<pre><code class="language-shell"><span class="hljs-meta">$</span><span class="bash"> curl --header <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \
  --request POST \
  --data <span class="hljs-string">&#x27;{&quot;data&quot;:{&quot;ndarray&quot;:[[34.0, 100.0, 1, 2]]}}&#x27;</span> \
  http://localhost:5000/predict</span>
</code></pre>
<p>This will return a prediction:</p>
<pre><code class="language-json">{
    <span class="hljs-attr">&quot;data&quot;</span>: {
        <span class="hljs-attr">&quot;names&quot;</span>: [<span class="hljs-string">&quot;t:0&quot;</span>,<span class="hljs-string">&quot;t:1&quot;</span>,<span class="hljs-string">&quot;t:2&quot;</span>],
        <span class="hljs-attr">&quot;ndarray&quot;</span>: [[<span class="hljs-number">0.0</span>,<span class="hljs-number">0.9980208571211083</span>,<span class="hljs-number">0.00197914287889168</span>]]},
    <span class="hljs-attr">&quot;meta&quot;</span>: {}
}
</code></pre>
<p>This response corresponds to the probability of each segment (<code>0</code>, <code>1</code> and <code>2</code>), respectively.
We can see that a customer with this profile is classified as a segment <code>1</code> with an associated probability of 99.8%.</p>
<h3 id="with-metrics">With metrics</h3>
<p>Seldon provides basic metrics by default, covering service, predictor and model name, version and image. However, you can directly add custom metrics. Going back to our <code>Model</code> wrapper class, we add a new method called <code>metrics</code> which returns custom metrics. The metrics are compatible with Prometheus and, therefore, the metric type should be familiar if you have dealt with Prometheus before. These include, for instance:</p>
<ul>
<li>Counters</li>
<li>Gauges</li>
<li>Timers</li>
</ul>
<p>Let's add to the wrapper:</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> joblib
<span class="hljs-keyword">import</span> logging

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>
        logger.info(<span class="hljs-string">&quot;Initializing.&quot;</span>)
        logger.info(<span class="hljs-string">&quot;Loading model.&quot;</span>)
        self.model = joblib.load(<span class="hljs-string">&#x27;model.pkl&#x27;</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self, X, features_names</span>):</span>
        <span class="hljs-keyword">return</span> self.model.predict_proba(X)

    <span class="hljs-comment"># new custom metrics endpoint</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">metrics</span>(<span class="hljs-params">self</span>):</span>
        <span class="hljs-keyword">return</span> [
            <span class="hljs-comment"># a counter which will increase by the given value</span>
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;COUNTER&quot;</span>, <span class="hljs-string">&quot;key&quot;</span>: <span class="hljs-string">&quot;mycounter&quot;</span>, <span class="hljs-string">&quot;value&quot;</span>: <span class="hljs-number">1</span>},
             <span class="hljs-comment"># a gauge which will be set to given value</span>
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;GAUGE&quot;</span>, <span class="hljs-string">&quot;key&quot;</span>: <span class="hljs-string">&quot;mygauge&quot;</span>, <span class="hljs-string">&quot;value&quot;</span>: <span class="hljs-number">10</span>},
            <span class="hljs-comment"># a timer which will add sum and count metrics - assumed millisecs</span>
            {<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;TIMER&quot;</span>, <span class="hljs-string">&quot;key&quot;</span>: <span class="hljs-string">&quot;mytimer&quot;</span>, <span class="hljs-string">&quot;value&quot;</span>: <span class="hljs-number">1.1</span>},
        ]
</code></pre>
<p>If we now request a new prediction, as previously, we can see the custom metrics included in the model's response.</p>
<pre><code class="language-json">{
    <span class="hljs-attr">&quot;data&quot;</span>: {
        <span class="hljs-attr">&quot;names&quot;</span>: [<span class="hljs-string">&quot;t:0&quot;</span>,<span class="hljs-string">&quot;t:1&quot;</span>,<span class="hljs-string">&quot;t:2&quot;</span>],
        <span class="hljs-attr">&quot;ndarray&quot;</span>:[[<span class="hljs-number">0.0</span>,<span class="hljs-number">0.9980208571211083</span>,<span class="hljs-number">0.00197914287889168</span>]]},
    <span class="hljs-attr">&quot;meta&quot;</span>: {
        <span class="hljs-attr">&quot;metrics&quot;</span>: [
            {<span class="hljs-attr">&quot;key&quot;</span>:<span class="hljs-string">&quot;mycounter&quot;</span>,<span class="hljs-attr">&quot;type&quot;</span>:<span class="hljs-string">&quot;COUNTER&quot;</span>,<span class="hljs-attr">&quot;value&quot;</span>:<span class="hljs-number">1</span>},
            {<span class="hljs-attr">&quot;key&quot;</span>:<span class="hljs-string">&quot;mygauge&quot;</span>,<span class="hljs-attr">&quot;type&quot;</span>:<span class="hljs-string">&quot;GAUGE&quot;</span>,<span class="hljs-attr">&quot;value&quot;</span>:<span class="hljs-number">10</span>},
            {<span class="hljs-attr">&quot;key&quot;</span>:<span class="hljs-string">&quot;mytimer&quot;</span>,<span class="hljs-attr">&quot;type&quot;</span>:<span class="hljs-string">&quot;TIMER&quot;</span>,<span class="hljs-attr">&quot;value&quot;</span>:<span class="hljs-number">1.1</span>}]
    }
}
</code></pre>
<p>These values are available via the Prometheus endpoint.</p>
<p>The model can also be easily deployed in a container platform, for instance, OpenShift. Assuming you are logged to a cluster and your image is a registry accessible by OpenShift, you can simply deploy it using:</p>
<pre><code class="language-shell"><span class="hljs-meta">$</span><span class="bash"> oc new-app <span class="hljs-variable">$NAME</span>:<span class="hljs-variable">$TAG</span></span>
</code></pre>
<p>I hope this was useful to you.
Happy coding!</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1"  class="footnote-item"><p>As of the time of writing. <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
<li id="fn2"  class="footnote-item"><p>Other encoders are available in <code>scikit-learn</code>. I recommend you experiment with some of them. <a href="#fnref2" class="footnote-backref">↩</a></p>
</li>
<li id="fn3"  class="footnote-item"><p>You can use any file name, as long as it's consistent with <code>.s2i/environment</code>, which we'll look at soon. <a href="#fnref3" class="footnote-backref">↩</a></p>
</li>
<li id="fn4"  class="footnote-item"><p>More information can be found <a href="https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/s2i.html">here</a>. <a href="#fnref4" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
        <div class="footer">
            <span class="cc-symbol">&#127341;</span> 2020 CC BY Rui Vieira
        </div>
    </div>

    <div id="sidebar">
        <div id="sidebar-search">
            <input id="search_terms" type="search" placeholder="Search terms" />
            <button id="search_button" onclick="search()">?</button>
        </div>
        <div id="sidebar-home"><a href="/">Home</a></div>
        <div id="sidebar-all-pages"><a href="/content.html">All pages</a></div>
        <div id="sidebar-graph"><a href="/graph.html">Link network</a></div>

        <div id="sidebar-contents">
            <h3>Contents</h3>
            <ul>
<li><a href="#create-data">Create data</a></li>
<li><a href="#train-model">Train model</a></li>
<li><a href="#deploy-model">Deploy model</a>
<ul>
<li><a href="#simple-model">Simple model</a></li>
<li><a href="#with-metrics">With metrics</a></li>
</ul></li>
</ul>

                        <h3>Backlinks</h3>
            <ul>
                                <li><a href="/index.html">index</a><sup>&#5833</sup></li>
                            </ul>
                    </div>



        <div class="footer">
            modified 2 weeks ago        </div>

    </div>
    <div>

        <script>
            const input = document.getElementById('search_terms');
            const highlight = new URLSearchParams(document.location.search).get("h");

            if (highlight!=null) {
                const markInstance = new Mark("#content");
                markInstance.mark(highlight);
            }

            input.addEventListener("keyup", function(event) {
                // Number 13 is the "Enter" key on the keyboard
                if (event.keyCode === 13) {
                    // Cancel the default action, if needed
                    event.preventDefault();
                    // Trigger the button element with a click
                    search_button.click();
                }
            });

            let search = function() {
                const query = new URLSearchParams({"q": input.value});
                console.log(query.toString());
                window.location.href = "/search.html?" + query.toString();
            }
        </script>
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                renderMathInElement(
                    document.body,
                    {
                        delimiters: [
                            {left: "$$", right: "$$", display: true},
                            {left: "\\[", right: "\\]", display: true},
                            {left: "$", right: "$", display: false},
                            {left: "\\(", right: "\\)", display: false}
                        ]
                    }
                );
            });
        </script>

</body>
</html>