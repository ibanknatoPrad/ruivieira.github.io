<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Starboard Notebook</title>
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <link rel="icon" href="favicon.ico" />
    <style>
      @font-face {
        font-family: JuliaMono;
        src: url("/fonts/JuliaMono-Regular.woff2") format("woff2");
      }
    </style>
    <link href="/css/style.css" rel="stylesheet" />
    <link href="_static/starboard-notebook-0.5.6.css" rel="stylesheet" />
    <script src="_static/vendors~codemirror.chunk.js"></script>
    <script src="_static/codemirror.chunk.js"></script>
    <style>
      .cells-container {
        margin-left: 25% !important;
        max-width: 50%;
      }
      .ͼ1 .cm-line {
        display: block;
        padding: 0px 2px 0px 4px;
        font-family: "JuliaMono";
        font-size: 0.8rem;
      }
      .cm-line > * {
        font-family: "JuliaMono";
        font-size: 0.8rem;
      }
      .katex {
        font-size: 0.9rem;
      }
    </style>
  </head>
  <body>
    <div id="sidebar">
      <h2>Other pages</h2>
      <ul>
        <li><a href="/">Posts</a></li>
        <li><a href="/micro/">µ-posts</a></li>
        <li><a href="/pages/about.html">About</a></li>
      </ul>
      <h2>Codex</h2>
      <ul>
        <li><a href="/codex/machine-learning">Machine learning</a></li>
        <li><a href="/codex/python-experiments">Python experiments</a></li>
      </ul>
      <h2>Playground</h2>
      <ul>
        <li><a href="/playground/machine-learning">Machine learning</a></li>
      </ul>
    </div>

    <script>
      // The content of the notebook as a string, remember to escape the string properly.
      window.initialNotebookContent = `
%% md
# Counterfactuals

## Introduction

To create the dataset, we will use the same technique as used in Classification data.

We will create two well separated clusters in $\\mathbb{R}^2$, corresponding to two features $f_1$ and $f_2$,
with an associated outcome $y=f\\left(f_1,f_2\\right)$ with possible values true ($y=1$) and false ($y=1$).

%% python
import pandas as pd
from sklearn.datasets import make_classification

RANDOM_STATE = 23
N_FEATURES = 2
N = 100

data = make_classification(
    n_samples=N,
    n_features=N_FEATURES,
    n_redundant=0,
    n_repeated=0,
    n_classes=2,
    n_clusters_per_class=1,
    weights=None,
    flip_y=0.01,
    class_sep=2.0,
    shift=0.0,
    scale=1.0,
    shuffle=True,
    random_state=RANDOM_STATE,
)
df = pd.DataFrame(data[0], columns=["f1", "f2"])

df["y"] = data[1]

x1 = df[df["y"] == 0]["f1"]
y1 = df[df["y"] == 0]["f2"]
x2 = df[df["y"] == 1]["f1"]
y2 = df[df["y"] == 1]["f2"]
%% python
import matplotlib.pyplot as plt

colours = {0: "pink", 1: "lightgreen", 2: "#fffb96"}
edges = {0: "red", 1: "green", 2: "#cac200"}

x_min = -4
x_max = 4
y_min = -5
y_max = 1

plt.scatter(x1, y1, c=colours[0], edgecolor=edges[0], label="false")
plt.scatter(x2, y2, c=colours[1], edgecolor=edges[1], label="true")
plt.xlim([x_min, x_max])
plt.ylim([y_min, y_max])
plt.xlabel("$f_1$")
plt.ylabel("$f_2$")
plt.title("Outcome for $f(f_1, f_2)$")
plt.legend(loc="upper right")
plt.show()
%% md
We will now train an SVC model with the generated data.
%% python
from sklearn.svm import SVC

svm = SVC(gamma="scale", probability=True)
svm.fit(data[0], data[1])
%% md
And calculate the model’s accuracy.
%% python
print("SVM classification accuracy: ", svm.score(data[0], data[1]))
%% md
We are now interested in predicting the counterfactual of a certain point 𝑝=(𝑓1,𝑓2).

To do it, we first chose a point and determined its predicted outcome, 𝑦=𝑓(𝑝). We then proceed to find the closest set of features that would produce the opposite outcome, that is

𝑝′=(𝑓′1,𝑓′2):𝑦′=𝑓(𝑝′)=∼𝑦
Let’s define our input as point 𝑝=(−0.5,−1) and visualise it along with the predicted outcome.

%% python

import numpy as np

p = [-0.5, -1]

p_input = np.array(p).reshape(1, -1)
p_prediction = svm.predict(p_input)
print(f"y=f({p}) predicted as " + "true" if p_prediction[0] == 1 else "false")

%% md

We therefore are interested in the set of features 𝑝′ which will have a prediction of false.

%% python
import numpy as np
from matplotlib.colors import ListedColormap
import colorsys
import matplotlib.colors as mc

def adjust_lightness(color, amount=0.5):
    try:
        c = mc.cnames[color]
    except:
        c = color
    c = colorsys.rgb_to_hls(*mc.to_rgb(c))
    return colorsys.hls_to_rgb(c[0], max(0, min(1, amount * c[1])), c[2])


def adjust_opacity(color, amount=0.5):
    try:
        c = mc.cnames[color]
    except:
        c = color

    rgba = mc.to_rgba(c)
    return (rgba[0], rgba[1], rgba[2], amount)

cmap = ListedColormap(
    [adjust_opacity(colours[0], 0.1), adjust_opacity(colours[1], 0.2)]
)

h = 0.01
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, cmap=cmap, shading="auto")
plt.scatter(x1, y1, c=colours[0], edgecolor=edges[0], label="false")
plt.scatter(x2, y2, c=colours[1], edgecolor=edges[1], label="true")
plt.scatter(p[0], p[1], s=50, c="black", marker="x")
plt.annotate(
    "input",
    xy=(p[0], p[1]),
    xytext=(-20, -15),
    textcoords="offset points",
)

plt.xlim([x_min, x_max])
plt.ylim([y_min, y_max])
plt.xlabel("$f_1$")
plt.ylabel("$f_2$")
plt.title("Outcome for $f(f_1, f_2)$")
plt.legend(loc="upper right")
plt.show()

%% md
We define now a function, nearest, which will return the existing data point that is closest to the input and which provides a counterfactual (that is, false outcome).

%% python

def nearest(p, data):
    """Return the data point closest to p, which has a 'false' outcome"""
    data_n = data[data["y"] == 0]
    distances = dist.pairwise(data_n[["f1", "f2"]], p)
    data_n.insert(3, "distances", distances, True)
    index_min = data_n.distances.idxmin()
    nearest_point = np.array(
        [data_n.loc[index_min]["f1"], data_n.loc[index_min]["f2"]]
    ).reshape(1, -1)
    return nearest_point, index_min

%% md
## Nelder-Mead method

We also define the following functions to determine the distance used in the loss function.

The cityblock metric corresponds to the Manhattan distance. If we assume two points 𝑝 and 𝑞, their Manhattan distance is defined by:

𝑑1(𝐩,𝐪)=‖𝐩−𝐪‖1=∑𝑖=1𝑛|𝑝𝑖−𝑞𝑖|

%% python

def loss_function_l1norm(x_dash):
    L = 2 * (logreg.predict(x_dash.reshape(1, -1)) - 1) ** 2 + cdist(
        example, x_dash.reshape(1, -1), metric="cityblock"
    )
    return L


def dist_mad(cf, eg):
    manhat = [
        cdist(eg.T, cf.reshape(1, -1).T, metric="cityblock")[i][i]
        for i in range(len(eg.T))
    ]
    mad = stats.median_abs_deviation(X, scale="normal")
    return sum(manhat / mad)


def loss_function_mad(x_dash):
    target = 0
    L = lamda * (
        svm.predict_proba(x_dash.reshape(1, -1))[0][target] - 1
    ) ** 2 + dist_mad(x_dash.reshape(1, -1), p_input)
    return L

%% md
For completeness we will also show the data point which is closest to our original input 𝑝, but with a different outcome.

%% python
from scipy.optimize import minimize
from sklearn.neighbors import DistanceMetric

# nearest neighbour
dist = DistanceMetric.get_metric("euclidean")
cf_nn, i = nearest(p_input, df)

while int(svm.predict(cf_nn)) != 0:
    data_input = df.drop([i])
    cf_nn, i = nearest(p_input, data_input)

prediction = "true" if svm.predict(cf_nn)[0] == 1 else "false"
print(f"p=({cf_nn}) predicted as {prediction}")

%% md
We use the Nelder-Mead method to minimise the loss function loss_function_mad.

%% python
from scipy import stats
from scipy.optimize import minimize
from scipy.spatial.distance import cdist, pdist

pred_threshold = 0.90

# initial conditions
lamda = 0.1
x0 = np.array([0.5, 0.0]).reshape(1, -1)  # initial guess for cf

res = minimize(
    loss_function_mad,
    x0,
    method="nelder-mead",
    options={"maxiter": 1000, "xatol": 1e-8},
)
cf = res.x.reshape(1, -1)

TARGET = 0  # false
prob_target = svm.predict_proba(cf)[0][TARGET]

i = 0
while prob_target < pred_threshold:
    lamda += 0.1
    x0 = cf  # starting point is current cf
    res = minimize(
        loss_function_mad,
        x0,
        method="nelder-mead",
        options={"maxiter": 1000, "xatol": 1e-8},
    )
    cf = res.x.reshape(1, -1)
    prob_target = svm.predict_proba(cf)[0][TARGET]
    i += 1
    if i == 3000:
        print("Error condition not met after", i, "iterations")
        break

print(
    f"Found counterfactual: {cf[0]} after {i} steps. This point has "
    + f"an outcome of {svm.predict(cf)[0]} with a probability {svm.predict_proba(cf)[0].max()}"
)

%% python
plt.pcolormesh(xx, yy, Z, cmap=cmap, shading="auto")
plt.scatter(x1, y1, c=colours[0], edgecolor=edges[0], label="false")
plt.scatter(x2, y2, c=colours[1], edgecolor=edges[1], label="true")

plt.scatter(p_input[0][0], p_input[0][1], s=100, c="black", marker="x")
plt.annotate(
    "input",
    xy=(p_input[0][0], p_input[0][1]),
    xytext=(-20, -15),
    textcoords="offset points",
)

plt.scatter(cf[0][0], cf[0][1], s=100, c="black", marker="x")
plt.annotate(
    "counterfactual",
    xy=(cf[0][0], cf[0][1]),
    xytext=(-30, 10),
    textcoords="offset points",
)

plt.scatter(cf_nn[0][0], cf_nn[0][1], s=100, c="black", marker="x")
plt.annotate(
    "nearest neighbour",
    xy=(cf_nn[0][0], cf_nn[0][1]),
    xytext=(-10, -15),
    textcoords="offset points",
)

plt.xlim([x_min, x_max - 1])
plt.ylim([y_min, y_max])
plt.xlabel("$f_1$")
plt.ylabel("$f_2$")
plt.title("Counterfactual for $f(f_1, f_2)$")
plt.legend(loc="upper right")
plt.show()
`;
    </script>
    <script src="_static/starboard-notebook-0.5.6.min.js"></script>
  </body>
</html>
