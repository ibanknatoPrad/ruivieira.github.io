<!DOCTYPE html>
<head>
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/favicons/favicon.ico">

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
          integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
            integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
            crossorigin="anonymous"></script>

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
            integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>

    <script src="/assets/mark.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Nunito:400,300i,800&display=swap" rel="stylesheet"/>
    <link href="/assets/style.css" rel="stylesheet"/>
    <link href="/assets/code.css" rel="stylesheet"/>
    <title>ruivieira.dev - Introduction to Isolation Forests</title>
    <script type="application/javascript">
        var doNotTrack = false;
        if (!doNotTrack) {
            (function (i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function () {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
            ga('create', 'UA-10507665-2', 'auto');

            ga('send', 'pageview');
        }
    </script>
</head>
<style>
        #search_terms {
            font-size: 1rem;
            font-family: Nunito;
            width: 60%;
        }
        #search_terms::placeholder {
            color: #bbb;
        }
        #search_button {
            background-color: #eee;
            border: none;
            color: black;
            padding: 0.25rem 0.5rem;
            font-size: 1rem;
            font-family: Nunito;
            text-decoration: none;
            cursor: pointer;
            border-radius: 5px;
            width: 3rem;
        }
</style>
<body>

<div id="content">
    <h1>Introduction to Isolation Forests</h1>
<p><em>Isolation Forests</em> (IFs), presented in Liu<sup id="fnref:Liu"><a class="footnote-ref" href="#fn:Liu">1</a></sup> et. al (2012), are a popular algorithm used for outlier classification. In a very simplified way, the method consists of building an ensemble of
<em>Isolation Trees</em> (ITs) for a given data set and observations are deemed anomalies if they have short adjusted average path lengths on the ITs.</p>
<p>ITs, which will be covered shortly, have several properties in common with a fundamental data structure: the <a href="https://en.wikipedia.org/wiki/Binary_search_tree">Binary Search Tree</a> (BSTs). In a very simplified way, BSTs are a special instance of tree structures where keys are kept in such an order that a node search is performed by iteratively (or recursively) choosing a left or right branch based on a quantitative comparison (<em>e.g.</em> lesser or greater). Node insertion is performed by doing a tree search, using the method described previously, until reaching an <em>external node</em>, where the new node will be inserted. This allows for efficient node searches since, on average, half the tree will not be visited. To illustrate this assume the values \(x=[1, 10, 2, 4, 3, 5, 26, 9, 7, 54]\) and the respective insertion on a BST. The intermediate steps would then be as shown below.</p>
<p><img alt="" src="./images/isolationforests/bst_steps.png" /></p>
<p>One of the properties of BSTs is that, with randomly generated data, the path between the root node and the outliers will typically be shorter. We can see from the illustration below that, with our example data, the path length for (say) 7 is twice the length than for the suspicious value of 54. This property will play an important role in the IF algorithm, as we will see further on.</p>
<p><img alt="" src="./images/isolationforests/bst_path_length.png" /></p>
<h2 id="isolation-trees">Isolation Trees</h2>
<p>Since ITs are the fundamental component of IFs, we will start by describing their building process. We start by defining \(t\) as the number of trees in the IF, \(\mathcal{D}\) as the training data (contained in an \(n\)-dimensional feature space, \(\mathcal{D} \subset \mathbb{R}^n\)) and \(\psi\) as the subsampling size. The building of a IT consists then in recursively partitioning the data \(\mathcal{D}\) by sampling (without replacement) a subsample \(\mathcal{D}^{\prime}\) of size \(\psi\). We then build an isolation tree \(\mathcal{T}^{\prime}\) with this subsample (in order to later add it to the isolation forest \(\mathcal{F}\)) and the process is repeated \(t\) times.</p>
<p>To build an isolation tree \(\mathcal{T}^{\prime}\) from the subsample we proceed as follows: if the data subsample \(\mathcal{D}^{\prime}\) is indivisible, a tree is returned containing a single <em>external node</em> corresponding to the feature dimensions, \(n\).  If it can be divided, a series of steps must be performed. Namely, if we consider \(Q = \lbrace q_1,\dots,q_n\rbrace\) as the list of features in \(\mathcal{D}^{\prime}\), we select a random feature \(q \in Q\) and a random <em>split point</em> \(p\) such that </p>
\[
\min(q) &lt; p &lt; \max(q), \qquad q \in Q.
\]

<p>Based on the cut-off point \(p\), we filter the features into a BST’s left and right nodes according to</p>
\[
\mathcal{D}_l := \lbrace \mathcal{D}^{\prime} : q \in Q,  q&lt;p\rbrace \\
\mathcal{D}_r := \lbrace \mathcal{D}^{\prime} : q \in Q,  q \geq p\rbrace,
\]

<p>and return an <em>internal node</em> having an isolation tree with left and right nodes as respectively \(\mathcal{D}_l\) and \(\mathcal{D}_r\).</p>
<p>To illustrate this (and the general method of identifying anomalies in a two dimensional feature space, \(x\in\mathbb{R}^2\)) we will look at some simulated data and its processing. We start by simulating two clusters of data from a multivariate normal distribution, one centred in \(x_a=[-10, 10]\) and another centred in \(x_b=[10, 10]\), with a variance of \(\Sigma=\text{diag}(2, 2)\), that is</p>
\[
X_a \sim \mathcal{N}\left([-10, -10], \text{diag}(2, 2)\right) \\
X_b \sim \mathcal{N}\left([10, 10], \text{diag}(2, 2)\right).
\]

<p>The particular realisation of this simulation looks like this:</p>
<p><img alt="" src="./images/isolationforests/data.png" /></p>
<p>Below we illustrate the building of a <em>single</em> IT (given the data), illustrating the feature split point \(p\) and respective division of the feature list into <em>left</em> or <em>right</em> IT nodes. The process is conducted recursively until the feature list is no longer divisible. As mentioned previously, this process, the creation of an IT, is repeated \(t\) times in order to create the IF.</p>
<p> 
  <video width="100%" autoplay>
    <source src="./images/isolationforests/split.mp4" type="video/mp4">
  </video> 
</p>

<p>In order to perform anomaly detection (<em>e.g.</em> observation scoring) we will then use the IT equivalent of the BST unsuccessful search heuristics. An external node termination in an IT is equivalent to a BST unsuccessful search. Given an observation \(x\), our goal is then to calculate the score for this observation, given our defined subsampling size, that is, \(s(x,\psi)\).</p>
<p>This technique amounts to partitioning the feature space randomly until feature points are “isolated”. Intuitively, points in high density regions will need more partitioning steps, whereas anomalies (by definition away from high density regions) will need fewer splits. Since the building of the ITs is performed in a randomised fashion and using a subsample of the data, this density predictor can be average over a number of ITs, the <em>Isolation Forest</em>.</p>
<p>Intuitively, this could be done by calculating the average path length for our \(\mathcal{T}n, n=1,\dots,t\) ITs, \(\overline{h}(x)\).
However, as pointed in Liu<sup id="fnref2:Liu"><a class="footnote-ref" href="#fn:Liu">1</a></sup> <em>et. al</em> (2012), a problem with calculating this is that maximum possible height of each \(\mathcal{T}_n\) grows as \(\mathcal{O}(\log(\psi))\). To compare \(h(x)\) given different subsampling sizes, a normalisation factor, \(c(\psi)\) must be established. This can be calculated by</p>
\[
c(\psi) = \begin{cases}
2H(\psi-1)-2\frac{\psi-1}{n},\text{if}\ \psi &gt;2,\\
1, \text{if}\ \psi=2,\\
0, \text{otherwise}, 
\end{cases}
\]

<p>where \(H(i)\) is the harmonic number estimated by \(H(i)\approx\log(i) + e\).</p>
<p>Denoting \(h_{max}\) as the tree height limit and e as the <em>current path length</em>, initialised as \(e=0\) we can then calculate \(h(x)\) recursively as:</p>
\[
h(x,\mathcal{T},h_{max},e) = \begin{cases}
h(x,\mathcal{T}_{n,left},h_{max},e+1) \text{if}\ x_a &lt; q_{\mathcal{T}} \\
h(x,\mathcal{T}_{n,right},h_{max},e+1) \text{if}\ x_a \geq q_{\mathcal{T}} \\
e+c(\mathcal{T_{n,s}}) \text{if}\ \mathcal{T} \text{is a terminal node or}\ e \geq h_{max}.
\end{cases}
\]

<p>Given these quantities we can then, finally, calculate the anomaly score, \(s\) as</p>
\[
s(x,\psi) = 2^{-\frac{\text{E}[h(x)]}{c(\psi)}}
\]

<p>with \(\text{E}[h(x)]\) being the average \(h(x)\) for a collection of ITs.</p>
<h2 id="parameters">Parameters</h2>
<p>As mentioned in Liu<sup id="fnref3:Liu"><a class="footnote-ref" href="#fn:Liu">1</a></sup> <em>et. al</em> (2012), the empirical subsampling size \(\psi=2^8\) is typically enough to perform anomaly detection in a wide range of data. Regarding the number of trees, \(t\) no considerable accuracy gain is usually observed with \(t&gt;100\). In the plots below, we can see the score calculation for two point in our data, namely an outlier (\(x_o=[3.10, -12.69])\) and a normal observation (\(x_n=[8.65, 9.71]\)) with a varying number of trees and \(\psi=2^8\) (<em>left</em>) and a varying subsample size and \(t=100\) (<em>right</em>). We can see that the score value stabilised quite early on when using \(\psi=2^8\) and that very low subsampling sizes can lead to problems when classifying anomalies.</p>
<p><img alt="" src="./images/isolationforests/avg_score.png" /></p>
<p>Now that we know how to implement an IF algorithm and calculate an anomaly score, we will try to visualise the anomaly score distribution in the vicinity of the simulated data. To do so, we simply create a two dimensional lattice enclosing our data an iteratively calculate \(s(\cdot, \psi)\). The result is show below:</p>
<p><img alt="" src="./images/isolationforests/score_field.png" /></p>
<p>The above steps fully define a naive isolation forest algorithm, which when applied to the previously simulated data, result in 88% of the anomalies being correctly identified.</p>
<p><img alt="" src="./images/isolationforests/detection.png" /></p>
<p>Thanks for reading! If you have any questions or comments, please let me know on <a href="https://mastodon.social/@ruivieira">Mastodon</a> or <a href="https://twitter.com/ruimvieira">Twitter</a>.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:Liu">
<p>Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2012). <em>Isolation-Based Anomaly Detection.</em> ACM Transactions on Knowledge Discovery from Data, 6(1), 1–39. https://doi.org/10.1145/2133360.2133363&#160;<a class="footnote-backref" href="#fnref:Liu" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:Liu" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:Liu" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
    <div class="footer">
        <span class="cc-symbol">&#127341;</span> 2020 CC BY Rui Vieira
    </div>
</div>

<div id="sidebar">
    <p>
        <input id="search_terms" type="search" placeholder="Search terms"/>
        <button id="search_button" onclick="search()">?</button>
    </p>
    <p><a href="/">Home</a></p>
    <p><a href="/content.html">All pages</a></p>

    <h3>Contents</h3>
    <div class="toc">
<ul>
<li><a href="#isolation-trees">Isolation Trees</a></li>
<li><a href="#parameters">Parameters</a></li>
</ul>
</div>

    
    <h3>Backlinks</h3>
        <ul>
        
            <li><a href="/index.html">index</a><sup>&#5833</sup></li>
        
        </ul>
    



    <div class="footer">
        modified 1611740059
    </div>

</div>

<script>
const input = document.getElementById('search_terms');
const highlight = new URLSearchParams(document.location.search).get("h");

if (highlight!=null) {
    const markInstance = new Mark("#content");
    markInstance.mark(highlight);
}

input.addEventListener("keyup", function(event) {
            // Number 13 is the "Enter" key on the keyboard
            if (event.keyCode === 13) {
                // Cancel the default action, if needed
                event.preventDefault();
                // Trigger the button element with a click
                search_button.click();
            }
        });

let search = function() {
            const query = new URLSearchParams({"q": input.value});
            console.log(query.toString());
            window.location.href = "/search.html?" + query.toString();
}
</script>

</body>