<!DOCTYPE html>
<html lang="en-uk">
<head>
  <script data-goatcounter="https://ruivieira-dev.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
  <link rel="preload" href="/lib/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="/css/kbd.css" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Optimising random forest hyperparameters | Rui Vieira</title>
  <link rel = 'canonical' href = 'https://ruivieira.dev/optimising-random-forest-hyperparamaters.html'>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Optimising random forest hyperparameters" />
<meta property="og:description" content="Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:
 The number of decision trees in a random forest The split criteria Maximum depth of individual trees Minimum samples per internal node Maximum number of leaf nodes Random features per split Number of samples in bootstrap dataset  We will look at each of these hyper-parameters individually with examples of how to select them." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ruivieira.dev/optimising-random-forest-hyperparamaters.html" /><meta property="article:section" content="posts" />

<meta property="article:modified_time" content="2021-12-07T21:49:31+00:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Optimising random forest hyperparameters"/>
<meta name="twitter:description" content="Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:
 The number of decision trees in a random forest The split criteria Maximum depth of individual trees Minimum samples per internal node Maximum number of leaf nodes Random features per split Number of samples in bootstrap dataset  We will look at each of these hyper-parameters individually with examples of how to select them."/>

  
  
    
  
  
  <link rel="stylesheet" href="https://ruivieira.dev/css/styles.11452b79971f363af7cee81005cd790dc0a7c965142f8adb59e36437063f2d447687b3711ada73b6a88b3b41a41508a42a384d9620740b48cefa0ea9f5e49459.css" integrity="sha512-EUUreZcfNjr3zugQBc15DcCnyWUUL4rbWeNkNwY/LUR2h7NxGtpztqiLO0GkFQikKjhNliB0C0jO&#43;g6p9eSUWQ=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://ruivieira.dev/images/favicon.ico" />

  
  
  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

  <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;" aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/map/">All pages</a></li>
         
        <li><a href="/search.html">Search</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li>
          <a class="icon" href=" https://ruivieira.dev/pandas.html" aria-label="Previous">
            <i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i>
          </a>
        </li>
        
        
        <li>
          <a class="icon" href="https://ruivieira.dev/optaplanner.html" aria-label="Next">
            <i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i>
          </a>
        </li>
        
        <li>
          <a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
            <i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i>
          </a>
        </li>
        <li>
          <a class="icon" href="#" aria-label="Share">
            <i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i>
          </a>
        </li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html" aria-label="Facebook">
      <i class="fab fa-facebook " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&text=Optimising%20random%20forest%20hyperparameters" aria-label="Twitter">
      <i class="fab fa-twitter " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label="Linkedin">
      <i class="fab fa-linkedin " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&is_video=false&description=Optimising%20random%20forest%20hyperparameters" aria-label="Pinterest">
      <i class="fab fa-pinterest " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Optimising%20random%20forest%20hyperparameters&body=Check out this article: https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html" aria-label="Email">
      <i class="fas fa-envelope " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label="Pocket">
      <i class="fab fa-get-pocket " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label="reddit">
      <i class="fab fa-reddit " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&name=Optimising%20random%20forest%20hyperparameters&description=Typically%20the%20hyper-parameters%20which%20will%20have%20the%20most%20significant%20impact%20on%20the%20behaviour%20of%20a%20random%20forest%20are%20the%20following%3a%0a%20The%20number%20of%20decision%20trees%20in%20a%20random%20forest%20The%20split%20criteria%20Maximum%20depth%20of%20individual%20trees%20Minimum%20samples%20per%20internal%20node%20Maximum%20number%20of%20leaf%20nodes%20Random%20features%20per%20split%20Number%20of%20samples%20in%20bootstrap%20dataset%20%20We%20will%20look%20at%20each%20of%20these%20hyper-parameters%20individually%20with%20examples%20of%20how%20to%20select%20them." aria-label="Tumblr">
      <i class="fab fa-tumblr " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&t=Optimising%20random%20forest%20hyperparameters" aria-label="Hacker News">
      <i class="fab fa-hacker-news " aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>
    
    
    <div id="toc">
      <h4>Contents</h4>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#data">Data</a></li>
    <li><a href="#naive-model">Naive model</a></li>
    <li><a href="#hyperparameter-search">Hyperparameter search</a></li>
    <li><a href="#parameters">Parameters</a>
      <ul>
        <li><a href="#number-of-decision-trees">Number of decision trees</a></li>
        <li><a href="#the-split-criteria">The split criteria</a></li>
        <li><a href="#maximum-depth-of-individual-trees">Maximum depth of individual trees</a></li>
        <li><a href="#maximum-number-of-leaf-nodes">Maximum number of leaf nodes</a></li>
        <li><a href="#random-features-per-split">Random features per split</a></li>
      </ul>
    </li>
  </ul>
</nav>
      
      <h4>Related</h4>
      
      <nav>
      <ul>
      
      
        <li class="header-post toc"><a href="https://ruivieira.dev/scikit-learn.html">Scikit-learn</a></li>
      
      
      </ul>
    </nav>
    </div>
    
  </span>
</div>


  <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <header>
      <h1 class="posttitle" itemprop="name headline">
        Optimising random forest hyperparameters
      </h1>
      <div class="meta">
        
        <div class="postdate">
          
          Updated <time datetime="2021-12-07 21:49:31 &#43;0000 GMT" itemprop="datePublished">2021-12-07</time>
          <span class="commit-hash">(186a405)</span>
        </div>
        
        
        
        
      </div>
    </header>

  
    
    <div class="content" itemprop="articleBody">
      <p>Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:</p>
<ul>
<li><a href="#number-of-decision-trees">The number of decision trees</a> in a random forest</li>
<li>The split criteria</li>
<li>Maximum depth of individual trees</li>
<li>Minimum samples per internal node</li>
<li>Maximum number of leaf nodes</li>
<li>Random features per split</li>
<li>Number of samples in bootstrap dataset</li>
</ul>
<p>We will look at each of these hyper-parameters individually with examples of how to select them.</p>
<h2 id="data">Data</h2>
<p>To understand how we can optimise the hyperparameters in a random forest model, we will use [Scikit-learn|scikit-learn&rsquo;s]] <code>RandomForestClassifier</code> and a subset of <em>Titanic</em><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> dataset.</p>
<p>First, we will import the features and labels using [Pandas]].</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">import pandas as pd

train_features = pd.read_csv(&quot;data/svm-hyperparameters-train-features.csv&quot;)
train_label = pd.read_csv(&quot;data/svm-hyperparameters-train-label.csv&quot;)
</code></pre><p>Let&rsquo;s look at a random sample of entries from this dataset, both for features and labels.</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">train_features.sample(10)
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">     Pclass  Sex   Age  SibSp  Parch      Fare
404       3    0  20.0      0      0    8.6625
813       3    0   6.0      4      2   31.2750
790       3    1  30.0      0      0    7.7500
18        3    0  31.0      1      0   18.0000
331       1    1  45.5      0      0   28.5000
99        2    1  34.0      1      0   26.0000
311       1    0  18.0      2      2  262.3750
815       1    1  30.0      0      0    0.0000
44        3    0  19.0      0      0    7.8792
665       2    1  32.0      2      0   73.5000
</code></pre></div><p>Some of the available features are:</p>
<ul>
<li><code>Pclass</code>, ticket class</li>
<li><code>Sex</code></li>
<li><code>Age</code>, age in years</li>
<li><code>Sibsp</code>, number of siblings/spouses aboard</li>
<li><code>Parch</code>, number of parents/children aboard</li>
<li><code>Fare</code>, passenger fare</li>
</ul>
<!--listend-->
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">train_label.sample(10)
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">     Survived
694         0
230         1
600         1
4           0
552         0
815         0
564         0
859         0
724         1
173         0
</code></pre></div><p>The outcome label indicates whether a passenger survived the disaster.</p>
<p>As part of the typical initial steps for model training, we will prepare the data by splitting it into a training and testing subset.</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.33, random_state=23)
</code></pre><h2 id="naive-model">Naive model</h2>
<p>First we will train a &ldquo;naive&rdquo; model, that is a model using the defaults provided by <code>RandomForestClassifier</code><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. These defaults are:</p>
<ul>
<li><code>n_estimators = 10</code></li>
<li><code>criterion=’gini’</code></li>
<li><code>max_depth=None</code></li>
<li><code>min_samples_split=2</code></li>
<li><code>min_samples_leaf=1</code></li>
<li><code>min_weight_fraction_leaf=0.0</code></li>
<li><code>max_features=’auto’</code></li>
<li><code>max_leaf_nodes=None</code></li>
<li><code>min_impurity_decrease=0.0</code></li>
<li><code>min_impurity_split=None</code></li>
<li><code>bootstrap=True</code></li>
<li><code>oob_score=False</code></li>
<li><code>n_jobs=1</code></li>
<li><code>random_state=None</code></li>
<li><code>verbose=0</code></li>
<li><code>warm_start=False</code></li>
<li><code>class_weight=None</code></li>
</ul>
<p>We will instantiate a random forest classifier:</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
</code></pre><p>And training it using the <code>X_train</code> and <code>y_train</code> subsets using the appropriate <code>fit</code> method<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">true_labels = train_label.values.ravel()

rf.fit(X_train, y_train.values.ravel())
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">RandomForestClassifier()
</code></pre></div><p>We can now evaluate trained naive model&rsquo;s score.</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from sklearn.metrics import precision_score

predicted_labels = rf.predict(X_test)

precision_score(y_test, predicted_labels)
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">0.7522935779816514
</code></pre></div><h2 id="hyperparameter-search">Hyperparameter search</h2>
<p>A simple example of a generic hyperparameter search using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model%5Fselection.GridSearchCV.html"><code>GridSearchCV</code></a> method in <code>scikit-learn</code>. The score used to measure the &ldquo;best&rdquo; model is the <code>mean_test_score</code>, but other metrics could be used, such as the [OOB score in random forests|Out-of-bag (OOB)]] error.</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">parameters = {
    &quot;n_estimators&quot;:[5,10,50,100,250],
    &quot;max_depth&quot;:[2,4,8,16,32,None]

}
</code></pre><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">rfc = RandomForestClassifier()
</code></pre><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from sklearn.model_selection import GridSearchCV
</code></pre><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">cv = GridSearchCV(rfc,parameters,cv=5)
cv.fit(X_train, y_train.values.ravel())
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">GridSearchCV(cv=5, estimator=RandomForestClassifier(),
             param_grid={&#39;max_depth&#39;: [2, 4, 8, 16, 32, None],
                         &#39;n_estimators&#39;: [5, 10, 50, 100, 250]})
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">def display(results):
    print(f'Best parameters are: {results.best_params_}')
    print(&quot;\n&quot;)
    mean_score = results.cv_results_['mean_test_score']
    std_score = results.cv_results_['std_test_score']
    params = results.cv_results_['params']
    for mean,std,params in zip(mean_score,std_score,params):
        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')
</code></pre><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">display(cv)
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Best parameters are: {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 250}


0.762 + or -0.039 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 5}
0.767 + or -0.016 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 10}
0.782 + or -0.03 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 50}
0.787 + or -0.035 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 100}
0.775 + or -0.021 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 250}
0.815 + or -0.025 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 5}
0.802 + or -0.028 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 10}
0.815 + or -0.019 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 50}
0.812 + or -0.021 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 100}
0.809 + or -0.023 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 250}
0.805 + or -0.011 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 5}
0.804 + or -0.014 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 10}
0.815 + or -0.012 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 50}
0.819 + or -0.008 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 100}
0.822 + or -0.016 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 250}
0.772 + or -0.036 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 5}
0.804 + or -0.039 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 10}
0.805 + or -0.019 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 50}
0.819 + or -0.023 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 100}
0.805 + or -0.024 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 250}
0.8 + or -0.025 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 5}
0.799 + or -0.024 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 10}
0.807 + or -0.022 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 50}
0.81 + or -0.027 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 100}
0.809 + or -0.022 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 250}
0.779 + or -0.041 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 5}
0.789 + or -0.023 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 10}
0.807 + or -0.026 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 50}
0.805 + or -0.033 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 100}
0.81 + or -0.024 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 250}
</code></pre></div><h2 id="parameters">Parameters</h2>
<h3 id="number-of-decision-trees">Number of decision trees</h3>
<p>This is specified using the <code>n_estimators</code> hyper-parameter on the random forest initialisation.</p>
<p>Typically, a higher number of trees will lead to greater accuracy at the expense of model size and training time.</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">cv = GridSearchCV(rfc,{&quot;n_estimators&quot;:[2, 4, 8, 16, 32, 64, 128, 256, 512]},cv=5)
cv.fit(X_train, y_train.values.ravel())
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">GridSearchCV(cv=5, estimator=RandomForestClassifier(),
             param_grid={&#39;n_estimators&#39;: [2, 4, 8, 16, 32, 64, 128, 256, 512]})
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">results = pd.DataFrame({&quot;n_estimators&quot;: [param[&quot;n_estimators&quot;] for param in cv.cv_results_['params']],
             &quot;mean_score&quot;: list(cv.cv_results_['mean_test_score']),
             &quot;std_score&quot;: cv.cv_results_['std_test_score']})
results
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">   n_estimators  mean_score  std_score
0             2    0.775126   0.033603
1             4    0.773515   0.022884
2             8    0.807003   0.032551
3            16    0.802017   0.025230
4            32    0.802031   0.028781
5            64    0.805378   0.017020
6           128    0.810420   0.022129
7           256    0.815462   0.032160
8           512    0.810420   0.017894
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from plotnine import *

(
     ggplot(results) + geom_boxplot(aes(x='factor(n_estimators)', y='mean_score')) +
    geom_errorbar(aes(x='factor(n_estimators)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
    theme_classic() + xlab('Number of trees') + ylab('Mean score')
)
</code></pre><figure><img src="./.ob-jupyter/045f68596e633ba4fbbd72e90dbfe824838b7b4d.png"/>
</figure>

<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&lt;ggplot: (326732746)&gt;
</code></pre></div><h3 id="the-split-criteria">The split criteria</h3>
<p>At each node, a random forest decides, according to a specific algorithm, which feature and value split the tree.
Therefore, the choice of splitting algorithm is crucial for the random forest&rsquo;s performance.</p>
<p>Since, in this example, we are dealing with a classification problem, the choices of split algorithm are, for instance:</p>
<ul>
<li>Gini</li>
<li>Entropy</li>
</ul>
<p>If we were dealing with a random forest for regression, other methods (such as [Error metrics|MSE]]) would be a possible choice.
We will now compare both split algorithms as specified above, in training a random forest with our data:</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">rfc = RandomForestClassifier(n_estimators=256)

cv = GridSearchCV(rfc,{&quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]},cv=5)
cv.fit(X_train, y_train.values.ravel())
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">GridSearchCV(cv=5, estimator=RandomForestClassifier(n_estimators=256),
             param_grid={&#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;]})
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">results = pd.DataFrame({&quot;criterion&quot;: [param[&quot;criterion&quot;] for param in cv.cv_results_['params']],
             &quot;mean_score&quot;: list(cv.cv_results_['mean_test_score']),
             &quot;std_score&quot;: cv.cv_results_['std_test_score']})
results
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">  criterion  mean_score  std_score
0      gini    0.815462   0.025828
1   entropy    0.812087   0.025764
</code></pre></div><h3 id="maximum-depth-of-individual-trees">Maximum depth of individual trees</h3>
<p>In theory, the &ldquo;longer&rdquo; the tree, the more splits it can have and better accommodate the data. However, at the tree level can this can lead to overfitting.
Although this is a problem for decision trees, it is not necessarily a problem for the ensemble, the random forest.
Although the key is to strike a balance between trees that aren&rsquo;t too large or too short, there&rsquo;s no universal heuristic to determine the size.
Let&rsquo;s try a few option for maximum depth:</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">rfc = RandomForestClassifier(n_estimators=256,
                           criterion=&quot;entropy&quot;)

cv = GridSearchCV(rfc,{'max_depth': [2, 4, 8, 16, 32, None]},cv=5)
cv.fit(X_train, y_train.values.ravel())
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">GridSearchCV(cv=5,
             estimator=RandomForestClassifier(criterion=&#39;entropy&#39;,
                                              n_estimators=256),
             param_grid={&#39;max_depth&#39;: [2, 4, 8, 16, 32, None]})
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">results = pd.DataFrame({&quot;max_depth&quot;: [param[&quot;max_depth&quot;] for param in cv.cv_results_['params']],
             &quot;mean_score&quot;: list(cv.cv_results_['mean_test_score']),
             &quot;std_score&quot;: cv.cv_results_['std_test_score']})
results = results.dropna()
results
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">   max_depth  mean_score  std_score
0        2.0    0.780224   0.022550
1        4.0    0.812143   0.020297
2        8.0    0.817143   0.024915
3       16.0    0.810434   0.023838
4       32.0    0.810406   0.016414
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from plotnine import *

(
     ggplot(results) + geom_boxplot(aes(x='factor(max_depth)', y='mean_score')) +
    theme_classic() + xlab('Max tree depth') + ylab('Mean score')
)
</code></pre><figure><img src="./.ob-jupyter/bf0ab40c62a0aaac3a4e55726362a7189e06b6fb.png"/>
</figure>

<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&lt;ggplot: (326878576)&gt;
</code></pre></div><h3 id="maximum-number-of-leaf-nodes">Maximum number of leaf nodes</h3>
<p>This hyperparameter can be of importance to other topics, such as [Explainability]].</p>
<p>It is specified in <code>scikit-learn</code> using the <code>max_leaf_nodes</code> parameter. Let&rsquo;s try a few different values:</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">rfc = RandomForestClassifier(n_estimators=256,
                           criterion=&quot;entropy&quot;,
                            max_depth=8)

cv = GridSearchCV(rfc,{'max_leaf_nodes': [2**i for i in range(1, 8)]},cv=5)
cv.fit(X_train, y_train.values.ravel())
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">GridSearchCV(cv=5,
             estimator=RandomForestClassifier(criterion=&#39;entropy&#39;, max_depth=8,
                                              n_estimators=256),
             param_grid={&#39;max_leaf_nodes&#39;: [2, 4, 8, 16, 32, 64, 128]})
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">results = pd.DataFrame({&quot;max_leaf_nodes&quot;: [param[&quot;max_leaf_nodes&quot;] for param in cv.cv_results_['params']],
             &quot;mean_score&quot;: list(cv.cv_results_['mean_test_score']),
             &quot;std_score&quot;: cv.cv_results_['std_test_score']})
results = results.dropna()
results
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">   max_leaf_nodes  mean_score  std_score
0               2    0.755042   0.016142
1               4    0.778557   0.015940
2               8    0.810462   0.022316
3              16    0.813796   0.013894
4              32    0.810420   0.017087
5              64    0.817115   0.015382
6             128    0.813754   0.012408
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from plotnine import *

(
     ggplot(results) + geom_boxplot(aes(x='factor(max_leaf_nodes)', y='mean_score')) +
    geom_errorbar(aes(x='factor(max_leaf_nodes)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
    theme_classic() + xlab('Maximum leaf nodes') + ylab('Mean score')
)
</code></pre><figure><img src="./.ob-jupyter/404098ec72ed98cd5753efbb518f92039ef81842.png"/>
</figure>

<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&lt;ggplot: (326892069)&gt;
</code></pre></div><h3 id="random-features-per-split">Random features per split</h3>
<p>This is an important hyperparameter that will depend on how noisy the original data is.
Typically, if the data is not very noisy, the number of used random features can be kept low. Otherwise, it needs to be kept high.</p>
<p>An important consideration is also the following trade-off:</p>
<ul>
<li>A low number of random features decrease the forest&rsquo;s overall variance</li>
<li>A low number of random features increases the bias</li>
<li>A high number of random features increases computational time</li>
</ul>
<p>In `scikit-learn` this is specified with the `max_features` parameter. Assuming \(N_f\) is the total number of features,
some possible values for this parameter are:</p>
<ul>
<li>`sqrt`, this will take the `max_features` as the rounded \(\sqrt{N_f}\)</li>
<li>`log2`, as above, takes the \(\log_2(N_f)\)</li>
<li>The actual maximum number of features can be directly specified</li>
</ul>
<p>Let&rsquo;s try a simple benchmark, even though our data does not have many features to begin with:</p>
<p>```python</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">rfc = RandomForestClassifier(n_estimators=256,
                             criterion=&quot;entropy&quot;,
                             max_depth=8)

cv = GridSearchCV(rfc,{'max_features': [&quot;sqrt&quot;, &quot;log2&quot;, 1, 2, 3, 4, 5, 6]},cv=5)
cv.fit(X_train, y_train.values.ravel())
```
</code></pre><p>```python</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">results = pd.DataFrame({&quot;max_features&quot;: [param[&quot;max_features&quot;] for param in cv.cv_results_['params']],
             &quot;mean_score&quot;: list(cv.cv_results_['mean_test_score']),
             &quot;std_score&quot;: cv.cv_results_['std_test_score']})
results
```
</code></pre><p>```python</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from plotnine import *

(
     ggplot(results) + geom_boxplot(aes(x='factor(max_features)', y='mean_score')) +
    geom_errorbar(aes(x='factor(max_features)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
    theme_classic() + xlab('Maximum number of features') + ylab('Mean score')
)
```
</code></pre><p>### Bootstrap dataset size</p>
<p>This hyperparameter relates to the proportion of the training data to be used by decision trees.</p>
<p>It is specified in `scikit-learn` by `max_samples` and can take the value of either:</p>
<ul>
<li>`None`, take the entirety of the samples</li>
<li>An integer, representing the actual number of samples</li>
<li>A float, representing a proportion between `0` and `1` or the samples to take.</li>
</ul>
<p>Let&rsquo;s try a hyperparameter search with some values:</p>
<pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">rfc = RandomForestClassifier(n_estimators=256,
                             criterion=&quot;entropy&quot;,
                             max_depth=8,
                             max_features=6)

cv = GridSearchCV(rfc,{'max_samples': [i/10.0 for i in range(1, 10)]},cv=5)
cv.fit(X_train, y_train.values.ravel())
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">GridSearchCV(cv=5,
             estimator=RandomForestClassifier(criterion=&#39;entropy&#39;, max_depth=8,
                                              max_features=6,
                                              n_estimators=256),
             param_grid={&#39;max_samples&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,
                                         0.9]})
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">results = pd.DataFrame({&quot;max_samples&quot;: [param[&quot;max_samples&quot;] for param in cv.cv_results_['params']],
             &quot;mean_score&quot;: list(cv.cv_results_['mean_test_score']),
             &quot;std_score&quot;: cv.cv_results_['std_test_score']})
results
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">   max_samples  mean_score  std_score
0          0.1    0.803754   0.026864
1          0.2    0.818824   0.013165
2          0.3    0.817143   0.015923
3          0.4    0.822157   0.017840
4          0.5    0.815434   0.013033
5          0.6    0.827185   0.019558
6          0.7    0.825518   0.016969
7          0.8    0.827199   0.015364
8          0.9    0.830546   0.021439
</code></pre></div><pre tabindex="0"><code class="language-jupyter-python" data-lang="jupyter-python">from plotnine import *

(
     ggplot(results) + geom_boxplot(aes(x='factor(max_samples)', y='mean_score')) +
    geom_errorbar(aes(x='factor(max_samples)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
    theme_classic() + xlab('Proportion bootstrap samples') + ylab('Mean score')
)
</code></pre><figure><img src="./.ob-jupyter/3b70d501d65fb06e49c88da199718b89a530c6d0.png"/>
</figure>

<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&lt;ggplot: (327014903)&gt;
</code></pre></div><section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>: Titanic Dataset - <a href="https://www.kaggle.com/c/titanic-dataset/data">https://www.kaggle.com/c/titanic-dataset/data</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit">https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>
  </article>



  
  






  <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/map/">All pages</a></li>
         
          <li><a href="/search.html">Search</a></li>
        
      </ul>
    </div>

    
    <div id="toc-footer" style="display: none">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#data">Data</a></li>
    <li><a href="#naive-model">Naive model</a></li>
    <li><a href="#hyperparameter-search">Hyperparameter search</a></li>
    <li><a href="#parameters">Parameters</a>
      <ul>
        <li><a href="#number-of-decision-trees">Number of decision trees</a></li>
        <li><a href="#the-split-criteria">The split criteria</a></li>
        <li><a href="#maximum-depth-of-individual-trees">Maximum depth of individual trees</a></li>
        <li><a href="#maximum-number-of-leaf-nodes">Maximum number of leaf nodes</a></li>
        <li><a href="#random-features-per-split">Random features per split</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    

    <div id="share-footer" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html" aria-label="Facebook">
      <i class="fab fa-facebook fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&text=Optimising%20random%20forest%20hyperparameters" aria-label="Twitter">
      <i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label="Linkedin">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&is_video=false&description=Optimising%20random%20forest%20hyperparameters" aria-label="Pinterest">
      <i class="fab fa-pinterest fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Optimising%20random%20forest%20hyperparameters&body=Check out this article: https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html" aria-label="Email">
      <i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label="Pocket">
      <i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label="reddit">
      <i class="fab fa-reddit fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&name=Optimising%20random%20forest%20hyperparameters&description=Typically%20the%20hyper-parameters%20which%20will%20have%20the%20most%20significant%20impact%20on%20the%20behaviour%20of%20a%20random%20forest%20are%20the%20following%3a%0a%20The%20number%20of%20decision%20trees%20in%20a%20random%20forest%20The%20split%20criteria%20Maximum%20depth%20of%20individual%20trees%20Minimum%20samples%20per%20internal%20node%20Maximum%20number%20of%20leaf%20nodes%20Random%20features%20per%20split%20Number%20of%20samples%20in%20bootstrap%20dataset%20%20We%20will%20look%20at%20each%20of%20these%20hyper-parameters%20individually%20with%20examples%20of%20how%20to%20select%20them." aria-label="Tumblr">
      <i class="fab fa-tumblr fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&t=Optimising%20random%20forest%20hyperparameters" aria-label="Hacker News">
      <i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>

    <div id="actions-footer">
      
        <a id="menu-toggle" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;" aria-label="Menu">
          <i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
        <a id="toc-toggle" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;" aria-label="TOC">
          <i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share-toggle" class="icon" href="#" onclick="$('#share-footer').toggle();return false;" aria-label="Share">
          <i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
          <i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>


  <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2021  Rui Vieira 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/map/">All pages</a></li>
         
        <li><a href="/search.html">Search</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/css/fa.min.css>
<script src=/js/jquery-3.6.0.min.js></script>
<script src=/js/main.js></script>



  


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</html>
