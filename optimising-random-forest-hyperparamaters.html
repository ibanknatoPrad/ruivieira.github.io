<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/favicons/favicon.ico">

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Notes on Optimising random forest hyperparamaters">
    <meta name="robots" content="index">
    <link rel="canonical" href="https://ruivieira.dev/optimising-random-forest-hyperparamaters.html">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
          integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
            integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
            crossorigin="anonymous"></script>

        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
            integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>

    <script src="/assets/mark.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Nunito:400,300i,800&display=swap" rel="stylesheet"/>
    <link href="/assets/style.css" rel="stylesheet">
    <style>
        /*Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>*/

        .hljs {
            display: block;
            overflow-x: auto;
            padding: 0.5em;
            background: white;
            color: black;
            -webkit-text-size-adjust: none;
        }

        .hljs-string,
        .hljs-tag .hljs-value,
        .hljs-filter .hljs-argument,
        .hljs-addition,
        .hljs-change,
        .hljs-name,
        .apache .hljs-tag,
        .apache .hljs-cbracket,
        .nginx .hljs-built_in,
        .tex .hljs-formula {
            color: #888;
        }

        .hljs-comment,
        .hljs-shebang,
        .hljs-doctype,
        .hljs-pi,
        .hljs-javadoc,
        .hljs-deletion,
        .apache .hljs-sqbracket {
            color: #ccc;
        }

        .hljs-keyword,
        .hljs-tag .hljs-title,
        .ini .hljs-title,
        .lisp .hljs-title,
        .http .hljs-title,
        .nginx .hljs-title,
        .css .hljs-tag,
        .hljs-winutils,
        .hljs-flow,
        .apache .hljs-tag,
        .tex .hljs-command,
        .hljs-request,
        .hljs-status {
            font-weight: bold;
        }
    </style>
    <title>ruivieira.dev - Optimising random forest hyperparamaters</title>
    <script data-goatcounter="https://ruivieira-dev.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
    <style>
        #search_terms {
            font-size: 1rem;
            font-family: Nunito;
            width: 40%;
        }
        #search_terms::placeholder {
            color: #bbb;
        }
        #search_button {
            background-color: #eee;
            border: none;
            color: black;
            padding: 0.25rem 0.25rem;
            font-size: 1rem;
            font-family: Nunito;
            text-decoration: none;
            cursor: pointer;
            border-radius: 5px;
            width: 3rem;
        }
    </style>
</head>
<body>
<div id="grid">

    <div id="content">
        <h1 id="optimising-random-forest-hyperparameters">Optimising random forest hyperparameters</h1>
<p>Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:</p>
<ul>
<li>[The number of decision trees](optimising-random-forest-hyperparamaters.html#Number of decision trees) in a random forest</li>
<li>The split criteria</li>
<li>Maximum depth of individual trees</li>
<li>Minimum samples per internal node</li>
<li>Maximum number of leaf nodes</li>
<li>Random features per split</li>
<li>Number of samples in bootstrap dataset</li>
</ul>
<p>We will look at each of these hyper-parameters individually with examples of how to select them.</p>
<h2 id="data">Data</h2>
<p>To understand how we can optimise the hyperparameters in a random forest model, we will use <a href="scikit-learn.html">scikit-learn's</a> <code>RandomForestClassifier</code> and a subset of <em>Titanic</em><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> dataset.</p>
<p>First, we will import the features and labels using <a href="pandas.html">Pandas</a>.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

train_features = pd.read_csv(<span class="hljs-string">&quot;data/svm-hyperparameters-train-features.csv&quot;</span>)
train_label = pd.read_csv(<span class="hljs-string">&quot;data/svm-hyperparameters-train-label.csv&quot;</span>)
</code></pre>
<p>Let's look at a random sample of entries from this dataset, both for features and labels.</p>
<pre><code class="language-python">train_features.sample(<span class="hljs-number">10</span>)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>517</th>
      <td>3</td>
      <td>1</td>
      <td>30.0</td>
      <td>0</td>
      <td>0</td>
      <td>24.1500</td>
    </tr>
    <tr>
      <th>538</th>
      <td>3</td>
      <td>1</td>
      <td>30.0</td>
      <td>0</td>
      <td>0</td>
      <td>14.5000</td>
    </tr>
    <tr>
      <th>735</th>
      <td>3</td>
      <td>1</td>
      <td>28.5</td>
      <td>0</td>
      <td>0</td>
      <td>16.1000</td>
    </tr>
    <tr>
      <th>75</th>
      <td>3</td>
      <td>1</td>
      <td>25.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.6500</td>
    </tr>
    <tr>
      <th>395</th>
      <td>3</td>
      <td>1</td>
      <td>22.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.7958</td>
    </tr>
    <tr>
      <th>560</th>
      <td>3</td>
      <td>1</td>
      <td>30.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.7500</td>
    </tr>
    <tr>
      <th>875</th>
      <td>3</td>
      <td>0</td>
      <td>15.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.2250</td>
    </tr>
    <tr>
      <th>579</th>
      <td>3</td>
      <td>1</td>
      <td>32.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
    </tr>
    <tr>
      <th>701</th>
      <td>1</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>26.2875</td>
    </tr>
    <tr>
      <th>707</th>
      <td>1</td>
      <td>1</td>
      <td>42.0</td>
      <td>0</td>
      <td>0</td>
      <td>26.2875</td>
    </tr>
  </tbody>
</table>
</div>
Some of the available features are:
<ul>
<li><code>Pclass</code>, ticket class</li>
<li><code>Sex</code></li>
<li><code>Age</code>, age in years</li>
<li><code>Sibsp</code>, number of siblings/spouses aboard</li>
<li><code>Parch</code>, number of parents/children aboard</li>
<li><code>Fare</code>, passenger fare</li>
</ul>
<pre><code class="language-python">train_label.sample(<span class="hljs-number">10</span>)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>342</th>
      <td>0</td>
    </tr>
    <tr>
      <th>436</th>
      <td>0</td>
    </tr>
    <tr>
      <th>414</th>
      <td>1</td>
    </tr>
    <tr>
      <th>379</th>
      <td>0</td>
    </tr>
    <tr>
      <th>847</th>
      <td>0</td>
    </tr>
    <tr>
      <th>397</th>
      <td>0</td>
    </tr>
    <tr>
      <th>82</th>
      <td>1</td>
    </tr>
    <tr>
      <th>671</th>
      <td>0</td>
    </tr>
    <tr>
      <th>317</th>
      <td>0</td>
    </tr>
    <tr>
      <th>681</th>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
The outcome label indicates whether a passenger survived the disaster.
<p>As part of the typical initial steps for model training, we will prepare the data by splitting it into a training and testing subset.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split

X_train, X_test, y_train, y_test = train_test_split(train_features, train_label, test_size=<span class="hljs-number">0.33</span>, random_state=<span class="hljs-number">23</span>)
</code></pre>
<h2 id="naive-model">Naive model</h2>
<p>First we will train a &quot;naive&quot; model, that is a model using the defaults provided by <code>RandomForestClassifier</code><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>. These defaults are:</p>
<ul>
<li><code>n_estimators = 10</code></li>
<li><code>criterion=’gini’</code></li>
<li><code>max_depth=None</code></li>
<li><code>min_samples_split=2</code></li>
<li><code>min_samples_leaf=1</code></li>
<li><code>min_weight_fraction_leaf=0.0</code></li>
<li><code>max_features=’auto’</code></li>
<li><code>max_leaf_nodes=None</code></li>
<li><code>min_impurity_decrease=0.0</code></li>
<li><code>min_impurity_split=None</code></li>
<li><code>bootstrap=True</code></li>
<li><code>oob_score=False</code></li>
<li><code>n_jobs=1</code></li>
<li><code>random_state=None</code></li>
<li><code>verbose=0</code></li>
<li><code>warm_start=False</code></li>
<li><code>class_weight=None</code></li>
</ul>
<p>We will instantiate a random forest classifier:</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier

rf = RandomForestClassifier()
</code></pre>
<p>And training it using the <code>X_train</code> and <code>y_train</code> subsets using the appropriate <code>fit</code> method<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.</p>
<pre><code class="language-python">true_labels = train_label.values.ravel()

rf.fit(X_train, y_train.values.ravel())
</code></pre>
<p>We can now evaluate trained naive model's score.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> precision_score
</code></pre>
<pre><code class="language-python">predicted_labels = rf.predict(X_test)

precision_score(y_test, predicted_labels)
</code></pre>
<h2 id="hyperparameter-search">Hyperparameter search</h2>
<p>A simple example of a generic hyperparameter search using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"><code>GridSearchCV</code></a> method in <code>scikit-learn</code>. The score used to measure the &quot;best&quot; model is the <code>mean_test_score</code>, but other metrics could be used, such as the <a href="oob-score-in-random-forests.html">Out-of-bag (OOB)</a> error.</p>
<pre><code class="language-python">parameters = {
    <span class="hljs-string">&quot;n_estimators&quot;</span>:[<span class="hljs-number">5</span>,<span class="hljs-number">10</span>,<span class="hljs-number">50</span>,<span class="hljs-number">100</span>,<span class="hljs-number">250</span>],
    <span class="hljs-string">&quot;max_depth&quot;</span>:[<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">8</span>,<span class="hljs-number">16</span>,<span class="hljs-number">32</span>,<span class="hljs-literal">None</span>]
    
}
</code></pre>
<pre><code class="language-python">rfc = RandomForestClassifier()
</code></pre>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV
</code></pre>
<pre><code class="language-python">cv = GridSearchCV(rfc,parameters,cv=<span class="hljs-number">5</span>)
cv.fit(X_train, y_train.values.ravel())
</code></pre>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">display</span>(<span class="hljs-params">results</span>):</span>
    print(<span class="hljs-string">f&#x27;Best parameters are: <span class="hljs-subst">{results.best_params_}</span>&#x27;</span>)
    print(<span class="hljs-string">&quot;\n&quot;</span>)
    mean_score = results.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]
    std_score = results.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]
    params = results.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]
    <span class="hljs-keyword">for</span> mean,std,params <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(mean_score,std_score,params):
        print(<span class="hljs-string">f&#x27;<span class="hljs-subst">{<span class="hljs-built_in">round</span>(mean,<span class="hljs-number">3</span>)}</span> + or -<span class="hljs-subst">{<span class="hljs-built_in">round</span>(std,<span class="hljs-number">3</span>)}</span> for the <span class="hljs-subst">{params}</span>&#x27;</span>)
</code></pre>
<pre><code class="language-python">display(cv)
</code></pre>
<pre><code><span class="hljs-attribute">Best</span> parameters are: {&#x27;max_depth&#x27;: <span class="hljs-number">8</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">50</span>}


<span class="hljs-attribute">0</span>.<span class="hljs-number">795</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">02</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">2</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">5</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">775</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">024</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">2</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">10</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">774</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">025</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">2</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">50</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">779</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">02</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">2</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">100</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">78</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">018</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">2</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">250</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">814</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">024</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">4</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">5</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">81</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">028</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">4</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">10</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">807</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">022</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">4</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">50</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">804</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">018</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">4</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">100</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">817</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">025</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">4</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">250</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">815</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">017</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">8</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">5</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">815</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">017</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">8</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">10</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">822</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">019</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">8</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">50</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">82</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">014</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">8</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">100</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">82</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">016</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">8</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">250</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">807</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">028</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">16</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">5</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">812</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">028</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">16</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">10</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">814</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">028</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">16</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">50</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">807</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">028</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">16</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">100</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">812</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">026</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">16</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">250</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">779</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">036</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">32</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">5</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">807</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">027</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">32</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">10</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">814</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">021</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">32</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">50</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">802</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">023</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">32</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">100</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">797</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">021</span> for the {&#x27;max_depth&#x27;: <span class="hljs-number">32</span>, &#x27;n_estimators&#x27;: <span class="hljs-number">250</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">789</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">027</span> for the {&#x27;max_depth&#x27;: None, &#x27;n_estimators&#x27;: <span class="hljs-number">5</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">795</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">033</span> for the {&#x27;max_depth&#x27;: None, &#x27;n_estimators&#x27;: <span class="hljs-number">10</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">804</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">023</span> for the {&#x27;max_depth&#x27;: None, &#x27;n_estimators&#x27;: <span class="hljs-number">50</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">802</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">014</span> for the {&#x27;max_depth&#x27;: None, &#x27;n_estimators&#x27;: <span class="hljs-number">100</span>}
<span class="hljs-attribute">0</span>.<span class="hljs-number">815</span> + or -<span class="hljs-number">0</span>.<span class="hljs-number">021</span> for the {&#x27;max_depth&#x27;: None, &#x27;n_estimators&#x27;: <span class="hljs-number">250</span>}

</code></pre>
<h2 id="parameters">Parameters</h2>
<h3 id="number-of-decision-trees">Number of decision trees</h3>
<p>This is specified using the <code>n_estimators</code> hyper-parameter on the random forest initialisation.</p>
<p>Typically, a higher number of trees will lead to greater accuracy at the expense of model size and training time.</p>
<pre><code class="language-python">cv = GridSearchCV(rfc,{<span class="hljs-string">&quot;n_estimators&quot;</span>:[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>]},cv=<span class="hljs-number">5</span>)
cv.fit(X_train, y_train.values.ravel())
</code></pre>
<pre><code class="language-python">results = pd.DataFrame({<span class="hljs-string">&quot;n_estimators&quot;</span>: [param[<span class="hljs-string">&quot;n_estimators&quot;</span>] <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> cv.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]],
             <span class="hljs-string">&quot;mean_score&quot;</span>: <span class="hljs-built_in">list</span>(cv.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]),
             <span class="hljs-string">&quot;std_score&quot;</span>: cv.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]})
results
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>n_estimators</th>
      <th>mean_score</th>
      <th>std_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>0.766793</td>
      <td>0.016062</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>0.773515</td>
      <td>0.029848</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>0.796989</td>
      <td>0.034546</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16</td>
      <td>0.803669</td>
      <td>0.011729</td>
    </tr>
    <tr>
      <th>4</th>
      <td>32</td>
      <td>0.800308</td>
      <td>0.030873</td>
    </tr>
    <tr>
      <th>5</th>
      <td>64</td>
      <td>0.807073</td>
      <td>0.021654</td>
    </tr>
    <tr>
      <th>6</th>
      <td>128</td>
      <td>0.805378</td>
      <td>0.018606</td>
    </tr>
    <tr>
      <th>7</th>
      <td>256</td>
      <td>0.812101</td>
      <td>0.022103</td>
    </tr>
    <tr>
      <th>8</th>
      <td>512</td>
      <td>0.807059</td>
      <td>0.020457</td>
    </tr>
  </tbody>
</table>
</div>
```python
from plotnine import *
<p>(
ggplot(results) + geom_boxplot(aes(x='factor(n_estimators)', y='mean_score')) +
geom_errorbar(aes(x='factor(n_estimators)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
theme_classic() + xlab('Number of trees') + ylab('Mean score')
)</p>
<pre><code>
![optimising-<span class="hljs-built_in">random</span>-forest-hyperparamaters_1](./images/optimising-<span class="hljs-built_in">random</span>-forest-hyperparamaters_1.png)
### The <span class="hljs-built_in">split</span> criteria
At each node, a <span class="hljs-built_in">random</span> forest decides, according to a specific algorithm, which <span class="hljs-built_in">feature</span> <span class="hljs-keyword">and</span> value <span class="hljs-built_in">split</span> the tree.
Therefore, the choice of splitting algorithm <span class="hljs-built_in">is</span> crucial <span class="hljs-keyword">for</span> the <span class="hljs-built_in">random</span> forest&#x27;s performance.

Since, <span class="hljs-keyword">in</span> this <span class="hljs-built_in">example</span>, we are dealing with a classification problem, the choices of <span class="hljs-built_in">split</span> algorithm are, <span class="hljs-keyword">for</span> instance:

- Gini
- Entropy

If we were dealing with a <span class="hljs-built_in">random</span> forest <span class="hljs-keyword">for</span> regression, other methods (such as [MSE](<span class="hljs-built_in">error</span>-metrics.html)) would be a possible choice.
We will now <span class="hljs-built_in">compare</span> both <span class="hljs-built_in">split</span> algorithms as specified above, <span class="hljs-keyword">in</span> training a <span class="hljs-built_in">random</span> forest with our data:
```python
rfc = RandomForestClassifier(n_estimators=<span class="hljs-number">256</span>)

<span class="hljs-built_in">cv</span> = GridSearchCV(rfc,{<span class="hljs-string">&quot;criterion&quot;</span>: [<span class="hljs-string">&quot;gini&quot;</span>, <span class="hljs-string">&quot;entropy&quot;</span>]},<span class="hljs-built_in">cv</span>=<span class="hljs-number">5</span>)
<span class="hljs-built_in">cv</span>.fit(X_train, y_train.<span class="hljs-built_in">values</span>.ravel())
</code></pre>
<pre><code class="language-python">results = pd.DataFrame({<span class="hljs-string">&quot;criterion&quot;</span>: [param[<span class="hljs-string">&quot;criterion&quot;</span>] <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> cv.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]],
             <span class="hljs-string">&quot;mean_score&quot;</span>: <span class="hljs-built_in">list</span>(cv.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]),
             <span class="hljs-string">&quot;std_score&quot;</span>: cv.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]})
results
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>criterion</th>
      <th>mean_score</th>
      <th>std_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>gini</td>
      <td>0.802003</td>
      <td>0.023607</td>
    </tr>
    <tr>
      <th>1</th>
      <td>entropy</td>
      <td>0.812087</td>
      <td>0.013550</td>
    </tr>
  </tbody>
</table>
</div>
### Maximum depth of individual trees
In theory, the "longer" the tree, the more splits it can have and better accommodate the data. However, at the tree level can this can lead to overfitting.
Although this is a problem for decision trees, it is not necessarily a problem for the ensemble, the random forest.
Although the key is to strike a balance between trees that aren't too large or too short, there's no universal heuristic to determine the size.
Let's try a few option for maximum depth:
```python
rfc = RandomForestClassifier(n_estimators=256,
                           criterion="entropy")
<p>cv = GridSearchCV(rfc,{'max_depth': [2, 4, 8, 16, 32, None]},cv=5)
cv.fit(X_train, y_train.values.ravel())</p>
<pre><code>
```python
results = pd<span class="hljs-selector-class">.DataFrame</span>({<span class="hljs-string">&quot;max_depth&quot;</span>: <span class="hljs-selector-attr">[param[<span class="hljs-string">&quot;max_depth&quot;</span>]</span> <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> cv<span class="hljs-selector-class">.cv_results_</span><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;params&#x27;</span>]</span>],
             <span class="hljs-string">&quot;mean_score&quot;</span>: list(cv<span class="hljs-selector-class">.cv_results_</span><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]</span>),
             <span class="hljs-string">&quot;std_score&quot;</span>: cv<span class="hljs-selector-class">.cv_results_</span><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]</span>})
results = results<span class="hljs-selector-class">.dropna</span>()
results
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>max_depth</th>
      <th>mean_score</th>
      <th>std_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.0</td>
      <td>0.770154</td>
      <td>0.018592</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.0</td>
      <td>0.817185</td>
      <td>0.022074</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.0</td>
      <td>0.812087</td>
      <td>0.008403</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.0</td>
      <td>0.805378</td>
      <td>0.019350</td>
    </tr>
    <tr>
      <th>4</th>
      <td>32.0</td>
      <td>0.813768</td>
      <td>0.018610</td>
    </tr>
  </tbody>
</table>
</div>
```python
from plotnine import *
<p>(
ggplot(results) + geom_boxplot(aes(x='factor(max_depth)', y='mean_score')) +
theme_classic() + xlab('Max tree depth') + ylab('Mean score')
)</p>
<pre><code>
![optimising-<span class="hljs-built_in">random</span>-forest-hyperparamaters_2](./images/optimising-<span class="hljs-built_in">random</span>-forest-hyperparamaters_2.png)
### Maximum number of leaf nodes
This hyperparameter can be of importance to other topics, such as [Explainability](explainability.html).

It <span class="hljs-built_in">is</span> specified <span class="hljs-keyword">in</span> `scikit-learn` using the `max_leaf_nodes` parameter. Let&#x27;s try a few different <span class="hljs-built_in">values</span>:
```python
rfc = RandomForestClassifier(n_estimators=<span class="hljs-number">256</span>,
                           criterion=<span class="hljs-string">&quot;entropy&quot;</span>,
                            max_depth=<span class="hljs-number">8</span>)

<span class="hljs-built_in">cv</span> = GridSearchCV(rfc,{&#x27;max_leaf_nodes&#x27;: [<span class="hljs-number">2</span>**i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">8</span>)]},<span class="hljs-built_in">cv</span>=<span class="hljs-number">5</span>)
<span class="hljs-built_in">cv</span>.fit(X_train, y_train.<span class="hljs-built_in">values</span>.ravel())
</code></pre>
<pre><code class="language-python">results = pd.DataFrame({<span class="hljs-string">&quot;max_leaf_nodes&quot;</span>: [param[<span class="hljs-string">&quot;max_leaf_nodes&quot;</span>] <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> cv.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]],
             <span class="hljs-string">&quot;mean_score&quot;</span>: <span class="hljs-built_in">list</span>(cv.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]),
             <span class="hljs-string">&quot;std_score&quot;</span>: cv.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]})
results = results.dropna()
results
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>max_leaf_nodes</th>
      <th>mean_score</th>
      <th>std_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>0.755014</td>
      <td>0.024759</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>0.773487</td>
      <td>0.024367</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>0.817185</td>
      <td>0.026706</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16</td>
      <td>0.812115</td>
      <td>0.014137</td>
    </tr>
    <tr>
      <th>4</th>
      <td>32</td>
      <td>0.813782</td>
      <td>0.019204</td>
    </tr>
    <tr>
      <th>5</th>
      <td>64</td>
      <td>0.812073</td>
      <td>0.008712</td>
    </tr>
    <tr>
      <th>6</th>
      <td>128</td>
      <td>0.813754</td>
      <td>0.013498</td>
    </tr>
  </tbody>
</table>
</div>
```python
from plotnine import *
<p>(
ggplot(results) + geom_boxplot(aes(x='factor(max_leaf_nodes)', y='mean_score')) +
geom_errorbar(aes(x='factor(max_leaf_nodes)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
theme_classic() + xlab('Maximum leaf nodes') + ylab('Mean score')
)</p>
<pre><code>
![optimising-<span class="hljs-built_in">random</span>-forest-hyperparamaters_3](./images/optimising-<span class="hljs-built_in">random</span>-forest-hyperparamaters_3.png)
### Random <span class="hljs-built_in">features</span> per <span class="hljs-built_in">split</span>
This <span class="hljs-built_in">is</span> an important hyperparameter that will depend on how noisy the original data <span class="hljs-built_in">is</span>.
Typically, <span class="hljs-keyword">if</span> the data <span class="hljs-built_in">is</span> <span class="hljs-keyword">not</span> very noisy, the number of used <span class="hljs-built_in">random</span> <span class="hljs-built_in">features</span> can be kept low. Otherwise, it needs to be kept high.

An important consideration <span class="hljs-built_in">is</span> also the following trade-off:

- A low number of <span class="hljs-built_in">random</span> <span class="hljs-built_in">features</span> decrease the forest&#x27;s overall variance
- A low number of <span class="hljs-built_in">random</span> <span class="hljs-built_in">features</span> increases the bias
- A high number of <span class="hljs-built_in">random</span> <span class="hljs-built_in">features</span> increases computational <span class="hljs-built_in">time</span>

In `scikit-learn` this <span class="hljs-built_in">is</span> specified with the `max_features` parameter. Assuming $N_f$ <span class="hljs-built_in">is</span> the total number of <span class="hljs-built_in">features</span>,
<span class="hljs-built_in">some</span> possible <span class="hljs-built_in">values</span> <span class="hljs-keyword">for</span> this parameter are:

- `<span class="hljs-built_in">sqrt</span>`, this will take the `max_features` as the rounded $\<span class="hljs-built_in">sqrt</span>{N_f}$
- `log2`, as above, takes the $\log_2(N_f)$
- The actual maximum number of <span class="hljs-built_in">features</span> can be directly specified

Let&#x27;s try a simple benchmark, <span class="hljs-built_in">even</span> though our data does <span class="hljs-keyword">not</span> have many <span class="hljs-built_in">features</span> to begin with:
```python
rfc = RandomForestClassifier(n_estimators=<span class="hljs-number">256</span>,
                             criterion=<span class="hljs-string">&quot;entropy&quot;</span>,
                             max_depth=<span class="hljs-number">8</span>)

<span class="hljs-built_in">cv</span> = GridSearchCV(rfc,{&#x27;max_features&#x27;: [<span class="hljs-string">&quot;sqrt&quot;</span>, <span class="hljs-string">&quot;log2&quot;</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]},<span class="hljs-built_in">cv</span>=<span class="hljs-number">5</span>)
<span class="hljs-built_in">cv</span>.fit(X_train, y_train.<span class="hljs-built_in">values</span>.ravel())
</code></pre>
<pre><code class="language-python">results = pd.DataFrame({<span class="hljs-string">&quot;max_features&quot;</span>: [param[<span class="hljs-string">&quot;max_features&quot;</span>] <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> cv.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]],
             <span class="hljs-string">&quot;mean_score&quot;</span>: <span class="hljs-built_in">list</span>(cv.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]),
             <span class="hljs-string">&quot;std_score&quot;</span>: cv.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]})
results
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>max_features</th>
      <th>mean_score</th>
      <th>std_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>sqrt</td>
      <td>0.820476</td>
      <td>0.018793</td>
    </tr>
    <tr>
      <th>1</th>
      <td>log2</td>
      <td>0.810420</td>
      <td>0.015345</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0.820504</td>
      <td>0.017704</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>0.820490</td>
      <td>0.017064</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>0.815420</td>
      <td>0.009488</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4</td>
      <td>0.823838</td>
      <td>0.011681</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5</td>
      <td>0.828880</td>
      <td>0.016225</td>
    </tr>
    <tr>
      <th>7</th>
      <td>6</td>
      <td>0.828880</td>
      <td>0.013361</td>
    </tr>
  </tbody>
</table>
</div>
```python
from plotnine import *
<p>(
ggplot(results) + geom_boxplot(aes(x='factor(max_features)', y='mean_score')) +
geom_errorbar(aes(x='factor(max_features)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
theme_classic() + xlab('Maximum number of features') + ylab('Mean score')
)</p>
<pre><code>
![optimising-random-forest-hyperparamaters_4](./images/optimising-random-forest-hyperparamaters_4.png)
<span class="hljs-comment">### Bootstrap dataset size</span>
This hyperparameter relates <span class="hljs-keyword">to</span> <span class="hljs-keyword">the</span> proportion <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> training data <span class="hljs-keyword">to</span> be used <span class="hljs-keyword">by</span> decision trees.

It <span class="hljs-keyword">is</span> specified <span class="hljs-keyword">in</span> `scikit-learn` <span class="hljs-keyword">by</span> `max_samples` <span class="hljs-keyword">and</span> can take <span class="hljs-keyword">the</span> value <span class="hljs-keyword">of</span> either:

- `None`, take <span class="hljs-keyword">the</span> entirety <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> samples
- An <span class="hljs-built_in">integer</span>, representing <span class="hljs-keyword">the</span> actual <span class="hljs-built_in">number</span> <span class="hljs-keyword">of</span> samples
- A float, representing a proportion <span class="hljs-keyword">between</span> `<span class="hljs-number">0</span>` <span class="hljs-keyword">and</span> `<span class="hljs-number">1</span>` <span class="hljs-keyword">or</span> <span class="hljs-keyword">the</span> samples <span class="hljs-keyword">to</span> take.

Let&#x27;s <span class="hljs-keyword">try</span> a hyperparameter search <span class="hljs-keyword">with</span> <span class="hljs-keyword">some</span> values:
```python
rfc = RandomForestClassifier(n_estimators=<span class="hljs-number">256</span>,
                             criterion=<span class="hljs-string">&quot;entropy&quot;</span>,
                             max_depth=<span class="hljs-number">8</span>,
                             max_features=<span class="hljs-number">6</span>)

cv = GridSearchCV(rfc,{&#x27;max_samples&#x27;: [i/<span class="hljs-number">10.0</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>)]},cv=<span class="hljs-number">5</span>)
cv.fit(X_train, y_train.values.ravel())
</code></pre>
<pre><code class="language-python">results = pd.DataFrame({<span class="hljs-string">&quot;max_samples&quot;</span>: [param[<span class="hljs-string">&quot;max_samples&quot;</span>] <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> cv.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]],
             <span class="hljs-string">&quot;mean_score&quot;</span>: <span class="hljs-built_in">list</span>(cv.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]),
             <span class="hljs-string">&quot;std_score&quot;</span>: cv.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]})
results
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>max_samples</th>
      <th>mean_score</th>
      <th>std_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.1</td>
      <td>0.805434</td>
      <td>0.021459</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2</td>
      <td>0.817171</td>
      <td>0.019534</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.3</td>
      <td>0.815448</td>
      <td>0.010392</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.4</td>
      <td>0.818796</td>
      <td>0.016416</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.5</td>
      <td>0.832227</td>
      <td>0.019042</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.6</td>
      <td>0.827199</td>
      <td>0.015364</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.7</td>
      <td>0.825518</td>
      <td>0.016115</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.8</td>
      <td>0.820490</td>
      <td>0.011031</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.9</td>
      <td>0.827199</td>
      <td>0.014415</td>
    </tr>
  </tbody>
</table>
</div>
```python
from plotnine import *
<p>(
ggplot(results) + geom_boxplot(aes(x='factor(max_samples)', y='mean_score')) +
geom_errorbar(aes(x='factor(max_samples)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
theme_classic() + xlab('Proportion bootstrap samples') + ylab('Mean score')
)</p>
<pre><code>
![optimising-<span class="hljs-built_in">random</span>-forest-hyperparamaters_5](./images/optimising-<span class="hljs-built_in">random</span>-forest-hyperparamaters_5.png)
```python

</code></pre>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1"  class="footnote-item"><p>Titanic Dataset - <a href="https://www.kaggle.com/c/titanic-dataset/data">https://www.kaggle.com/c/titanic-dataset/data</a> <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
<li id="fn2"  class="footnote-item"><p><a href="https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a> <a href="#fnref2" class="footnote-backref">↩</a></p>
</li>
<li id="fn3"  class="footnote-item"><p><a href="https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit">https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit</a> <a href="#fnref3" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
        <div class="footer">
            <span class="cc-symbol">&#127341;</span> 2020 CC BY Rui Vieira
        </div>
    </div>

    <div id="sidebar">
        <div id="sidebar-search">
            <input id="search_terms" type="search" placeholder="Search terms" />
            <button id="search_button" onclick="search()">?</button>
        </div>
        <div id="sidebar-home"><a href="/">Home</a></div>
        <div id="sidebar-all-pages"><a href="/content.html">All pages</a></div>
        <div id="sidebar-graph"><a href="/graph.html">Link network</a></div>

        <div id="sidebar-contents">
            <h3>Contents</h3>
            <ul>
<li><a href="#data">Data</a></li>
<li><a href="#naive-model">Naive model</a></li>
<li><a href="#hyperparameter-search">Hyperparameter search</a></li>
<li><a href="#parameters">Parameters</a>
<ul>
<li><a href="#number-of-decision-trees">Number of decision trees</a></li>
<li><a href="#the-split-criteria">The split criteria</a></li>
<li><a href="#maximum-depth-of-individual-trees">Maximum depth of individual trees</a></li>
<li><a href="#maximum-number-of-leaf-nodes">Maximum number of leaf nodes</a></li>
<li><a href="#random-features-per-split">Random features per split</a></li>
<li><a href="#bootstrap-dataset-size">Bootstrap dataset size</a></li>
</ul></li>
</ul>

                        <h3>Backlinks</h3>
            <ul>
                                <li><a href="/optimising-random-forest-hyperparamaters.html">Optimising random forest hyperparamaters</a><sup>&#5833</sup></li>
                                <li><a href="/scikit-learn.html">Scikit-learn</a><sup>&#5833</sup></li>
                                <li><a href="/index.html">index</a><sup>&#5833</sup></li>
                            </ul>
                    </div>



        <div class="footer">
            modified 1 week ago        </div>

    </div>
    <div>

        <script>
            const input = document.getElementById('search_terms');
            const highlight = new URLSearchParams(document.location.search).get("h");

            if (highlight!=null) {
                const markInstance = new Mark("#content");
                markInstance.mark(highlight);
            }

            input.addEventListener("keyup", function(event) {
                // Number 13 is the "Enter" key on the keyboard
                if (event.keyCode === 13) {
                    // Cancel the default action, if needed
                    event.preventDefault();
                    // Trigger the button element with a click
                    search_button.click();
                }
            });

            let search = function() {
                const query = new URLSearchParams({"q": input.value});
                console.log(query.toString());
                window.location.href = "/search.html?" + query.toString();
            }
        </script>
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                renderMathInElement(
                    document.body,
                    {
                        delimiters: [
                            {left: "$$", right: "$$", display: true},
                            {left: "\\[", right: "\\]", display: true},
                            {left: "$", right: "$", display: false},
                            {left: "\\(", right: "\\)", display: false}
                        ]
                    }
                );
            });
        </script>

</body>
</html>