<!doctype html><html lang=en-uk>
<head>
<script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script type=module src=/js/deeplinks/deeplinks.js></script>
<link rel=preload href=/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous>
<link rel=stylesheet href=/css/kbd.css type=text/css>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<title> Optimising random forest hyperparameters Â· Rui Vieira</title>
<link rel=canonical href=https://ruivieira.dev/optimising-random-forest-hyperparamaters.html>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=robots content="all,follow">
<meta name=googlebot content="index,follow,snippet,archive">
<meta property="og:title" content="Optimising random forest hyperparameters">
<meta property="og:description" content="Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:
 The number of decision trees in a random forest The split criteria Maximum depth of individual trees Minimum samples per internal node Maximum number of leaf nodes Random features per split Number of samples in bootstrap dataset  We will look at each of these hyper-parameters individually with examples of how to select them.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ruivieira.dev/optimising-random-forest-hyperparamaters.html"><meta property="article:section" content="posts">
<meta property="article:modified_time" content="2022-01-03T21:56:06+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Optimising random forest hyperparameters">
<meta name=twitter:description content="Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:
 The number of decision trees in a random forest The split criteria Maximum depth of individual trees Minimum samples per internal node Maximum number of leaf nodes Random features per split Number of samples in bootstrap dataset  We will look at each of these hyper-parameters individually with examples of how to select them.">
<link rel=stylesheet href=https://ruivieira.dev/css/styles.c7943c484a0ec624069ce4f9908c7aad84700a9adbf05dc4b61a91514c462f7ac69eb9afa125b9b4d0418de29d3cb150a48c4fbd3c6cb716ac308290ea466621.css integrity="sha512-x5Q8SEoOxiQGnOT5kIx6rYRwCprb8F3EthqRUUxGL3rGnrmvoSW5tNBBjeKdPLFQpIxPvTxstxasMIKQ6kZmIQ=="><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]-->
<link rel=icon type=image/png href=https://ruivieira.dev/images/favicon.ico>
</head>
<body class="max-width mx-auto px3 ltr">
<div class="content index py4">
<div id=header-post>
<a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast')" style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu>
<span id=nav>
<ul>
<li><a href=/>Home</a></li>
<li><a href=/map/>All pages</a></li>
<li><a href=/search.html>Search</a></li>
</ul>
</span>
<br>
<span id=actions>
<ul>
<li>
<a class=icon href=https://ruivieira.dev/pandas.html aria-label=Previous>
<i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle()" onmouseout="$('#i-prev').toggle()"></i>
</a>
</li>
<li>
<a class=icon href=https://ruivieira.dev/optaplanner.html aria-label=Next>
<i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle()" onmouseout="$('#i-next').toggle()"></i>
</a>
</li>
<li>
<a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast')" aria-label="Top of Page">
<i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle()" onmouseout="$('#i-top').toggle()"></i>
</a>
</li>
<li>
<a class=icon href=# aria-label=Share>
<i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle()" onmouseout="$('#i-share').toggle()" onclick="return $('#share').toggle(),!1"></i>
</a>
</li>
</ul>
<span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span>
</span>
<br>
<div id=share style=display:none>
<ul>
<li>
<a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html" aria-label=Facebook>
<i class="fab fa-facebook" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://twitter.com/share?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&text=Optimising%20random%20forest%20hyperparameters" aria-label=Twitter>
<i class="fab fa-twitter" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label=Linkedin>
<i class="fab fa-linkedin" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&is_video=false&description=Optimising%20random%20forest%20hyperparameters" aria-label=Pinterest>
<i class="fab fa-pinterest" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="mailto:?subject=Optimising%20random%20forest%20hyperparameters&body=Check out this article: https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html" aria-label=Email>
<i class="fas fa-envelope" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label=Pocket>
<i class="fab fa-get-pocket" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label=reddit>
<i class="fab fa-reddit" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&name=Optimising%20random%20forest%20hyperparameters&description=Typically%20the%20hyper-parameters%20which%20will%20have%20the%20most%20significant%20impact%20on%20the%20behaviour%20of%20a%20random%20forest%20are%20the%20following%3a%0a%20The%20number%20of%20decision%20trees%20in%20a%20random%20forest%20The%20split%20criteria%20Maximum%20depth%20of%20individual%20trees%20Minimum%20samples%20per%20internal%20node%20Maximum%20number%20of%20leaf%20nodes%20Random%20features%20per%20split%20Number%20of%20samples%20in%20bootstrap%20dataset%20%20We%20will%20look%20at%20each%20of%20these%20hyper-parameters%20individually%20with%20examples%20of%20how%20to%20select%20them." aria-label=Tumblr>
<i class="fab fa-tumblr" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&t=Optimising%20random%20forest%20hyperparameters" aria-label="Hacker News">
<i class="fab fa-hacker-news" aria-hidden=true></i>
</a>
</li>
</ul>
</div>
<div id=toc>
<h4>Contents</h4>
<nav id=TableOfContents>
<ul>
<li><a href=#data>Data</a></li>
<li><a href=#naive-model>Naive model</a></li>
<li><a href=#hyperparameter-search>Hyperparameter search</a></li>
<li><a href=#parameters>Parameters</a>
<ul>
<li><a href=#number-of-decision-trees>Number of decision trees</a></li>
<li><a href=#the-split-criteria>The split criteria</a></li>
<li><a href=#maximum-depth-of-individual-trees>Maximum depth of individual trees</a></li>
<li><a href=#maximum-number-of-leaf-nodes>Maximum number of leaf nodes</a></li>
<li><a href=#random-features-per-split>Random features per split</a></li>
</ul>
</li>
</ul>
</nav>
<h4>Related</h4>
<nav>
<ul>
<li class="header-post toc"><a href=https://ruivieira.dev/scikit-learn.html>Scikit-learn</a></li>
</ul>
</nav>
</div>
</span>
</div>
<article class=post itemscope itemtype=http://schema.org/BlogPosting>
<header>
<h1 class=posttitle itemprop="name headline">
Optimising random forest hyperparameters
</h1>
<div class=meta>
<div class=postdate>
Updated <time datetime="2022-01-03 21:56:06 +0000 GMT" itemprop=datePublished>2022-01-03</time>
<span class=commit-hash>(261c776)</span>
</div>
</div>
</header>
<div class=content itemprop=articleBody>
<p>Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:</p>
<ul>
<li><a href=#number-of-decision-trees>The number of decision trees</a> in a random forest</li>
<li>The split criteria</li>
<li>Maximum depth of individual trees</li>
<li>Minimum samples per internal node</li>
<li>Maximum number of leaf nodes</li>
<li>Random features per split</li>
<li>Number of samples in bootstrap dataset</li>
</ul>
<p>We will look at each of these hyper-parameters individually with examples of how to select them.</p>
<h2 id=data>Data</h2>
<p>To understand how we can optimise the hyperparameters in a random forest model, we will use [Scikit-learn|scikit-learn&rsquo;s]] <code>RandomForestClassifier</code> and a subset of <em>Titanic</em><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> dataset.</p>
<p>First, we will import the features and labels using [Pandas]].</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>import</span> <span style=color:#555>pandas</span> <span style=font-weight:700>as</span> <span style=color:#555>pd</span>

train_features <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>read_csv(<span style=color:#b84>&#34;data/svm-hyperparameters-train-features.csv&#34;</span>)
train_label <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>read_csv(<span style=color:#b84>&#34;data/svm-hyperparameters-train-label.csv&#34;</span>)
</code></pre></div><p>Let&rsquo;s look at a random sample of entries from this dataset, both for features and labels.</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_features<span style=font-weight:700>.</span>sample(<span style=color:#099>10</span>)
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>     Pclass  Sex    Age  SibSp  Parch   Fare
753       3    1 23.000      0      0  7.896
454       3    1 30.000      0      0  8.050
554       3    0 22.000      0      0  7.775
112       3    1 22.000      0      0  8.050
476       2    1 34.000      1      0 21.000
265       2    1 36.000      0      0 10.500
158       3    1 30.000      0      0  8.662
330       3    0 30.000      2      0 23.250
687       3    1 19.000      0      0 10.171
803       3    1  0.420      0      1  8.517
</code></pre></div><p>Some of the available features are:</p>
<ul>
<li><code>Pclass</code>, ticket class</li>
<li><code>Sex</code></li>
<li><code>Age</code>, age in years</li>
<li><code>Sibsp</code>, number of siblings/spouses aboard</li>
<li><code>Parch</code>, number of parents/children aboard</li>
<li><code>Fare</code>, passenger fare</li>
</ul>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_label<span style=font-weight:700>.</span>sample(<span style=color:#099>10</span>)
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>     Survived
557         0
145         0
380         1
846         0
134         0
807         0
349         0
582         0
867         0
604         1
</code></pre></div><p>The outcome label indicates whether a passenger survived the disaster.</p>
<p>As part of the typical initial steps for model training, we will prepare the data by splitting it into a training and testing subset.</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>from</span> <span style=color:#555>sklearn.model_selection</span> <span style=font-weight:700>import</span> train_test_split

X_train, X_test, y_train, y_test <span style=font-weight:700>=</span> train_test_split(train_features, train_label, test_size<span style=font-weight:700>=</span><span style=color:#099>0.33</span>, random_state<span style=font-weight:700>=</span><span style=color:#099>23</span>)
</code></pre></div><h2 id=naive-model>Naive model</h2>
<p>First we will train a &ldquo;naive&rdquo; model, that is a model using the defaults provided by <code>RandomForestClassifier</code><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. These defaults are:</p>
<ul>
<li><code>n_estimators = 10</code></li>
<li><code>criterion=âginiâ</code></li>
<li><code>max_depth=None</code></li>
<li><code>min_samples_split=2</code></li>
<li><code>min_samples_leaf=1</code></li>
<li><code>min_weight_fraction_leaf=0.0</code></li>
<li><code>max_features=âautoâ</code></li>
<li><code>max_leaf_nodes=None</code></li>
<li><code>min_impurity_decrease=0.0</code></li>
<li><code>min_impurity_split=None</code></li>
<li><code>bootstrap=True</code></li>
<li><code>oob_score=False</code></li>
<li><code>n_jobs=1</code></li>
<li><code>random_state=None</code></li>
<li><code>verbose=0</code></li>
<li><code>warm_start=False</code></li>
<li><code>class_weight=None</code></li>
</ul>
<p>We will instantiate a random forest classifier:</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>from</span> <span style=color:#555>sklearn.ensemble</span> <span style=font-weight:700>import</span> RandomForestClassifier

rf <span style=font-weight:700>=</span> RandomForestClassifier()
</code></pre></div><p>And training it using the <code>X_train</code> and <code>y_train</code> subsets using the appropriate <code>fit</code> method<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>true_labels <span style=font-weight:700>=</span> train_label<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel()

rf<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>RandomForestClassifier()
</code></pre></div><p>We can now evaluate trained naive model&rsquo;s score.</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>from</span> <span style=color:#555>sklearn.metrics</span> <span style=font-weight:700>import</span> precision_score

predicted_labels <span style=font-weight:700>=</span> rf<span style=font-weight:700>.</span>predict(X_test)

precision_score(y_test, predicted_labels)
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>0.7383177570093458
</code></pre></div><h2 id=hyperparameter-search>Hyperparameter search</h2>
<p>A simple example of a generic hyperparameter search using the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.model%5Fselection.GridSearchCV.html><code>GridSearchCV</code></a> method in <code>scikit-learn</code>. The score used to measure the &ldquo;best&rdquo; model is the <code>mean_test_score</code>, but other metrics could be used, such as the [OOB score in random forests|Out-of-bag (OOB)]] error.</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>parameters <span style=font-weight:700>=</span> {
    <span style=color:#b84>&#34;n_estimators&#34;</span>:[<span style=color:#099>5</span>,<span style=color:#099>10</span>,<span style=color:#099>50</span>,<span style=color:#099>100</span>,<span style=color:#099>250</span>],
    <span style=color:#b84>&#34;max_depth&#34;</span>:[<span style=color:#099>2</span>,<span style=color:#099>4</span>,<span style=color:#099>8</span>,<span style=color:#099>16</span>,<span style=color:#099>32</span>,<span style=font-weight:700>None</span>]

}
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>rfc <span style=font-weight:700>=</span> RandomForestClassifier()
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>from</span> <span style=color:#555>sklearn.model_selection</span> <span style=font-weight:700>import</span> GridSearchCV
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,parameters,cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>GridSearchCV(cv=5, estimator=RandomForestClassifier(),
             param_grid={&#39;max_depth&#39;: [2, 4, 8, 16, 32, None],
                         &#39;n_estimators&#39;: [5, 10, 50, 100, 250]})
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>display</span>(results):
    <span style=color:#999>print</span>(<span style=color:#b84>f</span><span style=color:#b84>&#39;Best parameters are: </span><span style=color:#b84>{</span>results<span style=font-weight:700>.</span>best_params_<span style=color:#b84>}</span><span style=color:#b84>&#39;</span>)
    <span style=color:#999>print</span>(<span style=color:#b84>&#34;</span><span style=color:#b84>\n</span><span style=color:#b84>&#34;</span>)
    mean_score <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]
    std_score <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]
    params <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]
    <span style=font-weight:700>for</span> mean,std,params <span style=font-weight:700>in</span> <span style=color:#999>zip</span>(mean_score,std_score,params):
        <span style=color:#999>print</span>(<span style=color:#b84>f</span><span style=color:#b84>&#39;</span><span style=color:#b84>{</span><span style=color:#999>round</span>(mean,<span style=color:#099>3</span>)<span style=color:#b84>}</span><span style=color:#b84> + or -</span><span style=color:#b84>{</span><span style=color:#999>round</span>(std,<span style=color:#099>3</span>)<span style=color:#b84>}</span><span style=color:#b84> for the </span><span style=color:#b84>{</span>params<span style=color:#b84>}</span><span style=color:#b84>&#39;</span>)
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>display(cv)
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>Best parameters are: {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 250}


0.765 + or -0.025 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 5}
0.787 + or -0.033 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 10}
0.777 + or -0.026 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 50}
0.78 + or -0.023 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 100}
0.779 + or -0.015 for the {&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 250}
0.819 + or -0.024 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 5}
0.814 + or -0.028 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 10}
0.814 + or -0.018 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 50}
0.812 + or -0.025 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 100}
0.817 + or -0.02 for the {&#39;max_depth&#39;: 4, &#39;n_estimators&#39;: 250}
0.8 + or -0.016 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 5}
0.802 + or -0.025 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 10}
0.815 + or -0.01 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 50}
0.814 + or -0.016 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 100}
0.817 + or -0.022 for the {&#39;max_depth&#39;: 8, &#39;n_estimators&#39;: 250}
0.797 + or -0.019 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 5}
0.805 + or -0.024 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 10}
0.81 + or -0.017 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 50}
0.81 + or -0.02 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 100}
0.819 + or -0.027 for the {&#39;max_depth&#39;: 16, &#39;n_estimators&#39;: 250}
0.777 + or -0.033 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 5}
0.795 + or -0.036 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 10}
0.814 + or -0.033 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 50}
0.812 + or -0.022 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 100}
0.812 + or -0.021 for the {&#39;max_depth&#39;: 32, &#39;n_estimators&#39;: 250}
0.774 + or -0.028 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 5}
0.792 + or -0.033 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 10}
0.807 + or -0.024 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 50}
0.807 + or -0.025 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 100}
0.804 + or -0.026 for the {&#39;max_depth&#39;: None, &#39;n_estimators&#39;: 250}
</code></pre></div><h2 id=parameters>Parameters</h2>
<h3 id=number-of-decision-trees>Number of decision trees</h3>
<p>This is specified using the <code>n_estimators</code> hyper-parameter on the random forest initialisation.</p>
<p>Typically, a higher number of trees will lead to greater accuracy at the expense of model size and training time.</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#34;n_estimators&#34;</span>:[<span style=color:#099>2</span>, <span style=color:#099>4</span>, <span style=color:#099>8</span>, <span style=color:#099>16</span>, <span style=color:#099>32</span>, <span style=color:#099>64</span>, <span style=color:#099>128</span>, <span style=color:#099>256</span>, <span style=color:#099>512</span>]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>GridSearchCV(cv=5, estimator=RandomForestClassifier(),
             param_grid={&#39;n_estimators&#39;: [2, 4, 8, 16, 32, 64, 128, 256, 512]})
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;n_estimators&#34;</span>: [param[<span style=color:#b84>&#34;n_estimators&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
results
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>   n_estimators  mean_score  std_score
0             2       0.753      0.016
1             4       0.790      0.024
2             8       0.804      0.016
3            16       0.792      0.029
4            32       0.810      0.022
5            64       0.809      0.036
6           128       0.805      0.023
7           256       0.810      0.023
8           512       0.810      0.018
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>

(
     ggplot(results) <span style=font-weight:700>+</span> geom_boxplot(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(n_estimators)&#39;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score&#39;</span>)) <span style=font-weight:700>+</span>
    geom_errorbar(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(n_estimators)&#39;</span>, ymin<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score - std_score&#39;</span>, ymax<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score + std_score&#39;</span>)) <span style=font-weight:700>+</span>
    theme_classic() <span style=font-weight:700>+</span> xlab(<span style=color:#b84>&#39;Number of trees&#39;</span>) <span style=font-weight:700>+</span> ylab(<span style=color:#b84>&#39;Mean score&#39;</span>)
)
</code></pre></div><figure><img src=/ox-hugo/846935f104a5b9268b67ffdc459d9d7559ca0572.png>
</figure>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>&lt;ggplot: (337725832)&gt;
</code></pre></div><h3 id=the-split-criteria>The split criteria</h3>
<p>At each node, a random forest decides, according to a specific algorithm, which feature and value split the tree.
Therefore, the choice of splitting algorithm is crucial for the random forest&rsquo;s performance.</p>
<p>Since, in this example, we are dealing with a classification problem, the choices of split algorithm are, for instance:</p>
<ul>
<li>Gini</li>
<li>Entropy</li>
</ul>
<p>If we were dealing with a random forest for regression, other methods (such as [Error metrics|MSE]]) would be a possible choice.
We will now compare both split algorithms as specified above, in training a random forest with our data:</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>rfc <span style=font-weight:700>=</span> RandomForestClassifier(n_estimators<span style=font-weight:700>=</span><span style=color:#099>256</span>)

cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#34;criterion&#34;</span>: [<span style=color:#b84>&#34;gini&#34;</span>, <span style=color:#b84>&#34;entropy&#34;</span>]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>GridSearchCV(cv=5, estimator=RandomForestClassifier(n_estimators=256),
             param_grid={&#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;]})
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;criterion&#34;</span>: [param[<span style=color:#b84>&#34;criterion&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
results
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>  criterion  mean_score  std_score
0      gini       0.812      0.021
1   entropy       0.809      0.023
</code></pre></div><h3 id=maximum-depth-of-individual-trees>Maximum depth of individual trees</h3>
<p>In theory, the &ldquo;longer&rdquo; the tree, the more splits it can have and better accommodate the data. However, at the tree level can this can lead to overfitting.
Although this is a problem for decision trees, it is not necessarily a problem for the ensemble, the random forest.
Although the key is to strike a balance between trees that aren&rsquo;t too large or too short, there&rsquo;s no universal heuristic to determine the size.
Let&rsquo;s try a few option for maximum depth:</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>rfc <span style=font-weight:700>=</span> RandomForestClassifier(n_estimators<span style=font-weight:700>=</span><span style=color:#099>256</span>,
                           criterion<span style=font-weight:700>=</span><span style=color:#b84>&#34;entropy&#34;</span>)

cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#39;max_depth&#39;</span>: [<span style=color:#099>2</span>, <span style=color:#099>4</span>, <span style=color:#099>8</span>, <span style=color:#099>16</span>, <span style=color:#099>32</span>, <span style=font-weight:700>None</span>]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>GridSearchCV(cv=5,
             estimator=RandomForestClassifier(criterion=&#39;entropy&#39;,
                                              n_estimators=256),
             param_grid={&#39;max_depth&#39;: [2, 4, 8, 16, 32, None]})
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;max_depth&#34;</span>: [param[<span style=color:#b84>&#34;max_depth&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
results <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>dropna()
results
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>   max_depth  mean_score  std_score
0      2.000       0.779      0.030
1      4.000       0.812      0.021
2      8.000       0.810      0.019
3     16.000       0.810      0.020
4     32.000       0.802      0.024
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>

(
     ggplot(results) <span style=font-weight:700>+</span> geom_boxplot(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_depth)&#39;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score&#39;</span>)) <span style=font-weight:700>+</span>
    theme_classic() <span style=font-weight:700>+</span> xlab(<span style=color:#b84>&#39;Max tree depth&#39;</span>) <span style=font-weight:700>+</span> ylab(<span style=color:#b84>&#39;Mean score&#39;</span>)
)
</code></pre></div><figure><img src=/ox-hugo/e149d88c07b14dcbec14636e31f25714143efa81.png>
</figure>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>&lt;ggplot: (338114832)&gt;
</code></pre></div><h3 id=maximum-number-of-leaf-nodes>Maximum number of leaf nodes</h3>
<p>This hyperparameter can be of importance to other topics, such as [Explainability]].</p>
<p>It is specified in <code>scikit-learn</code> using the <code>max_leaf_nodes</code> parameter. Let&rsquo;s try a few different values:</p>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>rfc <span style=font-weight:700>=</span> RandomForestClassifier(n_estimators<span style=font-weight:700>=</span><span style=color:#099>256</span>,
                           criterion<span style=font-weight:700>=</span><span style=color:#b84>&#34;entropy&#34;</span>,
                            max_depth<span style=font-weight:700>=</span><span style=color:#099>8</span>)

cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#39;max_leaf_nodes&#39;</span>: [<span style=color:#099>2</span><span style=font-weight:700>**</span>i <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> <span style=color:#999>range</span>(<span style=color:#099>1</span>, <span style=color:#099>8</span>)]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>GridSearchCV(cv=5,
             estimator=RandomForestClassifier(criterion=&#39;entropy&#39;, max_depth=8,
                                              n_estimators=256),
             param_grid={&#39;max_leaf_nodes&#39;: [2, 4, 8, 16, 32, 64, 128]})
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;max_leaf_nodes&#34;</span>: [param[<span style=color:#b84>&#34;max_leaf_nodes&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
results <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>dropna()
results
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>   max_leaf_nodes  mean_score  std_score
0               2       0.765      0.019
1               4       0.777      0.029
2               8       0.819      0.027
3              16       0.812      0.017
4              32       0.822      0.019
5              64       0.815      0.014
6             128       0.814      0.013
</code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>

(
     ggplot(results) <span style=font-weight:700>+</span> geom_boxplot(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_leaf_nodes)&#39;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score&#39;</span>)) <span style=font-weight:700>+</span>
    geom_errorbar(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_leaf_nodes)&#39;</span>, ymin<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score - std_score&#39;</span>, ymax<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score + std_score&#39;</span>)) <span style=font-weight:700>+</span>
    theme_classic() <span style=font-weight:700>+</span> xlab(<span style=color:#b84>&#39;Maximum leaf nodes&#39;</span>) <span style=font-weight:700>+</span> ylab(<span style=color:#b84>&#39;Mean score&#39;</span>)
)
</code></pre></div><figure><img src=/ox-hugo/dddb466a94666b81d5ee906c2cc785c3dd0f96a3.png>
</figure>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>&lt;ggplot: (337576795)&gt;
</code></pre></div><h3 id=random-features-per-split>Random features per split</h3>
<p>This is an important hyperparameter that will depend on how noisy the original data is.
Typically, if the data is not very noisy, the number of used random features can be kept low. Otherwise, it needs to be kept high.</p>
<p>An important consideration is also the following trade-off:</p>
<ul>
<li>A low number of random features decrease the forest&rsquo;s overall variance</li>
<li>A low number of random features increases the bias</li>
<li>A high number of random features increases computational time</li>
</ul>
<p>In `scikit-learn` this is specified with the `max_features` parameter. Assuming \(N_f\) is the total number of features,
some possible values for this parameter are:</p>
<ul>
<li>`sqrt`, this will take the `max_features` as the rounded \(\sqrt{N_f}\)</li>
<li>`log2`, as above, takes the \(\log_2(N_f)\)</li>
<li>The actual maximum number of features can be directly specified</li>
</ul>
<p>Let&rsquo;s try a simple benchmark, even though our data does not have many features to begin with:</p>
<p>```python</p>
<pre tabindex=0><code class=language-jupyter-python data-lang=jupyter-python>rfc = RandomForestClassifier(n_estimators=256,
                             criterion=&quot;entropy&quot;,
                             max_depth=8)

cv = GridSearchCV(rfc,{'max_features': [&quot;sqrt&quot;, &quot;log2&quot;, 1, 2, 3, 4, 5, 6]},cv=5)
cv.fit(X_train, y_train.values.ravel())
```
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>  File &#34;/var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/ipykernel_5612/1759652987.py&#34;, line 7
    ```
    ^
SyntaxError: invalid syntax
</code></pre></div><p>```python</p>
<pre tabindex=0><code class=language-jupyter-python data-lang=jupyter-python>results = pd.DataFrame({&quot;max_features&quot;: [param[&quot;max_features&quot;] for param in cv.cv_results_['params']],
             &quot;mean_score&quot;: list(cv.cv_results_['mean_test_score']),
             &quot;std_score&quot;: cv.cv_results_['std_test_score']})
results
```
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>  File &#34;/var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/ipykernel_5612/3235852250.py&#34;, line 5
    ```
    ^
SyntaxError: invalid syntax
</code></pre></div><p>```python</p>
<pre tabindex=0><code class=language-jupyter-python data-lang=jupyter-python>from plotnine import *

(
     ggplot(results) + geom_boxplot(aes(x='factor(max_features)', y='mean_score')) +
    geom_errorbar(aes(x='factor(max_features)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
    theme_classic() + xlab('Maximum number of features') + ylab('Mean score')
)
```
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>  File &#34;/var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/ipykernel_5612/1677075331.py&#34;, line 8
    ```
    ^
SyntaxError: invalid syntax
</code></pre></div><p>### Bootstrap dataset size</p>
<p>This hyperparameter relates to the proportion of the training data to be used by decision trees.</p>
<p>It is specified in `scikit-learn` by `max_samples` and can take the value of either:</p>
<ul>
<li>`None`, take the entirety of the samples</li>
<li>An integer, representing the actual number of samples</li>
<li>A float, representing a proportion between `0` and `1` or the samples to take.</li>
</ul>
<p>Let&rsquo;s try a hyperparameter search with some values:</p>
<pre tabindex=0><code class=language-jupyter-python data-lang=jupyter-python>rfc = RandomForestClassifier(n_estimators=256,
                             criterion=&quot;entropy&quot;,
                             max_depth=8,
                             max_features=6)

cv = GridSearchCV(rfc,{'max_samples': [i/10.0 for i in range(1, 10)]},cv=5)
cv.fit(X_train, y_train.values.ravel())
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>GridSearchCV(cv=5,
             estimator=RandomForestClassifier(criterion=&#39;entropy&#39;, max_depth=8,
                                              max_features=6,
                                              n_estimators=256),
             param_grid={&#39;max_samples&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,
                                         0.9]})
</code></pre></div><pre tabindex=0><code class=language-jupyter-python data-lang=jupyter-python>results = pd.DataFrame({&quot;max_samples&quot;: [param[&quot;max_samples&quot;] for param in cv.cv_results_['params']],
             &quot;mean_score&quot;: list(cv.cv_results_['mean_test_score']),
             &quot;std_score&quot;: cv.cv_results_['std_test_score']})
results
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>   max_samples  mean_score  std_score
0        0.100       0.797      0.033
1        0.200       0.815      0.011
2        0.300       0.814      0.020
3        0.400       0.824      0.017
4        0.500       0.831      0.021
5        0.600       0.826      0.016
6        0.700       0.826      0.022
7        0.800       0.834      0.021
8        0.900       0.829      0.022
</code></pre></div><pre tabindex=0><code class=language-jupyter-python data-lang=jupyter-python>from plotnine import *

(
     ggplot(results) + geom_boxplot(aes(x='factor(max_samples)', y='mean_score')) +
    geom_errorbar(aes(x='factor(max_samples)', ymin='mean_score - std_score', ymax='mean_score + std_score')) +
    theme_classic() + xlab('Proportion bootstrap samples') + ylab('Mean score')
)
</code></pre><figure><img src=/ox-hugo/0968851683239aeac34b157a53c7fc589db10ba0.png>
</figure>
<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>&lt;ggplot: (337889771)&gt;
</code></pre></div><section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p>: Titanic Dataset - <a href=https://www.kaggle.com/c/titanic-dataset/data>https://www.kaggle.com/c/titanic-dataset/data</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:2 role=doc-endnote>
<p><a href=https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html>https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:3 role=doc-endnote>
<p><a href=https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit>https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
</article>
<div id=footer-post-container>
<div id=footer-post>
<div id=nav-footer style=display:none>
<ul>
<li><a href=/>Home</a></li>
<li><a href=/map/>All pages</a></li>
<li><a href=/search.html>Search</a></li>
</ul>
</div>
<div id=toc-footer style=display:none>
<nav id=TableOfContents>
<ul>
<li><a href=#data>Data</a></li>
<li><a href=#naive-model>Naive model</a></li>
<li><a href=#hyperparameter-search>Hyperparameter search</a></li>
<li><a href=#parameters>Parameters</a>
<ul>
<li><a href=#number-of-decision-trees>Number of decision trees</a></li>
<li><a href=#the-split-criteria>The split criteria</a></li>
<li><a href=#maximum-depth-of-individual-trees>Maximum depth of individual trees</a></li>
<li><a href=#maximum-number-of-leaf-nodes>Maximum number of leaf nodes</a></li>
<li><a href=#random-features-per-split>Random features per split</a></li>
</ul>
</li>
</ul>
</nav>
</div>
<div id=share-footer style=display:none>
<ul>
<li>
<a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html" aria-label=Facebook>
<i class="fab fa-facebook fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://twitter.com/share?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&text=Optimising%20random%20forest%20hyperparameters" aria-label=Twitter>
<i class="fab fa-twitter fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label=Linkedin>
<i class="fab fa-linkedin fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&is_video=false&description=Optimising%20random%20forest%20hyperparameters" aria-label=Pinterest>
<i class="fab fa-pinterest fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="mailto:?subject=Optimising%20random%20forest%20hyperparameters&body=Check out this article: https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html" aria-label=Email>
<i class="fas fa-envelope fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label=Pocket>
<i class="fab fa-get-pocket fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&title=Optimising%20random%20forest%20hyperparameters" aria-label=reddit>
<i class="fab fa-reddit fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&name=Optimising%20random%20forest%20hyperparameters&description=Typically%20the%20hyper-parameters%20which%20will%20have%20the%20most%20significant%20impact%20on%20the%20behaviour%20of%20a%20random%20forest%20are%20the%20following%3a%0a%20The%20number%20of%20decision%20trees%20in%20a%20random%20forest%20The%20split%20criteria%20Maximum%20depth%20of%20individual%20trees%20Minimum%20samples%20per%20internal%20node%20Maximum%20number%20of%20leaf%20nodes%20Random%20features%20per%20split%20Number%20of%20samples%20in%20bootstrap%20dataset%20%20We%20will%20look%20at%20each%20of%20these%20hyper-parameters%20individually%20with%20examples%20of%20how%20to%20select%20them." aria-label=Tumblr>
<i class="fab fa-tumblr fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fruivieira.dev%2foptimising-random-forest-hyperparamaters.html&t=Optimising%20random%20forest%20hyperparameters" aria-label="Hacker News">
<i class="fab fa-hacker-news fa-lg" aria-hidden=true></i>
</a>
</li>
</ul>
</div>
<div id=actions-footer>
<a id=menu-toggle class=icon href=# onclick="return $('#nav-footer').toggle(),!1" aria-label=Menu>
<i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick="return $('#toc-footer').toggle(),!1" aria-label=TOC>
<i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick="return $('#share-footer').toggle(),!1" aria-label=Share>
<i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast')" aria-label="Top of Page">
<i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a>
</div>
</div>
</div>
<footer id=footer>
<div class=footer-left>
Copyright &copy; 2022 Rui Vieira
</div>
<div class=footer-right>
<nav>
<ul>
<li><a href=/>Home</a></li>
<li><a href=/map/>All pages</a></li>
<li><a href=/search.html>Search</a></li>
</ul>
</nav>
</div>
</footer>
</div>
</body>
<link rel=stylesheet href=/css/fa.min.css>
<script src=/js/jquery-3.6.0.min.js></script>
<script src=/js/main.js></script>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</html>