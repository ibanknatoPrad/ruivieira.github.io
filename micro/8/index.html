<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Clustering metrics: Dunn index"/>
<meta name="twitter:description" content="There are several ways to measure the robustness of a clustering algorithm. Three commonly used metrics are the Dunn index, Davis-Bouldin index and Silhoutte index.
But before we start, let&rsquo;s introduce some concepts.
We are interested in clustering algorithms for a dataset $\mathcal{D}$ with $N$ elements in a $n$-dimensional real space, that is:
$$ \mathcal{D} = {x_1, x_2, \ldots, x_N} \in \mathbb{R}^p $$
The clustering algorithm will create a set $C$ of $K$ distinct disjoint groups from $\mathcal{D}$ $C={c_1, c_2, \ldots, c_k}$, such that:"/>


    <meta name="author" content="Rui Vieira" />
    <meta name="copyright" content="Rui Vieira" />
    <meta name="generator" content="Rui Vieira"> 
    
    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="/css/trac.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    
    <script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-10507665-2', 'auto');
	
	ga('send', 'pageview');
}
</script>


    </head>
<body>


  <div id="navigation">
    <span class="nav-item"><a href="/">Posts</a></span> ●
    <span class="nav-item"><a href="/micro/">µ-posts</a></span> ●
    <span class="nav-item"><a href="/pages/about.html">About</a></span>
  </div>


  <div id="sidebar">
  <h2>Other pages</h2>
  <ul>
    <li><a href="/">Posts</a></li>
    <li><a href="/micro/">µ-posts</a></li>
    <li><a href="/pages/about.html">About</a></li>
    <li><a href="/pages/codex.html">Codex</a></li>
  </ul>
</div>

<script>
  (function () {
    var elements = document.getElementById('main').querySelectorAll("h1, h2, h3, h4, h5, h6");
    if (elements.length > 1) {
      var div = document.getElementById('sidebar');
      div.innerHTML += '<h2>This page</h2>\n';
      

      var list = document.createElement("ul");

      var i = 1;
      for (var element of elements) {
        console.log(element);
        if (element.nodeName == "H2") {
          console.log(element.textContent);
          
          var anchor = document.createElement("a");
          anchor.setAttribute('name', i);
          anchor.classList.add('anchor');
          element.parentNode.insertBefore(anchor, element.nextSibling);
          
          var li = document.createElement("li");
          var a = document.createElement("a");
          a.setAttribute("href", "#" + i);
          a.textContent = element.textContent;
          li.appendChild(a);
          list.append(li);
          
          i++;
        }
        div.appendChild(list);

      }
      div.innerHTML += '</ul>\n';
    }
  })();      
</script>

  <div id="content" class="container">
  <div class="columns">

    <div class="column">
      
        <div>
          <h1>Clustering metrics: Dunn index</h1>
          <small class="date"><time pubdate="pubdate" datetime="2020-09-28 08:25:32 &#43;0200 &#43;0200">September 28, 2020</time></small>                
        </div>
        
      <div>
        <p>There are  several ways to measure the robustness of a clustering algorithm. Three commonly used metrics are the <em>Dunn index</em>, <em>Davis-Bouldin index</em> and <em>Silhoutte index</em>.</p>
<p>But before we start, let&rsquo;s introduce some concepts.</p>
<p>We are interested in clustering algorithms for a dataset $\mathcal{D}$ with $N$ elements in a $n$-dimensional real space, that is:</p>
<p>$$
\mathcal{D} = {x_1, x_2, \ldots, x_N} \in \mathbb{R}^p
$$</p>
<p>The clustering algorithm will create a set $C$ of $K$ distinct disjoint groups from $\mathcal{D}$ $C={c_1, c_2, \ldots, c_k}$, such that:</p>
<p>$$
\cup_{c_k\in C}c_k=\mathcal{D} \<br>
c_k \cap c_l \neq \emptyset \forall k\neq l
$$</p>
<p>Each group (or cluster) $c_k$, will have a <em>centroid</em>, $\bar{c_k}$, which is the mean vector of its elements such that:</p>
<p>$$
\bar{c_k}=\frac{1}{|c_k|}\sum_{x_i \in c_k}x_i
$$</p>
<p>We will also make use of the dataset&rsquo;s mean vector, $\bar{\mathcal{D}}$, defined as:</p>
<p>$$
\bar{\mathcal{D}}=\frac{1}{N}\sum_{x_i \in X}x_i
$$</p>
<h2 id="dunn-index">Dunn index</h2>
<p>The <em>Dunn index</em> <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> aims at quantifying the compactness and variance of the clustering.
A cluster is considered <em>compact</em> if there is small variance between members of the cluster.
This can be calculated using $\Delta(c_k)$, where</p>
<p>$$
\Delta(c_k) = \max_{x_i, x_j \in c_k}{d_e(x_i, x_j)}
$$</p>
<p>and $d_e$ is the Euclidian distance defined as:</p>
<p>$$
d_e=\sqrt{\sum_{j=1}^p (x_{ij}-x_{kj})^2}.
$$</p>
<p>A cluster is considered <em>well separated</em> if the cluster are far-apart. This can quantified using</p>
<p>$$
\delta(c_k, c_l) = \min_{x_i \in c_k}\min_{x_j\in c_l}{d_e(x_i, x_j)}.
$$</p>
<p>Given these quantities, the <em>Dunn index</em> for a set of clusters $C$, $DI(C)$, is then defined by:</p>
<p>$$
DI(C)=\frac{\min_{c_k \in C}{\delta(c_k, c_l)}}{\max_{c_k\in C}\Delta(c_k)}
$$</p>
<p>A higher <em>Dunn Index</em> will indicate compact, well-separated clusters, while a lower index will indicate less compact or less well-separated clusters.</p>
<p>We can now try to calculate the metric for the dataset we&rsquo;ve created previously.</p>
<p>Let&rsquo;s simulate some data and apply the <em>Dunn index</em> from scratch.
First, we will create a compact and well-separated dataset using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"><code>make_blobs</code></a> method in <code>scikit-learn</code>.
We will create a dataset of $\mathbb{R}^2$ data (for easier plotting), with three clusters.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/images/micro/8/output_4_0.png" alt="png"    /></p>
<p>We now cluster the data<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> and we will have, as expected three distinct clusters, plotted below.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cluster</span>

<span class="n">k_means</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">k_means</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">k_means</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pred&#39;</span><span class="p">])],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">clus0</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">prediction</span><span class="o">.</span><span class="n">pred</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">clus1</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">prediction</span><span class="o">.</span><span class="n">pred</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">clus2</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">prediction</span><span class="o">.</span><span class="n">pred</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">k_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">clus0</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">clus1</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">clus2</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
</code></pre></div><p><img loading="lazy" src="/images/micro/8/output_9_0.png" alt="png"    /></p>
<p>Let&rsquo;s focus now on two of these cluster, let&rsquo;s call them $c_k$ and $c_l$.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ck</span> <span class="o">=</span> <span class="n">k_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cl</span> <span class="o">=</span> <span class="n">k_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div><p>We know we have to calculate the distance between the points in $c_k$ and $c_l$. We know that the <code>len(ck)=len(cl)=333</code> we create</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">ck</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">cl</span><span class="p">)])</span>
<span class="n">values</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">    <span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
           <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
           <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
           <span class="o">...</span><span class="p">,</span>
           <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
           <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
           <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
</code></pre></div><p>For each pair of points, we then get the norm of $x_i-x_j$. For instance, for $i=0\in c_k$ and $i=1\in c_l$, we would have:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">ck</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">cl</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">ck</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cl</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">    <span class="p">[</span> <span class="mf">5.84161203</span> <span class="o">-</span><span class="mf">3.98182959</span>  <span class="mf">0.</span>        <span class="p">]</span> <span class="p">[</span><span class="mf">0.35023674</span> <span class="mf">9.74214185</span> <span class="mf">1.</span>        <span class="p">]</span>
    <span class="mf">14.815619955221214</span>
</code></pre></div><p>The calculation of $\delta(c_k, c_l)$ between two clusters $c_k$ and $c_l$ will be defined as follows:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="err">δ</span><span class="p">(</span><span class="n">ck</span><span class="p">,</span> <span class="n">cl</span><span class="p">):</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">ck</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">cl</span><span class="p">)])</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ck</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cl</span><span class="p">)):</span>
            <span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">ck</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cl</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</code></pre></div><p>So, for our two clusters above, $\delta(c_k, c_l)$ will be:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="err">δ</span><span class="p">(</span><span class="n">ck</span><span class="p">,</span> <span class="n">cl</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">    <span class="mf">8.64931117820059</span>
</code></pre></div><p>Within a single cluster $c_k$, we can calculate $\Delta(c_k)$ similarly as:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="err">Δ</span><span class="p">(</span><span class="n">ci</span><span class="p">):</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">ci</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">ci</span><span class="p">)])</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ci</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ci</span><span class="p">)):</span>
            <span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">ci</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">ci</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</code></pre></div><p>So, for instance, for our $c_k$ and $c_l$ we would have:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="err">Δ</span><span class="p">(</span><span class="n">ck</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="err">Δ</span><span class="p">(</span><span class="n">cl</span><span class="p">))</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">    <span class="mf">6.173844284636552</span>
    <span class="mf">5.8077337425156745</span>
</code></pre></div><p>We can now define the <em>Dunn index</em> as</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">dunn</span><span class="p">(</span><span class="n">k_list</span><span class="p">):</span>
    <span class="err">δ</span><span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">k_list</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">k_list</span><span class="p">)])</span>
    <span class="err">Δ</span><span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">k_list</span><span class="p">),</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">l_range</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">k_list</span><span class="p">)))</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">l_range</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="p">(</span><span class="n">l_range</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">]</span><span class="o">+</span><span class="n">l_range</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="err">δ</span><span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="err">δ</span><span class="p">(</span><span class="n">k_list</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">k_list</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
        
        <span class="err">Δ</span><span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="err">Δ</span><span class="p">(</span><span class="n">k_list</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

    <span class="n">di</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="err">δ</span><span class="n">s</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="err">Δ</span><span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">di</span>
</code></pre></div><p>and calculate the <em>Dunn index</em> for our clustered values list as</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">dunn</span><span class="p">(</span><span class="n">k_list</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">    <span class="mf">0.14867620697065728</span>
</code></pre></div><p>Intuitively, we can expect a dataset with less well-defined clusters to have a lower <em>Dunn index</em>. Let&rsquo;s try it.
We first generate the new dataset.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">])</span>
<span class="n">k_means</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">k_means</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="c1">#K-means training</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">k_means</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pred&#39;</span><span class="p">])],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">clus0</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">prediction</span><span class="o">.</span><span class="n">pred</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">clus1</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">prediction</span><span class="o">.</span><span class="n">pred</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">clus2</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">prediction</span><span class="o">.</span><span class="n">pred</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">k_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">clus0</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">clus1</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">clus2</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
</code></pre></div><p><img loading="lazy" src="/images/micro/8/output_30_1.png" alt="png"    /></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">dunn</span><span class="p">(</span><span class="n">k_list</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">    <span class="mf">0.019563892388205984</span>
</code></pre></div><section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Joseph C Dunn. Well-separated clusters and optimal fuzzy partitions. <em>Journal of cybernetics</em>, 4(1):95–104, 1974. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Using the default parameters, since that&rsquo;s not our primary concern at the moment. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
    </div>



  </div>

</body>
</html>