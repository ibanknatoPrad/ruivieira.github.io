<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Linear separability and Perceptrons"/>
<meta name="twitter:description" content="What is a linearly separable dataset and how to test it?
Formally, we can define an $n$-dimensional dataset as linearly separable if a hyperplane can completely separate two subsets. If we assume two subsets $A$ and $B$, such that $A={A^1,\ldots,A^a} \subseteq \mathbb{R}^d$ and $B={B^1,\ldots,B^b} \subset \mathbb{R}^d$ then:
$$ \exists a \in \mathbb{R}^n, b\in\mathbb{R}:A\subset\{x\in\mathbb{R}^n:a^Tx&gt;b\}, B\subset\{x\in\mathbb{R}^n:a^T\leq b\} $$
Let&rsquo;s illustrate this concept by first generating two datasets in $\mathbb{R}^2$, one clearly non-separable and another clearly separable and by a supervised learning technique with both to separate them."/>


    <meta name="author" content="Rui Vieira" />
    <meta name="copyright" content="Rui Vieira" />
    <meta name="generator" content="Rui Vieira"> 
    
    
<link rel="stylesheet" href="/css/style.css" />


    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    
    <script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-10507665-2', 'auto');
	
	ga('send', 'pageview');
}
</script>


    </head>
<body>


  <div id="navigation">
    <span class="nav-item"><a href="/">Posts</a></span> ●
    <span class="nav-item"><a href="/micro/">µ-posts</a></span> ●
    <span class="nav-item"><a href="/pages/about.html">About</a></span>
  </div>


  <div id="sidebar">
  <h2>Other pages</h2>
  <ul>
    <li><a href="/">Posts</a></li>
    <li><a href="/micro/">µ-posts</a></li>
    <li><a href="/pages/about.html">About</a></li>
  </ul>
  <h2>Codex</h2>
  <ul>
    <li><a href="/codex/machine-learning">Machine learning</a></li>
    <li><a href="/codex/python-experiments">Python experiments</a></li>
  </ul>
  <h2>Playground</h2>
  <ul>
    <li><a href="/playground/machine-learning">Machine learning</a></li>
  </ul>
</div>

<script>
  (function () {
    var elements = document
      .querySelector(".column")
      .querySelectorAll("h1, h2, h3, h4, h5, h6");
    if (elements.length > 1) {
      var div = document.getElementById("sidebar");
      div.innerHTML += "<h2>This page</h2>\n";
      

      var list = document.createElement("ul");

      var i = 1;
      for (var element of elements) {
        console.log(element);
        if (element.nodeName == "H2") {
          console.log(element.textContent);
          
          var anchor = document.createElement("a");
          anchor.setAttribute("name", i);
          anchor.classList.add("anchor");
          element.parentNode.insertBefore(anchor, element.nextSibling);
          
          var li = document.createElement("li");
          var a = document.createElement("a");
          a.setAttribute("href", "#" + i);
          a.textContent = element.textContent;
          li.appendChild(a);
          list.append(li);
          
          i++;
        }
        div.appendChild(list);
      }
      div.innerHTML += "</ul>\n";
    }
  })();
</script>


  <div id="content" class="container">
  <div class="columns">

    <div class="column">
      
        <div>
          <h1>Linear separability and Perceptrons</h1>
          <small class="date"><time pubdate="pubdate" datetime="2020-09-08 22:06:32 &#43;0200 &#43;0200">September 8, 2020</time></small>                
        </div>
        
      <div>
        <p>What is a linearly separable dataset and how to test it?</p>
<p>Formally, we can define an $n$-dimensional dataset as linearly separable if a hyperplane can completely separate two subsets.
If we assume two subsets $A$ and $B$, such that $A={A^1,\ldots,A^a} \subseteq \mathbb{R}^d$ and $B={B^1,\ldots,B^b} \subset \mathbb{R}^d$ then:</p>
<p>$$
\exists a \in \mathbb{R}^n, b\in\mathbb{R}:A\subset\{x\in\mathbb{R}^n:a^Tx&gt;b\}, B\subset\{x\in\mathbb{R}^n:a^T\leq b\}
$$</p>
<p>Let&rsquo;s illustrate this concept by first generating two datasets in $\mathbb{R}^2$, one clearly <em>non-separable</em> and another clearly <em>separable</em> and by a supervised learning technique with both to separate them.</p>
<h2 id="generating-datasets">Generating datasets</h2>
<p>To generate a clearly <em>non-separable</em> data, we can generate two concentric datasets in $\mathbb{R}^2$. To do that we will use <code>scikit-learn</code> utility method <code>make_circles</code><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">datasets</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">noise</span><span class="o">=.</span><span class="mo">05</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/images/micro/7_1.png" alt=""    /></p>
<p>Intuitively, we know that this dataset is non-separable. We cannot see any line that could cleanly divided these two datasets.
We now generate a clearly <em>separable</em> dataset and to do so, we use the <code>make_blobs</code> (also<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> from <code>scikit-learn</code>):</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X2</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/images/micro/7_2.png" alt=""    /></p>
<p>Again, intuitively, we know that this dataset is separable since we can clearly see that it is possible to separate these two datasets.</p>
<h2 id="perceptron">Perceptron</h2>
<p>The venerable Perceptron (which has a fascinating<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> history, by the way) is a simple binary classifier. For our scope, we will simply define it as a <em>threshold function</em> with the form:</p>
<p>$$
f(\mathbf{x}) =
\begin{cases}
1\qquad\text{if}\ \mathbf{w}\cdot\mathbf{x} + b &gt; 0, \\<br>
0\qquad\text{otherwise}
\end{cases}.
$$</p>
<p>Taking an input $\mathbf{x}$, the Perceptron return a binary classification, $f(\mathbf{x})$ using weights $\mathbf{w}$ and a bias $b$.</p>
<p>We will simply apply <code>scitkit-learn</code>&rsquo;s Perceptron implementation to both datasets (after normalisation) and try to estimate a separation line.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python">
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span>

<span class="c1"># normalisation</span>
<span class="n">sc</span><span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># fitting</span>
<span class="n">perceptron</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">perceptron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div><p>If we plot our decision boundary (below), we can see that our intuition was correct, of course. The perceptron simply could not find a clear separation between the two subsets.</p>
<p><img loading="lazy" src="/images/micro/7_3.png" alt=""    /></p>
<p>If we repeat the process for the <em>separable</em> dataset, then we will see that this time the Perceptron was able to find such a separation.</p>
<p><img loading="lazy" src="/images/micro/7_4.png" alt=""    /></p>
<p>We can quantify how well the Perceptron fared using the <code>perceptron.score()</code> method<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. This method returns the mean accuracy of the given data and labels. If we have a score of $1.0$, that means that it was able to completely separate the subsets and the label predictions were totally correct.
For the non-separable dataset, we get a score of $0.634$, which means that the method fared pretty badly, as expected. On the other hand, the score for the separable dataset we get of score of $1.0$.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://en.wikipedia.org/wiki/Perceptron#History">https://en.wikipedia.org/wiki/Perceptron#History</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.score">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.score</a> <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
    </div>



  </div>

</body>
</html>